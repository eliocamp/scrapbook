[
  {
    "path": "posts/2025-04-01-arctic_regime/",
    "title": "The Arctic might not be in a new regime",
    "description": "A paper claims the Arctic has entered a new regime, but it smells to much like the \"hiatus\".",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2025-05-06",
    "categories": [
      "arctic",
      "sea ice"
    ],
    "contents": "\n\n\n\n\n\n\nThis week I read this new paper “Regime Shift in Arctic Ocean Sea-Ice Extent” (Stern 2025).\nIt argues that Arctic sea ice has entered a “new regime” around 2007, going from a linear decrease to a stable (but lower state) extent.\nThe core of the argument is Figure 1, which compares a linear fit through the whole period with a linear fit until 2006 and then a constant line after.\n\n\n\n\nFigure 1: Figure 1 from Stern (2025). “(a) September Arctic sea-ice extent (\\(10^6 km^2\\)) from the NSIDC Sea Ice Index. The linear least-squares fit is shown in red with slope \\(−0.78 × 106 km^2\\) per decade. (b) The same data as in panel (a). The vertical dotted line between 2006 and 2007 marks the regime shift or breakpoint.\n\n\n\nI was following the discussions about Arctic sea ice around 2007–2012, with people arguing whether sea ice loss was accelerating or not, but eventually I stopped paying attention, so it’s interesting to see those discussions continuing even today.\nThey are particularly relevant in comparison with what is happening in the Antarctic sea ice, which seems to have switched from a slow but steady increase to a dramatic drop, which has also been called a “new regime”.\nSo I wanted to look a bit more into it.\n\n\n\nSo, let’s start by reproducing the plot.\nI downloaded daily Arctic sea ice extent from NOAA and computed September mean extent.\nFigure 2 shows the result plus linear fits for the period before and after 2007.\n\n\n\n\nFigure 2: September Arctic sea ice extent with a linear discontinuous fit with breakpoint at 2007.\n\n\n\nIndeed, the linear trends are very different.\nIs the difference statistically significant?\nKind of.\n\n                         Estimate  Std. Error    t value   Pr(>|t|)\n(Intercept)          24.357466601 39.55785694  0.6157428 0.54138692\nyear                 -0.009787556  0.01962676 -0.4986844 0.62060359\nbefore_2007TRUE      92.465235297 44.38901811  2.0830656 0.04337086\nyear:before_2007TRUE -0.045473519  0.02207630 -2.0598347 0.04564675\n\nThe fitting a model with varying slope and intercept for each period shows that the difference in the slope between periods is about -0.05 million squared meters per year, with a p-value of 0.046 which is not hugely significant.\nSo it seems that at face value the evidence for a change in “regime” based on the slope is not extremely strong.\nIf we look at all the data the picture grows even more complex.\nFigure 3 shows monthly extent anomalies computed using the whole period climatology (this will be important).\n\n\n\n\nFigure 3: Monthly Arctic sea ice extent anomalies with a linear discontinuous fit with breakpoint in 2007.\n\n\n\nNow it’s harder to say that Arctic sea ice is not decreasing.\nThe straight line fit starting in 2007 is still very much negative and not too hugely dissimilar from the previous period.\nThe record monthly minimum was reached just 4 years ago in October 2020.\nStatistical significance is a bit trickier to gauge here because monthly values autocorrelated, so the linear fit massively can overestimate residual degrees of freedom (just look at how narrow those confidence bands are!).\nA quick fix is to take yearly means.\nThe linear fit of the yearly mean extent with varying slope and intercept before and after 2006 shows that the change in slope is pretty much not statistically significant.\n\n                        Estimate  Std. Error   t value    Pr(>|t|)\n(Intercept)          57.71415781 20.38652721  2.830995 0.007029513\nyear                 -0.02339880  0.01011484 -2.313314 0.025553565\nbefore_2007TRUE      37.84963776 22.63858772  1.671908 0.101805048\nyear:before_2007TRUE -0.01863607  0.01125737 -1.655456 0.105113541\n\nSo the evidence for this “regime shift” is looking even weaker.\nSea ice extent is a “2D” measure that doesn’t take into account the thickness.\nHowever, thickness is hard to observe, so we don’t really have reliable observations spanning so many decades.\nWe do have PIOMAS, a model-based estimate of total sea ice volume.\nIt’s not perfect (who is) but it’s what we have.\nIn any case, does PIOMAS show a dramatic trend change in 2007?\nNot really. Figure 4 shows monthly Arctic sea ice volume anomalies with the usual broken linear trend.\n\n\n\n\n\n\n\nFigure 4: Arctic sea ice volume anomalies from PIOMAS with a linear discontinuous fit with breakpoint in 2007.\n\n\n\nAfter 2007 sea ice volume continued to decrease.\nOne could argue for a stabilisation after around 2010, but then the period is even shorter!\nSomething did change in the Arctic\nNow, there is some evidence for something fundamentally changing in the Arctic around 2007.\nThe monthly time series after 2007 in Figure 3 does look weird; the variability is through the roof.\nThe reason is that the seasonal cycle changed significantly in 2007.\n\n\n\n\nFigure 5: Monthly Arctic sea ice extent anomalies with respect to two different climatologies. The orange line was computed using on the 2014–2024 climatology and the green line was computed using the 1990–2000 climatology.\n\n\n\nFigure 5 shows monthly sea ice extent anomalies computed either using the 1990–2000 climatology or the 2014–2024 climatology.\nThe difference is striking!\nAnomalies computed with the early climatology look fine until 2007 when they start to swing wildly.\nOn the other hand, anomalies computed with the latter climatology look fine after 2007 but they swing before.\nFigure 6 shows those climatologies using daily values.\nThe amplitude of the seasonal cycle is much greater in the latter decade and arguably even the shape is slightly different.\n\n\n\n\nFigure 6: Daily mean Arctic sea ice extent deviation from annual mean computed for the two same periods from Figure 5.\n\n\n\nThis is not new.\nStefan Rahmstorf noticed this a while ago in this 2017 post, although I can’t seem to find any journal article describing the change.\nHe also just published an article analysing this “slowdown”, reaching basically the same conclusions as me.\nWhy so linear?\nEven though the evidence for a change in linear trend is not huge, I still believe the linear model is not correct.\nMathematically, it can’t be, since a linear model would predict negative sea ice extent if extrapolated into the future and an Earth covered in ice if extrapolated into the past.\nStatistically, it assumes an additive error model, which also can’t be right since could predict negative sea ice.\nPhysically, we wouldn’t expect ice to melt linearly.\nTake this extremely simplified equation for the evolution of sea ice concentration at a particular location:\n\\[\n\\frac{dC}{dt} =\n\\begin{cases}\nC\\alpha(T-T_f),& \\text{if } T\\geq T_f\\\\\n(1 - C)\\alpha(T-T_f),              & \\text{otherwise}\n\\end{cases}\n\\tag{1}\n\\]\nThis essentially states that sea ice concentration C will change at a rate that is proportional to the difference between the temperature and the freezing temperature (\\(T_f\\)) with \\(\\alpha\\) being the proportionality constant (I’m using \\(T_f\\) as 0°C which is not the correct freezing temperature for ocean water, but in principle it could be any other “equilibrium temperature” that balances thermodynamic freezing/melting with other processes).\nBut it will melt a proportion of what ice is there (that’s the first condition of the equation) and only freeze the open water regions (that’s the second condition of the equation).\n\n\n\n\n\n\n\n\n\n\n\n\nUnder this conditions, ice subjected to a linear increase in temperature would evolve as shown in Figure 7, which is very much not linear.\nIn this model higher latitudes start to melt at different moments because their starting temperature is lower, so it takes more time for it to get above freezing.\n\n\n\n\nFigure 7: Sea ice concentration at different latitudes simulated using the Equation (1) a forcing temperature with a constant linear trend.\n\n\n\nWhen combining all latitudes to compute an integrated area, the final curve is looks like Figure 8.\nThere’s an initial period of relative stability, as most regions are still to cold to experience melt, followed by period of rapid melt when the heat reaches most of the domain.\nFinally the rate of decrease starts to peter off as there is just too little ice to melt.\n\n\n\n\n\n\n\n\n\n\nFigure 8: Mean sea ice concentration from the simplified model.\n\n\n\nEven though a linear model can be a useful approximation to estimate rates of change, we know that it is not correct.\nSo one can’t argue that there has been a fundamental change in the behaviour of Arctic sea ice based on a simple linear model not working well.\nStern (2025) does explore briefly what I would expect to be a better model: a Generalised Logistic Function.\n\\[\nY(t)=A+{K-A \\over (C + Qe^{-Bt})^{1/\\nu }}\n\\]\nIt has left and right asymptotes (A and K, respectively), a rate of increase/decrease (B) and other shape parameters that I’m not going to use (I’ll set \\(C\\), \\(Q\\) and \\(\\nu\\) to 1).\nThis is an S-shaped function that model the idea that the environment has a certain “carrying capacity” above which a species can’t grow.\nFigure 9 shows the same simple simulation and the generalised logistic fit.\nThe fit works incredibly well!\n\n\n\n\nFigure 9: Mean sea ice concentration for the simple model with a Generalised Logistic Function fit.\n\n\n\nSo this type of function is more statistically sound and has a robust physically motivation.\nIt also fits the data very well, as can be seen in Figure 10.\n\n\n\n\nFigure 10: Annual mean Arctic sea ice extent and a generalised logistic function fit.\n\n\n\nThe authors fit a special case of the generalised logistic function called Gompertz function and show it in their supplementary materials, but they reject this fit in favour of their non-continuous piecewise linear fit based on the RMSE of the residuals, which doesn’t seem to me like a particularly strong reason to reject such a good and handsome curve.\n\n\n\nAlso, I can’t really reproduce that result!\nFor monthly September sea ice extent, the RMSE of the piecewise linear regression and the generalised logistic regression are virtually the same (0.444 vs. 0.439).\nUnder this model, the slowdown of Arctic sea ice retreat shouldn’t be surprising.\nAt least, it doesn’t seem to be some extraordinary feature that “call for an explanation in terms of physical processes”, as Stern (2025) puts it.\nIt’s not a fundamentally different state, just the expected curve of sea ice decline.\n\n\n\nWhat does seem to require explanation is the fact that the rate of sea ice extent is slowing down far from zero.\nThe yearly mean logistic model predicts a right asymptote of about 10.3640052 million squared kilometres.\nThere is still much ice to be melted away, so why are we slowing down?\nOne (admittedly hand-wavy) explanation is that our simple model is just too simple and that other processes act to stabilise sea ice far from zero.\nFor example, the model doesn’t account for ice thickness and age.\nOlder and thicker ice is much harder to melt so it could act as a sort of “buffer”.\nIt also doesn’t have a seasonal cycle.\nEven if temperatures increase, they will be below freezing during the winter, creating some sea ice that might survive until the summer in some areas if the response time of sea ice to temperature is slow enough.\nIt’s the temperature, stupid\nThe authors state in their discussion that\n\nThe fact that September Arctic SIE shows no trend during 2007–2024 may at first seem hard to explain.\nThe Earth continues to warm, and the Arctic is warming faster than the global average.\n\nHowever, Figure 11 shows annual mean zonal mean temperature at different latitudes.\nEquatorward of 60° or so, the temperature increase has been fairly linear, but in the higher latitudes it also follows an S-shape function that hasn’t gone below freezing in most of the polar regions.\n\n\n\n\nFigure 11: Zonal mean 2-metre temperature at different latitudes plus a generalised logistic fit for latitudes higher than 60° and a linear fit for latitudes lower than 60°.\n\n\n\n\n\n\n\n\n\nI fitted a generalised logistic fit north of 60° and a linear fit south everywhere else and used that as the “forcing temperature” in my simple toy sea ice model.\nThe result is that, sea ice decline slows down and remains relatively stable well above zero ice.\n\n\n\n\nFigure 12: Mean sea ice concentrations form Equation 8 but forced with temperature following the functions fitted in 11.\n\n\n\nSo has Arctic sea ice decline decelerated in recent years simply because temperature increase has decelerated in the Arctic?\nMaybe!\nSome evidence for this hypothesis is that even a simple linear regression model of zonal mean September sea ice concentration with zonal mean annual mean temperature at each latitude predicts the observed slowdown (Fig. 13).\n\n\n\n\nFigure 13: Observed mean September sea ice concentration in green and mean September sea ice concentration predicted from a linear model on zonal mean 2-metre temperature at each latitude.\n\n\n\nConclusions\nThe evidence for a slowdown is much weaker than advertised.\nThe statistics are simply not robust and depends on the specific month and dataset used (I haven’t explored it here, but it also breaks down a bit when using area instead of extent).\nAlso, we need to be extremely careful of making sweeping conclusions based on short and cherry-picked periods.\nWe don’t want a repeat of the whole “hiatus” debacle.\nBy the way, this preprint (which I came across to just as I was finishing this post), shows that periods of lower melt are not uncommon in forced simulations.\nThis suggests that just normal natural variability superimposed on a long term trend will look like a “slowdown” (England et al. 2025).\nTo the extent that there has been a slowdown, it doesn’t necessarily represent a fundamental shift in Arctic sea ice behaviour.\nThere are statistical and physical reasons not to expect sea ice to follow a constant rate of decline even in an environment of linear temperature increase.\nWe know that the linear model is fundamentally misspecified from the start.\nAlternative non-linear models naturally have different “regimes” but they don’t necessarily “call for explanation in terms of physical processes”.\nIndependent of all these arguments, the observed slowdown could be simply explained by slower temperature increase in the northern polar regions in the recent decade.\nThis could easily be due to internal variability and not a meaningful change in the trend (again, let’s learn from the “hiatus”) experience.\n\n\n\nEngland, Mark, Lorenzo M Polvani, James A Screen, and Anthony ChunYin Chan. 2025. “Surprising, but Not Unexpected, Multi-Decadal Pause in Arctic Sea Ice Loss.” http://dx.doi.org/10.22541/essoar.174329135.56312606/v1.\n\n\nStern, Harry L. 2025. “Regime Shift in Arctic Ocean Sea-Ice Extent.” Geophysical Research Letters 52 (8). https://doi.org/10.1029/2024gl114546.\n\n\n\n\n",
    "preview": "posts/2025-04-01-arctic_regime/img/fig1.jpg",
    "last_modified": "2025-05-06T15:57:22+10:00",
    "input_file": "arctic_regime.knit.md"
  },
  {
    "path": "posts/2025-03-11-qbo-ceof/",
    "title": "QBO using complex Empirical Orthogonal Functions",
    "description": {},
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2025-03-18",
    "categories": [
      "ceof",
      "qbo"
    ],
    "contents": "\n\n\n\n\n\n\n\n\n\nEvery 14 months or so the 30hPa mean zonal wind between 5ºS and 5ºN goes from easterly to westerly.\nThis oscillation is so clear and regular that it can be clearly seen just in the raw data (Fig. @ref(fig¨:u-timeseries).\n\n\n\n\nFigure 1: (ref:u-timeseries-cap)\n\n\n\nThis is not just an oscillation, though.\nThe positive and negative anomalies propagate from the upper stratosphere to the lower stratosphere, although this effect is better appreciated by looking at the zonal wind anomalies (Fig. 2).\n\n\n\n\nFigure 2: (ref:hov-anomalies-cap)\n\n\n\nThis very conspicuous mode is called the Quasi-Biennial Oscillation, or QBO.\nGiven that this is a propagating mode in principle it’s not enough to use a single time series to represent it, although it’s very common to read about “Easterly QBO” and “Westerly QBO” phasees.\nOne option is to combine two EOFS, similar to what the Bureau of Meteorology does with the Madden-Julian Oscillation.\nSome researchers are doing this (Wallace, Panetta, and Estberg 1993), although I don’t know how popular is this approach (the dataset in that webpage hasn’t been updated since 2011).\nBut, given that it’s a propagating mode, it might be interesting to use complex EOFs to build an index (also like the Madden-Julien Oscillation).\nOne article from two years ago proposed this idea (Xu and Ren 2022) and it looks promising.\nAlthough from the start I have to admit that the QBO is so clear in observations that I don’t expect regular EOFs to have that much trouble and that the results are going to be very similar.\nFirst of all there’s two ways of computing the QBO with complex EOF: I could compute them by level or by time.\nThe results are not terribly dissimilar, but I believe the edge effects of the required Fourier transform are going to be worse when computing in “level space”.\nI also don’t know how the irregular vertical grid would affect the results.\nIn any case, for this post I’m computing the complex EOF by time, which is what Xu and Ren (2022) also did.\nI’m not doing any weighting (although I should).\nI’m also rotating and reflecting the complex axis to make the results comparable to the regular EOF results.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: (ref:vertical-cap)\n\n\n\nFigure 3 compares the complex EOF representation of the QBO with the “two EOF” version.\nEach line represents the two orthogonal phases whose linear combination is the complete wave.\nA more intuitive illustration is Figure 4, which shows the shape of the reconstructed wind anomalies at each phase of the QBO.\n\n\n\n\nFigure 4: (ref:phases-cap)\n\n\n\nThis makes it more clear how these indices can represent this propagating mode.\nIn the 360º/0º phase, winds are westerly with a maximum at around 5hPa and the easterly anomalies are starting to rear their head at the top of the stratosphere.\nAs the phase advances clockwise, both the westerly and the easterly anomalies propagate downward.\nThe westerly winds get to the 30hPa level at around the 240º phase, which would indicate the maximum of the “westerly QBO”.\nThe maximum of the “easterly QBO” is around the 60º phase.\nThe only noticeable difference between the methods is that the EOF-based QBO signal seems to have more amplitude in the upper stratosphere.\nThis is more evident by looking at the QBO amplitude as shown in Figure 5.\nThis is defined as the modulo of the complex EOF and as \\(\\sqrt{PC1^2 + PC2^2}\\) in the normal EOF case.\n\n\n\n\nFigure 5: (ref:amplitude-cap)\n\n\n\nThe amplitude can also be computed in the time series.\nThis is something that can’t really be done using an univariate index and which tells us how strong the QBO was at a particular point in time.\n\n\n\n\n\n\n\nFigure 6: (ref:amplitude-clim-cap)\n\n\n\nSo, for example, we can look at the seasonality of the QBO amplitude.\nIs the QBO more active in particular months of the year?\nFigure @Seasonal cycle of the time amplitude of the complex EOF and normal EOF. shows that the answer depends on he index.\nThe complex EOF amplitude has a moderate seasonality with maximum amplitude between March and June with a secondary maximum in October.\nIn the normal EOF, the seasonality is much weaker compared with the interannual variability, which is particularly larger fo June–August\n\n\n\n\nFigure 7: (ref:amplitude-time-cap)\n\n\n\nLooking at the full time series, Figure 7 plots the standardised amplitude anomalies computed with each method as described above.\nThe long term trend (or lack thereof) is very similar with both methods, as is the overall variability.\nThere are some differences.\nIn 2015–2016 the QBO was disrupted, with easterly winds suddenly replacing the expected westerlies(Newman et al. 2016).\nThis event is very well captured by the complex EOF amplitude, showing one of the lowest values on record.\nThe EOF-based amplitude, on the other hand, doesn’t seem to capture it.\nThe most striking feature of this plot seem to be that dip in the 50s and 60s, which is much more pronounced in the complex EOF index.\nNot only that, but there’s something strange going on with the EOF-based index around 1954, where the amplitude jumps up to almost record levels while the complex EOF index stays at the general low levels of that decade.\nWe can check what was going on in that decade looking at the actual zonal wind anomalies (Fig. 8).\n\n\n\n\nFigure 8: (ref:wind-anomalies-zoom-cap)\n\n\n\nIt looks like in the 50s and 60s the usual vertical propagation of wind anomalies was severely disturbed.\nInstead of vertical propagation, there’s almost a dipole of positive and negative wind anomalies between the upper and lowe stratosphere.\nLooking at the broader picture, it also looks like the QBO was faster in the 40s then in the modern period.\nAs it’s always the case with these data, it’s very possible that this is a problem with the reanalysis.\nI don’t think there were a lot of good stratospheric observations in those years and from what I know, models are not great at simulating the QBO.\nSomeone with more expertise in tropical meteorology could probably interpret this better.\nWhether the changes in the 40s, 50s and 60s are real or not, they are real in the data.\nThe complex EOF reconstruction looks like Figure 9.\n\n\n\n\nFigure 9: (ref:ceof-reconstruction-cap)\n\n\n\nDuring the “tumultuous” period, the complex EOF index detects almost no QBO (something that Figure 7 already shows).\n\n\n\n\nFigure 10: (ref:eof-reconstruction-cap)\n\n\n\nThe reconstruction using EOF (Fig. 10) is quite different.\nDuring the tumultuous period there is also a decrease in amplitude, but we can see that weird spike in amplitude around 1954 as a very strong dipole that then disappears.\nThis is clearly not a downward-propagating pattern, but the EOF-based analysis captured it as a very strong QBO event anyway.\nMore generally, the overall patterns look a bit less organised –more “noisy”– than the complex EOF reconstruction.\nThe reason the complex EOF “recognises” that the dipole is not the QBO is the time complex transformation.\nThe problem with that is that it need information “from the future” to know that, so that it’s challenging to create an operational index based con complex EOF in the time domain.\nWe can look at the phase evolution of the QBO.\n\n\n\n\nFigure 11: (ref:trajectories-cap)\n\n\n\nFigure 11 plots the trajectories of each part of the complex EOF and the PC1–PC2 space in the normal EOF.\nIt’s hard to get anything from this figure, really, but to me the complex EOF does look more tidy.\nFinally we can look at the periodicity of the QBO using these indices.\n\n\n\n\nFigure 12: (ref:period-cap)\n\n\n\nIn Figure 12 I’m computing the periodogram of the complex-valued EOF, which I constructed as \\(PC1 + iPC2\\) for the regular EOF.\nThe complex EOF period is very well defined at around 28 months.\nThe EOF period, on the other hand, is kind of a mess.\nI do get a better behaved period if I compute the periodogram of each individual EOF like in Figure 13, but even then the complex EOF period is a bit more sharp.\nI don’t know what it would mean to have two periodograms for the QBO, though.\n\n\n\n\nFigure 13: (ref:period2-cap)\n\n\n\nI started this analysis not expecting to find much if any difference between the two methods, but I was wrong.\nThe complex EOF analysis has some interesting good properties for studying the QBO.\nThere is one caveat, though.\nThe reason that the complex EOF is able to detect those disruptions in 2015 and in the 50s is because by computing the Hilbert transform in time, we are essentially combining information from the future.\nBut that means that this method cannot be used as an operational index. **sad trombone**\n\n\n\nNewman, P. A., L. Coy, S. Pawson, and L. R. Lait. 2016. “The Anomalous Change in the QBO in 20152016.” Geophysical Research Letters 43 (16): 8791–97. https://doi.org/10.1002/2016gl070373.\n\n\nWallace, John M., R. Lee Panetta, and Jerry Estberg. 1993. “Representation of the Equatorial Stratospheric Quasi-Biennial Oscillation in EOF Phase Space.” Journal of Atmospheric Sciences 50 (12): 1751–62. https://doi.org/10.1175/1520-0469(1993)050<1751:ROTESQ>2.0.CO;2.\n\n\nXu, Wenwen, and Hong-Li Ren. 2022. “A CEOF-Based Method for Measuring Amplitude and Phase Properties of the QBO.” Climate Dynamics 61 (1-2): 923–37. https://doi.org/10.1007/s00382-022-06625-2.\n\n\n\n\n",
    "preview": "posts/2025-03-11-qbo-ceof/qbo-ceof_files/figure-html5/u-timeseries-1.png",
    "last_modified": "2025-04-09T13:09:20+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-04-18-MJO-RMM/",
    "title": "An index for the MJO using complex Empirical Orthogonal Functions",
    "description": "An early exploration of using cEOF to characterise the Maden-Julian Oscillation.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2024-04-29",
    "categories": [
      "ceof",
      "mjo"
    ],
    "contents": "\nThe Madden-Julian Oscillation is a tropical oscillation located mainly over the Indian and western Pacific oceans.\nIt’s not a standing oscillation, but instead it’s more like a propagating wave of enhanced and reduced convection that moves eastward.\n\n\n\n\nFigure 1: MJO schematic. From https://www.climate.gov/news-features/blogs/enso/what-mjo-and-why-do-we-care.\n\n\n\nDue to its propagating nature, it cannot be reproduced by a single EOF so MJO indices use two EFOs.\nThe Real-time Multivariate MJO (RMM) index is made up of the first leading EOFs of Outgoing Longwave Radiation, 200hPa zonal wind and 850 hPa zonal wind anomalies in the tropics.\nThey do some filtering of the time series to remove the seasonal cycle, short-scale fluctuations and the impact of El Niño-Southern Oscillation too.\nI think complex Empirical Orthogonal Functions (Horel 1984) might be a great fit for this kind of index, because they can naturally represent propagating patterns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe methods used to derive the RMM index are listed in Wheeler and Hendon (2004).\nIn short they are, for each variable averaged between 15ºS and 15ºN,\nremove the annual cycle, estimated as the waves 0 through 3 of daily means,\nremove the linear effect of ENSO using the monthly ONI interpolated to daily values,\nremove the 120 day running mean, and\nnormalise by the global standard deviation.\nHere I reproduce the method to the best of my ability.\nThe only difference is that, due to what data easily available to me, I will be using the 1994–2024 period instead of the 1979–2001 to define the annual cycle, remove the (linear) effect of ENSO and compute the EOFs.\nThe dataset has a few missing values, which I impute using DINEOF (Alvera-Azcárate et al. 2011).\n\n\n\nTo compute the cEOF, an extra fifth step is to “enrich” the original data by applying the Hilbert transform.\nIn the literature they usually apply this step in the time domain: considering the signal as an oscillation in time.\nIn this case, because this is a longitudinally-propagating wave, I’m computing this in the zonal domain: considering the signal as an oscillation in space.\nThe same way that EOFs are only defined up to a change in sign, cEOFs are only defined up to a rotation in the complex plane.\nAny rotation is equally “real” but to make it comparable with the RMM index, I will rotate the leading cEOF so that both indices are maximally correlated.\nTo compute the correlation between two bivariate indices I treat them as vectors, and compute their correlation as the mean cosine of the difference between their phases weighted by the product of their amplitudes.\nInstead of labelling each component as the “Real” and “Imaginary” part, I use the angle between each and the positive real line.\nSo the real part is the 0º phase and the imaginary part is the 90º phase.\nHere, the RMM1 index is aligned with the 0º phase and the RMM2 with the 90º phase.\n\n\n\n\n\n\nThe spatial pattern of the cEOF is shown in Figure 2, in which the map is shown only for reference, the vertical coordinates are arbitrary and the dark band indicates the area in which the variables were averaged.\n\n\n\n\nFigure 2: Spatial patterns of the leading cEOF of OLR, 850 hPa zonal wind and 200 hPa zonal wind anomalies.\n\n\n\nCompare this figure with Wheeler and Hendon (2004)’s Figure 1.\nThe MJO in its 0º phase is characterised by increased convection over Indonesia (around 120ºE), which is evident by the OLR minimum, convergence at the lower levels (negative slope in the 850 hPa zonal wind) and divergence at upper levels (positive slope in the 200 hPa zonal wind) and reduced convection in the western Indian ocean and Africa as evidenced by the inverse signal.\nThis is equivalent to the phases 4 and 5 in the RMM diagram.\nIn its 90º phase, the enhanced convection is over the Indian ocean, with drier conditions east of Indonesia.\nThe amplitude of the signal, particularly for OLR, is maximum over the Indian ocean and Indonesia, with little to no signal in the eastern Pacific, South America and the Atlantic Ocean (Fig. 3).\n\n\n\n\nFigure 3: Amplitude of the OLR component of the cEOF.\n\n\n\nA buttery smooth and oddly satisfying animation of the evolution of the cEOF shows how the wet and dry sections travel around the tropics (Fig. 4).\n\n\n\n\nFigure 4: Animation showing all the phases of the cEOF.\n\n\n\nThe correlation between the RMM index and the cEOF is 0.96, so they are essentially the same indices.\nFigure 5 show the trajectory of the two indices between March 6th 2024 and March 31st 2024 with arrows showing the difference.\n\n\n\n\n\n\n\nFigure 5: Sample trajectory in the RMM/cEOF phase space between March 6th 2024 and March 31st 2024. Black arrows indicate the difference between the two indices.\n\n\n\nThey are almost identical up to an arbitrary scale factor.\nThis scale factor comes up because the RMM1 is scaled so that each component has unit standard deviation in the climatological period, but I scaled the cEOF so that its amplitude has unit standard deviation1.\n\n\n\nThe arbitrary constant makes it hard to compare the amplitude of each index.\nThe BOM uses amplitude equal to 1 to more or less define when the MJO is active, but that same cut-off is not necessarily useful for the cEOF index.\nThere are a few ways of getting an equivalent threshold for the cEOF index.\nRun a linear regression of the cEOF amplitude as a function of RMM amplitude and use the cEOF amplitude that corresponds to 1 RMM amplitude.\nDo the same but using orthogonal regression, which might be more appropriate in this case because this procedure should be symmetrical.\nCompute the quantile corresponding to the RMM threshold and use the value of that quantile in the cEOF amplitude series.\nThankfully the three approaches give you basically the same answer (this might not be a coincidence), but I kind of like the 3rd one better.\nFor the record, the quantile associated with 1 RMM amplitude is 0.38, which translates to a threshhold value of 0.69 in cEOF amplitude.\nFigure 6 shows the relationship between the amplitude of each index and a line of best fit using orthogonal regression for each month and the variance explained by the line (this is actually the first Principal Component of the two series).\n\n\n\n\n\n\n\nFigure 6: Relationship between RMM amplitude and cEOF amplitude for each month. The blue line shows the orthogonal regression line, whose explained variance is shown as text, and the horizonal and vertical black lines indicate the 0.38 quantile of each series (computed for the whole period and not for each month).\n\n\n\nAlthough both variables are clearly highly correlated, there are some differences which make a sizeable proportion of days “active” by the cEOF definition but not “active” by the RMM definition and vice versa, especially in the boreal winter months.\nAn important characteristic of the MJO is its intraseasonal timescale of between 30 and 80 days.\nWheeler and Hendon (2004) computes the spectra of each RMM index, but with I can just compute the spectrum of the complex cEOF.\nThe three spectra are very similar (Fig. 7).\n\n\n\n\nFigure 7: Smoothed power spectra of the RMM1, RMM2 and cEOF indices scaled to unit area.\n\n\n\nThe main difference is that the cEOF spectrum is a bit more concentrated in the intraseasonal range and doesn’t have that “bump”around 200 days that is visible in both the RMM1 and RMM2 indices.\nI don’t know if that’s something to do with the method or with the change in climatological period.\nIn any case, I think this also highlights one advantage of treating a bivariate index as a complex signal, since it allows you to study a single spectrum for the whole signal.\n\n\n\n\n\n\n\n\n\nFinally, Figure 8 shows the regression between OLR and the MJO using three different methods.\nBoth the “RMM”and “cEOF”panels are the regression of OLR anomalies with the respective index amplitude for each “pizza slice” phase (i.e. the 1 phase encompases phases from -180º to -135º).\nA problem with this method is that each panel discards a lot of information since only around 12% of observations fall into each slice.\nThe “cEOF - linear” panels are obtained by first computing the multivariate linear regression of OLR with the 0º and 90º phases of the complex series and then using a linear combination to compute the regression associated with any other phase.\nThis method assumes that the relationship between OLR and the MJO is linear in every phase and that the total effect of the MJO can be linearly divided into the effect of two orthogonal phases (i.e. that the 0º phase is equal an oposite to the 180º phase and that the 45º phase is the combined effect of the 0º phase and the 90º phase in equal measure).\nThe big advantage of this method is that it uses al the information available.\n\n\n\n\nFigure 8: Linear regression between each index and OLR for different phases. The “pizza slice” used to construct each panel or the phase represented by each panel is shown by the small inset in the lower-left corner of each panel.\n\n\n\nAll three methods give more or less the same result that are (naturally) consistent with the cEOF description above; the “wet patch” moves east, reaching its maximum intensity over the Indian ocean and then being replaced by a “dry patch” with a similar behaviour.\nThere is some differences in the small details, but it’s not trivial to know how much of that-small scale structure is just sampling noise.\nThe linear method has the advantage of resulting in smoother patterns with less noise and more large-scale signal.\nOn the other hand, the linear method seems to exaggerate the intensity of the dry patch in phases 6-7, since this has to be equal and opposite to the wet patch in phases 2-3 by construction.\nThe linear method also creates buttery smooth and oddly satisfying animations.\n\n\n\n\nFigure 9: Animation of OLR regression on the difference phases of the cEOF.\n\n\n\n\n\n\nAlvera-Azcárate, A., A. Barth, D. Sirjacobs, F. Lenartz, and J. M. Beckers. 2011. “Data Interpolating Empirical Orthogonal Functions (DINEOF): A Tool for Geophysical Data Analyses.” Mediterranean Marine Science 12 (3): 5. https://doi.org/10.12681/mms.64.\n\n\nHorel, J. D. 1984. “Complex Principal Component Analysis: Theory and Examples.” Journal of Applied Meteorology and Climatology 23 (12): 1660–73. https://doi.org/10.1175/1520-0450(1984)023<1660:CPCATA>2.0.CO;2.\n\n\nWheeler, Matthew C., and Harry H. Hendon. 2004. “An All-Season Real-Time Multivariate MJO Index: Development of an Index for Monitoring and Prediction.” Monthly Weather Review 132 (8): 1917–32. https://doi.org/10.1175/1520-0493(2004)132<1917:aarmmi>2.0.co;2.\n\n\nThe variance can be generalised to complex signals naturally by considering it as the mean squared distance between each point and the average point.↩︎\n",
    "preview": "posts/2024-04-18-MJO-RMM/img/mjo.png",
    "last_modified": "2024-05-03T11:29:21+10:00",
    "input_file": {},
    "preview_width": 1000,
    "preview_height": 677
  },
  {
    "path": "posts/2023-06-01-stop-composites/",
    "title": "Stop using composites",
    "description": "Composites are bad. Use regressions instead.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2023-06-02",
    "categories": [
      "statistics"
    ],
    "contents": "\n\n\n\nComposites are a very common method used in atmospheric.\nTake a continuous index, discretise it into two categories defined by some threshold and then compute the mean anomaly for each category.\nMany researchers use this to show the “typical” expected value for “events”, such as the effect of El Niño events on precipitation.\nSometimes it’s also used to try to find non-linear effects by comparing composites for positive and negative events (e.g. El Niño and La Niña events).\nFigure 1 is an example.\nIt shows composites of wintertime precipitation anomalies for El Niño and La Niña years, defined as winters with mean Sea Surface Temperatures in the Niño 3.4 region (that’s the ONI index) than 0.5 or lower than -0.5 respectively.\n\n\n\n\n\nFigure 1: Observed (1979 – 2019) austral winter composites of precipitation anomalies for La Niña and El Niño winters.\n\n\n\nThey seem to indicate that precipitation anomalies associated with La Niña are somewhat waaker and less extensive.\nThis would suggest a non-linear effect in which the effect of negative Niño events is not the same as the negative of the positive Niño events.\nHowever, this is an illusion.\nI am not a fan of composites for three reasons:\nBy selecting “events” using a threshold, composites discard a lot of perfectly good data.\nEstimation of non-linear effects is much harder and noisier than of linear effects, so you need a lot of data. But we usually don’t have as much data as we want and, even then, this method is not data-efficient (related to 1).\nResults can be very sensitive to the chosen thresholds.\nThe mean value of a variable conditioned on an event depends on the mean intensity of the events.\nComposites are data-inefficient\nTable 1 which shows the number of years in each phase.\n\n\nTable 1: Number of years in each phase of ENSO\n  Phase\n      N\n    Neutral\n29El Niño\n6La Niña\n6\n\nThe original wintertime ONI index has 41 data points (years), but only selecting years that are larger than 0.5 or lower than -0.5 throws away more than 2/3 of the data.\nThis is terribly data-inefficient.\nThe composites of El Niño and La Niña years in Figure 1 are based on only 6 years each.\nThis is extremely tiny data, which translates to huge uncertainty in the computed means.\nTo illustrate this, Figure 2 plots maps of the lower and upper values for the 95% confidence intervals from a t-test.\nAccording to these composites, both El Niño and La Niña are compatible with zero signal in the precipitation in the central Pacific.\n\n\n\n\n\nFigure 2: Lower and upper values for 95% confidence intervals from a t-test comparing the mean precipitation anomaly in El Niño and La Niña years.\n\n\n\nNon-linear effects are harder to estimate than linear effects\nIf these composites have a hard time detecting the (gigantic) zero-order effect of El Niño, even less can they detect any non-linear effects.\nIndeed, the differences between the composites are not statistically significant.\nFigure 3 shows the result of a t-test on precipitation anomalies during El Niño years and negative precipitation anomalies during La Niña years to test if the effect of El Niño is not significantly different from the negative effect of La Niña.\nThe differences are barely significant at very few locations and the significance completely vanishes if one controls for multiple comparisons (Walker 1914; Wilks 2016).\n\n\n\n\n\nFigure 3: Difference between the mean precipitation anomaly in El Niño winters and the negative precipitation anomaly in La Niña winters. Crosses indicate raw p-values lower than 0.01.\n\n\n\nThis indicates that, using composites, precipitation anomalies during El Niño are almost indistinguishable to precipitation anomalies during La Niña reversed in sign.\nComposites depend on an arbitrarily-chosen threshold\nThe ±0.5 threshold used to define El Niño and La Niña years is arbitrary.\nThere’s no reason to think that the atmosphere cares that much if the ONI is 0.45 vs. 0.55.\nFigure 4 shows the same composites as Figure 1 but using ±0.1 instead of ±0.5 to define the phases.\nThese composites seem to suggest that precipitation anomalies are actually more sensitive to La Niña than to El Niño.\n\n\n\n\n\nFigure 4: Same as Figure 1 but for El Niño and La Niña defined using a ±0.1 threshold.\n\n\n\nChanging the threshold to ±0.3, however, now suggests no difference (Fig. 5).\n\n\n\n\n\nFigure 5: Same as Figure 1 but for El Niño and La Niña defined using a ±0.3 thershold.\n\n\n\nComposites don’t actually show the relationsip between variables\nTechnically what these plots are showing are, for each cell in the map, the expected value of precipitation anomalies conditioned on the ONI index falling into the El Niño or La Niña categories.\nCrucially, it doesn’t tell us how much precipitation increases or decreases when the ONI increases or decreases by 1 unit.\nThis is shown in cartoon form in Figure 6.\n\n\n\n\n\nFigure 6: Sketch showing a hypothetical linear relationship between ONI and precipitation anomalies at some point and the values of the mean precipitation anomalies for year with ONI greater than 0.5 or lower than -0.5.\n\n\n\nIn this fake example, the ONI index has larger mean magnitude in El Niño years than in La Niña years.\nIn other words, the typical El Niño is stronger than the typical La Niña.\nSo even though the relationship between precipitation and the ONI index is linear, the typical precipitation anomaly during El Niño is stronger than the typical precipitation anomaly during La Niña.\nTable 2 shows the mean mean magnitude of the ONI index during El Niño and La Niña years.\nEvidently the ONI index has greater mean magnitude during El Niño than La Niña.\nTherefore, composites using La Niña years will have smaller values (in magnitude) than composites using El Niño years even if the relationship were linear.\n\n\nTable 2: Mean ONI value for each phase. \n  Phase\n      ONI\n    El Niño\n1.16La Niña\n−0.89\n\nFigure 7 illustrates this effect on fake composites.\nHere I’ve created fake precipitation fields so that they have a perfectly linear relationship with the ONI index.\nHowever, the composite of La Niña years shows weaker anomalies than the composites of the El Niño years.\nIn other words, these plots suggest a non-linear effect even though the relationship is linear by construction.\n\n\n\n\n\n\n\n\nFigure 7: Same as Figure 1 but from synthetic precipitation anomalies that are perfectly linearly related with ONI by construction.\n\n\n\nThis problem can be corrected by normalising the composites by the absolute value of the mean ONI for each phase, like I did in Figure 8 using real precipitation fields.\nIn this figure, the map show the expected increase or decrease in precipitation due to a unit change in the ONI index in La Niña or El Niño years and suggest that precipitation anomalies are not more sensible to El Niño than La Niña.\n\n\n\n\n\nFigure 8: Same as Figure 1 but the composites are divided by the absolute value of the mean ONI in each phase.\n\n\n\nUse piecewise regression instead\nComposites throw away a lot of precious and perfectly good data, can’t actually detect non-linear effects unless they are really strong, and don’t actually show the relationship between the variables. So what’s the alternative?\nAn alternative is to compute simple linear regressions or correlations using all the data and don’t bother with trying to find non-linearities in relatively small datasets.\nThe sooner we accept that in many cases we only (barely) have enough data to estimate first-order effects, the better.\nBut if you really want to look for non-linearity, then fit a piecewise regression with the breakpoint at zero.\nThis will give you a slope for negative values and a slope for positive values.\nEach slope will be computed using all the (positive or negative) values and the line will be continuous at zero.\nSo, in this case, piecewise regression will give you an estimate of the change in precipitation when the ONI changes in one unit for positive values (~ El Niño) and negative values (~ La Niña) without discarding precious data, depending on arbitrarily chosen thresholds, nor suffering from the mean value issue.\nPiecewise regression is also not hard to compute when the breakpoint is fixed, like in this example.\nYou just need to fit the linear regression:\n\\[\npp \\sim ONI + (ONI - ONI_0)\\times I_{ONI\\le ONI_0}\n\\]\nWhere, \\(ONI_0\\) is the breakpoint value (0, in this case) and \\(I_{ONI\\le ONI_0}\\) is an indicator function that is 1 when \\(ONI\\le ONI_0\\) and 0 when is not (this translates simply to ONI <= ONI0 in any statistical software).\nUsing this formulation, the first regression coefficient corresponds to positive ONI and the second regression coefficient corresponds to the difference between positive ONI and negative ONI.\nThis means that piecewise regression directly computes the statistical significance of the difference between the slopes, providing you with a number to detect those pesky non-linear effects.\n(Although, again, you probably don’t have enough data to detect them and, since piecewise regression fits more parameters, estimates will be noisier than simple linear regression estimates; you really need to look out for type S and type M errors.)\nFigure 9 shows the result of piecewise regression applied to the relationship between ONI and precipitation (in that plot, I’ve flipped the sign of the La Niña values to match the composites).\n\n\n\n\n\nFigure 9: Linear regression slopes for positive and negative ONI values using a piecewise linear model with fixed breakpoint at ONI = 0. Crosses indicate raw p-values lower than 0.01 (yep, there’s only one).\n\n\n\nThis figure shows that the effect of La Niña on precipitation is of similar magnitude to the effect of El Niño.\nThe difference between the two signs of ONI is basially not significant even before ajustinf for multiple comparisons.\ntl;dr\nSo, composites throw away perfectly good data, can’t detect non-linear effects reliably, are not always robust to threshold choice and don’t actually show the relationship between the variables.\nDon’t use them.\nIn most cases, it’s better to use simple linear regressions (or correlations) to study first-order linear effects, as you probably don’t have enough data to reliably detect non-linearities. I\nIf you really want to check for non-linear effects, piecewise regression uses all the data and will even provide statistical significance of the difference.\n\n\n\nWalker, Sir Gilbert Thomas. 1914. Correlation in Seasonal Variations of Weather, III: On the Criterion for the Reality of Relationships Or Periodicities. Meteorological Office.\n\n\nWilks, D. S. 2016. “‘The Stippling Shows Statistically Significant Grid Points’: How Research Results Are Routinely Overstated and Overinterpreted, and What to Do about It.” Bulletin of the American Meteorological Society 97 (12): 2263–73. https://doi.org/10.1175/BAMS-D-15-00267.1.\n\n\n\n\n",
    "preview": "posts/2023-06-01-stop-composites/stop-composites_files/figure-html5/composites1-1.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 864
  },
  {
    "path": "posts/2022-09-20-uwind-sinewave/",
    "title": "Mini replication of \"Subseasonal Vacillations in the [boreal] Winter Stratosphere\"",
    "description": "Fun fitting a sine model",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2022-09-20",
    "categories": [
      "stratospheric circulation"
    ],
    "contents": "\nLast week on the Journal Club I participate in we read Subseasonal Vacillations in the [boreal] Winter Stratosphere (Hardiman et al. (2020)).\nThe data and methods were not too complicated and I had some small issues with some of the decisions so I was delighted to once again delve into a paper and try to reproduce it.\nThe underlining theory is that, due to the interaction between the zonal mean flow and the zonal waves, the stratospheric jet “vacillates” (d.h. oscillates) in a relatively regular way.\nFrom that, the authors posit that a simple sine wave model could predict the jet strength.\nThe conclusions are not very strong in that while the sine wave model is a relatively better fit than a linear model, it doesn’t have increased predictive power.\nHere I’ll replicate these methods with some minor changes and explore the results.\n\n\n\nFirst, I downloaded 00:00 UTC zonal wind hourly data from ERA5 at 10 hPa between between 55ºN and 65ºN.\nFigure 1 shows the timeseries, which has a very strong seasonal cycle.\n\n\n\n\nFigure 1: Daily zonal mean zonal wind at 10 hPa, averaged between 55ºN and 65ºN.\n\n\n\nTo remove the seasonal cycle, Hardiman et al. (2020) removed the climatological mean following MacLachlan et al. (2015):\n\nUsing the climatological (i.e., average across all years) daily U data, a rolling 61 day mean is formed (i.e., ±30 days), weighted with the function\n\\[w=\\mathrm e^{\\frac{-d^2}{100}}\\]\nwhere d is the lag/lead in days from the central day (i.e., an exponential decay on a time scale of 10 days). This climatological daily rolling mean is then removed from the daily winds for each year, producing the anomalous daily wind field.\n\nThe idea here is that the seasonal cycle computed by just the daily averages can be noisy due to sampling variability.\nSo they smooth it out with a weighted rolling mean function.\nThis might be fine, but I’m usually not a fan of rolling mean for this because it can have issues at the boundaries.\nNot only the mean is computed from fewer datapoints, but there is no assurance that the annual cycle to be periodic (the 31st of December needs to be almost equal to the 1st of January).\nMy preferred solution is to use Fourier (I dare you to find a problem that’s not solved with Fourier).\nJust filter out the higher wavenumbers to get a smooth and periodic seasonal cycle.\nIn this case, I filtered out wavenumbers greater than 3.\n\n\n\n\n\n\n\nFigure 2: Seasonal cycle of zonal mean zonal wind at 10 hPa, averaged between 55ºN and 65ºN. Raw cycle in read and a smoothed version in blue.\n\n\n\nAs you can see in Figure 2, the raw climatology is not super noisy (I’ve seen noisier) but the smoothed-out version is much better.\nThe difference is specially important in the boreal Winter months, which is the ones we are interested in!\nThe choice of maximum wavenumber does change the smooth slightly (of course).\nI chose 3 just by eye, but I suspect one could get to a more objective and automatic number using cross-validation (I dare you to find a problem that’s not solved with cross-validation).\n\n\n\nNext, after removing this smooth climatology, Hardiman et al. (2020) fits a sine wave model to each year’s November-March period.\nInitially they fit the model\n\\[\nU = \\bar{U} + A\\sin(\\mathit{freq}(\\mathit{time} + \\mathit{phase}))\n\\]\nin which \\(\\bar{U}\\) is the mean zonal wind anomaly of that winter (what they call “offset”), \\(A\\) is the amplitude of the vacillation, \\(\\mathit{freq}\\), its frequency and \\(\\mathir{phase}\\), its phase.\nAfter fitting this model, they then decide to opt for a simpler model with only \\(\\bar{U}\\) and \\(\\mathit{phase}\\) as free parameters and \\(A\\) and \\(\\mathit{freq}\\) fixed as \\(12\\ ms^{−1}\\) and 120 days respectively, which are approximatedly the mean values from the 4-parameter fit.\nI honestly don’t really get why they do this.\nWhile it’s important to be mindful of the fact that the goodness of fit improves monotonically with the number of parameters, I don’t see why 4 parameters is too much and 2 is fine.\nConsidering that each winter has between 151 and 152 observations (depending on the leap-ness of the year), 4 parameters doesn’t seem to be anywhere near the overfitting danger-zone.\nBesides, since they fix the two other parameters based on the same data, it’s not even technically true that they are only fitting 2 parameters!\nFrom a physical argument I would believe that fixing the frequency is desirable if one knows the approximate timescale of the phenomenon.\nBut the change amplitude is a fundamental aspect of the vacillations; the main idea of this process is that a weaker jet will have stronger oscillations.\nThey even test this with their 4-parameter fit.\nSo I don’t really understand why they decided to keep it fixed.\nI will ignore this parameter restriction and instead fit the full 4-parameter model.\nThe only restriction is that the maximum period I’m allowing is 300 days.\nGetting the sine wave fit was a bit of a challenge.\nAt first I was trying to use the nls() function, which performs Nonlinear Least Squares, but it not only errored out plenty of times due to numerical issues, but even when it ran without errors it would return nonsensical fits.\nIn the end, I decided to linearise the sine wave model for a fixed frequency, fit this linear model for a relatively large set of frequencies, and keep the model with the lowest Residual Sums of Squares.\nBelow is the R code I came up with, for which I’m not entirely proud, but also not completely ashamed.\n\n\nfit_sin <- function(x, time) {\n  t <- seq_along(x)\n  \n  # Try a bunch of periods and 120 (from the authors)\n  # (linear grid probably not the best)\n  periods <- 150*seq(1/10, 2, length.out = 200)\n  periods <- c(periods, 120)\n  frequencies <- 2*pi/periods\n  \n  models <- list()\n  for (k in seq_along(frequencies)) {\n    models[[k]] <- .lm.fit(cbind(1, cos(frequencies[k]*t), sin(frequencies[k]*t)), x)\n  }\n  \n  # Chose the one with lower RMSE\n  rmse <- vapply(models, function(model) sd(resid(model)), numeric(1))\n  model <- models[[which.min(rmse)]]\n  model120 <- models[[length(periods)]]\n  \n  # Return for the best and the 120-day model:\n  #   a model element with the fitted parameters\n  #   a fit element with fitted values, residuals and stuff. \n  resid <- resid(model)\n  resid120 <- resid(model120)\n  list(model = data.table::data.table(mean = coef(model)[1], \n                                      amplitude = sqrt(coef(model)[2]^2 + coef(model)[3]^2),\n                                      frequency = frequencies[which.min(rmse)],\n                                      period = periods[which.min(rmse)],\n                                      phase =  atan(coef(model)[3]/coef(model)[2])),\n       fit = data.table::data.table(\n         pred = x - resid,\n         resi = resid,\n         x = x,\n         t = seq_along(x),\n         time = time\n       ),\n       model120 = data.table::data.table(mean = coef(model120)[1], \n                                         amplitude = sqrt(coef(model120)[2]^2 + coef(model120)[3]^2),\n                                         frequency = frequencies[length(periods)],\n                                         period = periods[length(periods)],\n                                         phase =  atan(coef(model120)[3]/coef(model120)[2])),\n       fit120 = data.table::data.table(\n         pred = x - resid120,\n         resi = resid120,\n         x = x,\n         t = seq_along(x),\n         time = time\n       )\n       \n  )\n}\n\n\n\n\n\nFigure 3 shows the first nine years with their fit in blue.\nIn some years, such as 1979and 1981 the jet does show oscillations consistent with the sine wave fit.\nIn other years, like 1981 of 1985, the amplitude of the oscillation is minimal and not a very good fit.\n\n\n\nFigure 3: (ref:examples-cap)\n\n\n\nThe dotted line show the fit if the frequency is kept fixed at 120 days but the amplitude is allows to vary (so a 3-parameter fit).\nBoth lines feature a similar fit to the data since they only diverge when the amplitude of the wave is low.\nIn those year, the sinusoidal is more or less a straight line and thus the period makes little difference to the fit.\nCompared with Hardiman et al. (2020)’s Figure 2 reproduced below, their 2-parameter fit is not super bad for the years with reasonable oscillations, but it overfits strong vacillations in the “flat” years.\nThis is mainly due to the fixed amplitude, as discussed before.\n\n\n\n\nFigure 4: Same as Figure 3 but from Hardiman et al. (2020).\n\n\n\n\n\n\n\n\nFor completeness, Figure 5 shows all fits for all years.\nYou can sort the years chronologically or by the difference between the two fits.\nHonestly, the fixed-frequency model is not super bad except for a few cases.\nBut that’s not mind-blowing, since the fixed number was actually based on the full 4-parameter model.\n\n\n\n\nFigure 5: Same as Figure 3 but for all years. Ordered chronologically or by the difference between the two fits.\n\n\n\nWeave-mean flow interaction theory predicts that a weaker jet should be more wavy than a stronger jet.\nTherefore, there should be a negative correlation between the mean zonal wind zonal mean and the amplitude of the vacillations.\nHardiman et al. (2020) claim to have found this negative correlation:\n\n(…) across all years, the offset of these sine waves is negatively correlated to their amplitude (correlation coefficient =−0.36 […]).\n\nFigure 6 shows the relationship between the mean zonal wind and the amplitude in the 4-parameter model.\nThere seems to be a relationship, but is not as simple as the model predicts.\nIs not just a monotonic decrease in amplitude with increase in jet strength, but for some critical value of jet stength, the amplitude is relatively constant or even increasing; although there are not a lot of datapoints to make the relationship very clear.\n\n\n\nFigure 6: (ref:u-amplitude-cap)\n\n\n\n\n\n\nAs far as the correlation goes, the correlation between the mean zonal wind and the sine amplitude is -0.36 (CI: -0.60 – -0.06), which is consistent with Hardiman et al. (2020).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHardiman, S. C., A. A. Scaife, N. J. Dunstone, and L. Wang. 2020. “Subseasonal Vacillations in the Winter Stratosphere.” Geophysical Research Letters 47 (9): e2020GL087766. https://doi.org/https://doi.org/10.1029/2020GL087766.\n\n\nMacLachlan, C., A. Arribas, K. A. Peterson, A. Maidens, D. Fereday, A. A. Scaife, M. Gordon, et al. 2015. “Global Seasonal Forecast System Version 5 (GloSea5): A High-Resolution Seasonal Forecast System.” Quarterly Journal of the Royal Meteorological Society 141 (689): 1072–84. https://doi.org/https://doi.org/10.1002/qj.2396.\n\n\n\n\n",
    "preview": "posts/2022-09-20-uwind-sinewave/uwind-sinewave_files/figure-html5/timeseries-1.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-05-31-wave3-goyal/",
    "title": "Comment on \"A new zonal wave 3 index for the Southern Hemisphere\"",
    "description": "How to measure a wave that is not just a wave.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2022-06-02",
    "categories": [
      "wave 3",
      "general circulation"
    ],
    "contents": "\n\n\n\nMy PhD protect is (ostensibly) about the zonal wave 3 (ZW3) in the Southern Hemisphere.\nA zonal wave 3 is a short way of saying that a wave with a wavelenth 1/3rd of the circumference of the Earth at that latitude; which in essense means that on each latitude circle there are 3 maximums and 3 minimums.\nSo how do you measure the activity of this kind of wave in the atmosphere?\nTraditionally, one would compute the Fourier spectrum of geopotential height (or meridional wind) at some latitude or latitude band and look at the amplitude and phase of the 3rd wavenumber.\nAlternatively, Raphael (2004) took the mean value of geopotential anomalies in the climatological location of the three maximums.\nBoth approaches have the problem that assume that the wave 3 is a perfectly homogeneous wave with constant amplitude and phase in all longitudes.\nRaphael (2004)’s index is even more restrictive.\nBecause it’s based on fixed locations, it actually measures the amplitude of the wave 3 that is projected into the direction of the climatological wave (this might be an useful concept, though).\nHowever, wave 3 activity is not such a clean wave at all.\nMany studies and simulations show that the atmospheric circulation that is wave 3-like is much more intense over the South Pacific than over the southern Indian Ocean and that the waves don’t stick to a constant latitude.\nA recent paper by lead author Rishav Goyal proposes a different approach (which is very similar to my own, which will hopefully be available in a scientific journal near you soon).\nInstead of imposing the shape of the wave 3, Goyal et al. (2022) derive the suitable wave 3 pattern from the data using Empirical Orthogonal Functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: First two EOFs of meridional wind between 70°S and 40°S (shaded, positive values in red, negative in blue). The pure wave-3 field is shown in thin black contours. In parenthesis, the variance explained by each EOF. Compare with the top panel in Figure 2 of Goyal et al. (2022).\n\n\n\nAs you can see in Figure 1, the first two EOFs of meridional wind anomalies are basically a wave 3 pattern but with all the important deviations from a neat homogeneous wave that we want.\nThe amplitude is greater in the Pacific Ocean than in the Indian Ocean, the phase changes with latitude and the centres are not exactly where they would be otherwise.\nThis is essentially right, and something that is consistent with previous studies.\nThis is not the first time EOF was applied to the wave 3 pattern.\nYuan and Li (2008) did the same, but importantly, they used only one EOF to characterise the zonal wave 3 when you need two.\nThis kind of thing happens all the time in atmospheric sciences.\nSince EOFs can only capture standing oscillations, any pattern that moves needs two or more EOFs to be described.\nGoyal et al. (2022) does use two of them and furthermore argues that since they are clearly the two orthogonal phases of a wave 3-like pattern, one can combine the time series of each EOF into a single wave-like index\n\\[\nZW3 = \\sqrt{EOF1^2 + EOF2^2} \\cos\\left(\\tan^{-1} \\left ( \\frac{EOF2}{EOF1} \\right ) \\right )\n\\]\nSo, basically, you get the magnitude of the overall activity of the wave 3 as \\(\\sqrt{EOF1^2 + EOF2^2}\\) and the phase –the location– with \\(\\tan^{-1} \\left ( \\frac{EOF2}{EOF1} \\right )\\).\nI think this very neat and the way to go.\nIt combines the convenient mathematical formulation of a wave with the flexibility of describing something that is not a pure sine wave. But I think there’s a small detail that can be improved upon.\nIn theory, one can combine any two random time series that way, but the idea is that this works only if the two series actually represent the two orthogonal phases of a wave-like thing.\nSo the validity of this new index relies heavily on the patterns seen in Figure 1 being orthogonal.\nThey certainly look to be, but in detail, I think that they are not exactly.\nIf one plots the magnitude \\(\\sqrt{EOF1^1 + EOF^2}\\) (Figure 2 panel a) it looks like the amplitude changes spatially on a scale similar to the wavelength of the wave, particularly in the Indian sector.\nI think this is not ideal and shows that the patterns obtained with EOF are only approximately suitable for the task.\nThe panel b in Figure 2 shows how I think it should look. The amplitude is modulated on a spatial scale larger than the wave itself, creating a smoother field.\n\n\n\n\nFigure 2: Panel a: Spatial magnitude of the combined EOF magnitude (\\(\\sqrt{EOF1^2 + EOF2^2}\\)). Panel b: Spatial magnitude of the complex EOF.\n\n\n\nAs you might’ve guessed by the title Figure 2b, I think a slightly better approach might be to use complex EOF (cEOF).\nComplex EOFs were first described for the climate sciences by Horel (1984) as far as I can tell and are a very good fit for this problem.\nInstead of relying on chance that the two leading EOFs are orthogonal, one can create EOFs that are orthogonal by construction.\nTo do this, one has to move into the complex plane (which is always delight) by computing the analytic function of the variable, which is a complex number in which the real part is the same variable and the imaginary part is the Hilbert transform of the variable.\nTo give an idea of what it looks like, Figure 3 shows a (semi)randomly-selected meridional wind field in shading with it’s Hilbert transform in black contours.\n\n\n\n\n\n\n\nFigure 3: Monthly mean meridional wind (shaded, positive values in red, negative in blue) for July 1995. In black contour lines, its Hilbert transform (positive values in solid line, negatives in dashed).\n\n\n\nYou can see that the real field has a wave-like structure and that the imaginary field has the same structure but rotated 90º (the maximum amplitude of the imaginary field is exactly in the zeroes of the real field).\nAnd now for the magic reveal, Figure 4 shows the real and imaginary parts of the cEOF of meridional wind.\n\n\n\n\nFigure 4: Real and imaginary components of the complex EOF of meridional wind between 70°S and 40°S (shaded, positive values in red, negatives in blue).\n\n\n\nThey look very similar to Goyal et al. (2022)’s first two EOFs but they are truly orthogonal so there’s no ambiguity of whether it makes sense to calculate the amplitude and phase and the magnitude vaires smoothly in space (as seen before in Figure Figure 2 b)\n\n\n\nNow, to be fair, this is a small detail and the difference is not huge.\nFor instance, the correlation between the amplitude of Goyal et al. (2022)’s index and the equivalent complex index is a whooping 0.98. I think that using one or the other for research will not (should not, at any case) make a lot of difference.\n\n\n\nGoyal, Rishav, Martin Jucker, Alex Sen Gupta, and Matthew H. England. 2022. “A New Zonal Wave 3 Index for the Southern Hemisphere.” Journal of Climate -1 (May): 1–25. https://journals.ametsoc.org/view/journals/clim/aop/JCLI-D-21-0927.1/JCLI-D-21-0927.1.xml.\n\n\nHorel, J. D. 1984. “Complex Principal Component Analysis: Theory and Examples.” Journal of Applied Meteorology and Climatology 23 (12): 1660–73. https://doi.org/10.1175/1520-0450(1984)023<1660:CPCATA>2.0.CO;2.\n\n\nRaphael, M. N. 2004. “A Zonal Wave 3 Index for the Southern Hemisphere.” Geophysical Research Letters 31 (23). https://doi.org/10.1029/2004GL020365.\n\n\nYuan, Xiaojun, and Cuihua Li. 2008. “Climate Modes in Southern High Latitudes and Their Impacts on Antarctic Sea Ice.” Journal of Geophysical Research 113 (C6): C06S91. https://doi.org/10.1029/2006JC004067.\n\n\n\n\n",
    "preview": "posts/2022-05-31-wave3-goyal/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-09-asymsam/",
    "title": "Sources and impacts of the zonally asymmetric component of the Southern Annular Mode",
    "description": "Explanation of our recent paper on asymmetric and symmetric parts of the Southern Annular Mode.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-08-10",
    "categories": [
      "SAM",
      "ENSO",
      "general circulation"
    ],
    "contents": "\nToday I got my first paper published! 🎉\nAlong with my PhD advisors, Leandro Díaz and Carolina Vera, we wrote “Assessment of zonally symmetric and asymmetric components of the Southern Annular Mode using a novel approach” (because it wouldn’t be an academic publication if the title was shorter), and this post is a general explanation of the main methods and takeaways of the paper.\nWhat’s the deal with the SAM\nClimatologists love to think of the climate as a series of different modes. That is, take all the incomprehensible complexity of the evolving climate and distil it to a handful of phenomena that we can understand. For example, instead of having to think about all the variability of the Equatorial Pacific Ocean, which is a 3D field with multiple variables, you can think about the El Niño–Southern Oscillation (ENSO) phenomenon, which can be characterised by a single time series. Other members of the “Climate Oscillations Hall of Fame,” and their respective acronyms, are the Indian Ocean Dipole (IOD), the Northern Annular Mode (NAM), the North Atlantic Oscillation (NAO) and, important for this paper, the Southern Annular Mode (SAM).\nThe SAM describes an oscillating pattern of alternating low and high pressure anomalies over Antarctica and in the middle latitudes. To have a clear picture, the typical field of pressure anomalies when the SAM is on it’s positive phase looks like Figure 1.\n\n\n\n\nFigure 1: Typical pressure anomalies of the positive phase of the SAM. Lower pressure than usual over the Antarctic and higher pressure than usual in the mid-latitudes.\n\n\n\nAs you can see, this pattern lives up to its name in that the positive pressure anomalies form a ring (or, for the hoity-toity scientists, an annulus) around the negative pressure anomalies. In fact, almost every paper on the topic starts the introduction with a sentence along the lines of “The SAM is approximately zonally symmetric…” (Fogt and Marshall 2020) –where “zonally symmetric” means that it doesn’t depend on longitude. The word “approximately” is doing a lot of heavy lifting: the zonally symmetric ring is clearly deformed by zonally asymmetric anomalies.\nMost papers basically ignore these deviations from zonal symmetry and think of the SAM as zonally symmetric. This is fine as a first order approximation, but many aspects of the SAM are actually tied to its asymmetric nature. For example, the SAM is associated with anomalies in meridional (north–south) wind, which is not possible for a zonally symmetric pattern. And these meridional wind anomalies clearly are related to impacts in precipitation in over South America (Silvestri and Vera 2009) and temperature over the Antarctic Peninsula (Fogt, Jones, and Renwick 2012). These anomalies are also conspicuously similar to the effect of ENSO on these higher latitudes (e.g. Clem and Fogt 2013).\nThe understanding of the SAM as zonally symmetric also influences the theories behind the positive trend of the SAM index seen during the austral summer. Simulations show that increased concentrations of greenhouse gases and changes in stratospheric ozone combine to produce a zonally symmetric change of lower pressures over the poles and higher pressures at lower latitudes (Figure 2 from Arblaster and Meehl (2006)). Since these changes are similar to the positive phase of the SAM, then it’s only logical to identify this simulated changes with the observed SAM trend. But, of course, this only works for a zonally symmetric SAM.\n\n\n\n\nFigure 2: Trends in sea level pressure for the 1958–1999 period simulated with climate models that include various forcings. From Arblaster and Meehl (2006).\n\n\n\nIn essence, all evidence point to the fact that the asymmetric part of the SAM can have different sources of variability, different impacts, and different trends. So what we wanted to do is to try to separate the SAM into two indices, and index for the symmetric part of the SAM, and another for the asymmetric part of the SAM. The hope being that by studying these indices we could understand better the relationships between other parts of the climate to each part of the SAM.\nHow to divide the SAM\nWhat we did was, for each vertical level, to take the classical SAM pattern that we all know and love (Fig. 3a) and create two derived fields. The zonal mean field, which represents the zonally symmetric SAM (Fig. 3c) and then the difference between the full SAM field and the zonal mean, which represents the zonally asymmetric SAM (Fig. 3b). This figure really shows that the magnitude of the zonal anomalies is on par with the magnitude of the zonal mean!\n\n\n\n\nFigure 3: Spatial patterns of the first EOF of 700\\ hPa geopotential height for 1979 – 2018 period. (a) Full field, (b) zonally asymmetric component and (c) zonally symmetric component. Arbitrary units; positive values in blue and negative values in red.\n\n\n\nWe then projected monthly field into these three fields (again, for each vertical level) to obtain three time series, one for the classic “full” SAM, one for the asymmetric SAM (A-SAM) and one for the symmetric SAM (S-SAM).\nAs a sanity check, Figure 4 shows the regression of 700 hPa geopotential height and the three indices (the ones between A-SAM and S-SAM are actually the coefficients of multiple regression, check the paper for the details). And it works! The regression pattern of the full SAM index is the now-familiar SAM pattern. The pattern associated with the S-SAM is much more zonally symmetric and the one associated with the A-SAM is zonally asymmetric.\n\n\n\n\nFigure 4: Regression of 700 hPa geopotential height (meters) with (column a) SAM, (column b) A-SAM, and (column c) S-SAM for the 1979 – 2018 period.\n\n\n\nThe two faces of the SAM\nWith these indices at hand, we can now study the behaviours that are unique of the asymmetric or the symmetric SAM. For example, before I talked about the positive trend in the SAM index. Figure 5 show linear trends (in standard deviations per decade) for each index at each vertical level of the atmosphere.\n\n\n\n\nFigure 5: Linear trends (in standard deviations per decade) at each level for annual (row 1) and seasonal values (rows 2 to 5) for the period 1979 – 2018 and for the (column a) SAM index, (column b) A-SAM index, and (column c) S-SAM index. Shading indicates the 95% confidence interval from a t-distribution.\n\n\n\nNot surprisingly we detect the already-known positive trend of the SAM index, which is only present in the summer (and a but in autumn) and only in the troposphere (Fig. 5 column a). But what we also show is that only the symmetric part of the SAM has experienced a increase towards positive values (Fig. 5 column c), while the asymmetric part of the SAM appears to have suffered no long term (linear) trends ((Fig. 5 column b). Another added value of our paper is the computation of trends along many vertical levels. Most studies look at trends for a single SAM index defined using either 700 hPa geopotential height or sea level pressure. Here we show that the positive trend actually reaches its maximum near 100 hPa, which is much higher in the atmosphere.\n\n\n\n\nFigure 6: Linear trends (in percent per decade) of the variance explained by A-SAM and S-SAM at each level and for each trimester for the period 1979 – 2018.\n\n\n\nAnother interesting question is whether the SAM has become more or less asymmetric. Fogt, Jones, and Renwick (2012) suggested that the SAM is becoming more symmetric in summer and autumn between 1960 and 2000. In our study we use data between 1979 and 2018 (data before 1979 is highly suspect due to the lack of satellite observations) so it’s not terribly comparable, but we reach the opposite conclusion. Figure 6 shows linear trends of the explained variance of each index at each vertical level. In summer, the variance explained by the A-SAM in lower levels has increased by about 2% per decade. Honestly, this is not strong evidence by itself, so more research is needed™.\nAll that (and more!) indicates that the asymmetric part of the SAM behaves differently from the symmetric part. But we also show that they have different surface impacts.\n\n\n\n\nFigure 7: Regression of summer mean 2-metre temperature anomalies (Kelvin) from ERA5 with SAM, A-SAM and S-SAM for the 1979 – 2018 period. Black contours indicate areas with p-value smaller than 0.05 controlling for False Detection Rate. Note that the colour scale cuts-off at \\(\\pm0.6 \\mathrm{K}\\) to highlight mid-latitudes and tropics features at the expense of the higher values in polar regions.\n\n\n\nPanel a.1 in Figure 7 shows the regression between summer mean temperature and the classic SAM index. Compare it with panels b.1 and c.1, which show the same but for the A-SAM and S-SAM index. In most areas, you can see a relatively clean separation of the SAM relation ship into the asymmetric and symmetric component. For instance, in the Antarctic region positive values of the SAM are associated with negative temperature anomalies in an incomplete ring surrounding the continent. This ring is mostly explained by the S-SAM, while the A-SAM relationship with temperature is not so strong in these high latitudes.\nYou can look at the maps and find other differences (maps for other seasons are available on the paper), but I here I want to highlight what happens in the tropical oceans. Both in the Pacific and Indian ocean, there is no significant relationship between the SAM and air temperature, but there is a relationship between the A-SAM and temperatures there. That’s interesting to me, because it suggests that in some regions, there is an effect of the A-SAM that is masked by the S-SAM and thus is not detectable by using the full SAM index, which mixes the two.\nIf “temperature of the tropical Pacific” made you think of ENSO, you are not alone. It is known that ENSO is somewhat correlated with the SAM, but did you know know that there is no correlation between the zonally symmetric part of the SAM and ENSO in any season? If you look at Table 1, now you know! In fact, Table 1 also shows that the correlation with ENSO is higher for the A-SAM index than for the SAM index in all seasons except DJF.\n\n\nTable 1: Correlation between SAM indices and the Oceanic Niño Index. p-values corrected for False Detection Rate in parenthesis. In bold, correlations with p-value smaller than 0.05.\n\n\n\n\n\nCorrelation\n\n\n\n\nPartial correlation\n\n\n\n\n\nSAM\n\n\nA-SAM\n\n\nS-SAM\n\n\nYear\n\n\n-0.17 (0.001)\n\n\n-0.26 (<0.001)\n\n\n0.02 (0.775)\n\n\nDJF\n\n\n-0.31 (0.002)\n\n\n-0.30 (0.003)\n\n\n-0.17 (0.115)\n\n\nMAM\n\n\n-0.07 (0.530)\n\n\n-0.26 (0.011)\n\n\n0.14 (0.192)\n\n\nJJA\n\n\n0.01 (0.900)\n\n\n-0.14 (0.192)\n\n\n0.11 (0.300)\n\n\nSON\n\n\n-0.25 (0.014)\n\n\n-0.42 (<0.001)\n\n\n0.05 (0.686)\n\n\nThis is exciting because it says that if you want to study the relationship between the SAM and ENSO, you should probably look only at the S-SAM.\nMore SAM\nIf this piqued your interest, go read the rest of the paper (click here for a free, but DRMd copy). It covers other issues, such as the cross-correlation among levels, vertical regressions, and the relationship with precipitation. But overall, the point is that not only it’s possible to split the Southern Annular Mode into a zonally symmetric and a zonally asymmetric part, but it seems to be both physically meaningful and statistically useful.\nIf you want to use this on your research, you can get all the code that produced the paper on this repository and get the data for the three SAM indices here.\n\n\n\nArblaster, Julie M., and Gerald A. Meehl. 2006. “Contributions of External Forcings to Southern Annular Mode Trends.” Journal of Climate 19 (12): 2896–2905. https://doi.org/10.1175/JCLI3774.1.\n\n\nClem, Kyle R., and Ryan L. Fogt. 2013. “Varying Roles of ENSO and SAM on the Antarctic Peninsula Climate in Austral Spring.” Journal of Geophysical Research: Atmospheres 118 (20): 11, 481–11, 492. https://doi.org/10.1002/jgrd.50860.\n\n\nFogt, Ryan L., Julie M. Jones, and James Renwick. 2012. “Seasonal Zonal Asymmetries in the Southern Annular Mode and Their Impact on Regional Temperature Anomalies.” Journal of Climate 25 (18): 6253–70. https://doi.org/10.1175/JCLI-D-11-00474.1.\n\n\nFogt, Ryan L., and Gareth J. Marshall. 2020. “The Southern Annular Mode: Variability, Trends, and Climate Impacts Across the Southern Hemisphere.” WIREs Climate Change 11 (4): e652. https://doi.org/10.1002/wcc.652.\n\n\nSilvestri, Gabriel, and Carolina Vera. 2009. “Nonstationary Impacts of the Southern Annular Mode on Southern Hemisphere Climate.” Journal of Climate 22 (22): 6142–48. https://doi.org/10.1175/2009JCLI3036.1.\n\n\n\n\n",
    "preview": "posts/2021-08-09-asymsam/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 471,
    "preview_height": 471
  },
  {
    "path": "posts/2021-01-15-sam-symmetry/",
    "title": "Does the Southern Annular Mode exist?",
    "description": "Scavenging the literature on the Southern Annular Mode I came across two papers from the early 2000's which seem to question whether this mode is actually a global phenomenon and not a statistical artifact.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-01-15",
    "categories": [
      "SAM",
      "statistics",
      "general circulation"
    ],
    "contents": "\nIntroduction\nIn “The Structure and Composition of the Annular Modes in an Aquaplanet General Circulation Model” (Cash, Kushner, and Vallis 2002) and “Zonal Asymmetries, Teleconnections, and Annular Patterns in a GCM” (Cash, Kushner, and Vallis 2005) (CKV from now on), the authors analyse simulations on an aquaplanet model (in the former) and with some zonal asymmetries (in the latter). In particular, they study the configuration of their “annular modes” or, rather, their leading EOF. Their conclusion is that annular modes as seen from the leading EOF are a statistical artifact and that, in fact, they actually describe localised events.\nIf this is correct and is also valid for the real atmosphere (as opposed to their simple models), then it throws a monkey wrench to any attempts to understand the Souther Annular Mode –it doesn’t even exist as a physically meaningful entity.\nHere, I use reanalysis data to show that, while CKV raises important points, the concept of the global SAM is safe.\nCKV analysis\nCKV ran an aquaplanet model and they point out that the first EOF of sea level pressure describes an almost perfect annular mode.\n\n\n\n\nFigure 1: Figure 4 from Cash, Kushner, and Vallis (2002): Leading EOF of winter season surface pressure. (a) Normalized, nondimensional EOF of the zonal-mean surface pressure. (b) Regression map of zonal-mean surface pressure against the principal component of the leading EOF. Amplitude is the response to 1 std dev in the principal component. (Units: mb.) (c) As in (b), except for the zonally varying pressure. Solid lines are positive, dashed lines are negative, and a heavy contour denotes the zero line. Contour interval is 2 mb\n\n\n\nThe problem is that when the look at particular events of high / low values of EOF1, they are nowhere near “annular.”\n\n\n\n\nFigure 2: Excerpt from Figure 7 from Cash, Kushner, and Vallis (2002): High- and low-index annular-mode events. (a), (b), (c) High-index events. (d), (e), (f ) Low-index events. Events displayed are 3-day averages about the day that the projection coefficient attains its maximum value during the event. Solid contours are positive, dashed contours are negative, and a heavy contour denotes the zero line. Contour interval is 5 mb\n\n\n\nInspired by this disconnect between the structure of the EOF and the individual cases, they try a different approach. They go back to using teleconnection maps (correlation between SLP at one point and the rest of the globe) based on various latitudes. They conclude that negative correlations are maximised for points at 65º and 35º (Figure 2)\n\n\n\n\nFigure 3: SLP de casos seleccionados con eventos positivos (izquierda) y negativos (derecha)\n\n\n\nThese maps look like “zonally localised versions” of the global annular mode. From this (and other results), CKV conclude that\n\nThe low-frequency variability of the model is thus characterized by meridional dipoles in the sea level pressure, with centers near 35º and 65º latitude, and a zonal scale of 60º to 90º. Because these events are distributed uniformly in longitude in the aquaplanet model, they are represented in an EOF analysis as a single, zonally uniform pattern.\n\nStrong stuff.\nFurther analysis\nCKV’s proposal is intriguing, does the real atmosphere behaves thusly? The problem they put forward is very similar to the problem I had with Senapati, Dash, and Behera (2021) here so I’ll use similar methods.\n\n\nShow code\n\nlibrary(data.table)\nlibrary(magrittr)\nlibrary(metR)\nlibrary(ggplot2)\nlibrary(ggperiodic)\nlibrary(patchwork)\n\ntheme_set(theme_minimal() + \n            theme(panel.grid = element_blank()))\n\n# simple and dirty map\nmap <- ggplot2::map_data(\"world2\") %>%\n  subset(lat %between% c(-90, -10))\n\nquick_map <- list(geom_polygon(data = map,\n                               aes(long, lat, group = group), fill = NA, color = \"black\",\n                               size = 0.2),\n                  scale_x_longitude(),\n                  scale_y_latitude())\n\n\n\n\n\nShow code\n\n# This WILL take long. Go make yourself a cup of tea or brew some mate.\nera5_file <- here::here(\"_data\", \"era5-hgt.nc\")\n\nif (!file.exists(era5_file)) {\n  request <- list(\n    format = \"netcdf\",\n    product_type = \"monthly_averaged_reanalysis\",\n    variable = \"geopotential\",\n    pressure_level = \"700\",\n    year = c(\"1979\", \"1980\", \"1981\", \"1982\", \"1983\", \"1984\", \"1985\", \"1986\", \"1987\", \"1988\", \"1989\", \"1990\", \"1991\", \"1992\", \"1993\", \"1994\", \"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\"),\n    month = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"),\n    time = \"00:00\",\n    area = c(-20, -180, -90, 180),\n    grid = c(\"2.5\", \"2.5\"),   # we don't need high resolution\n    dataset_short_name = \"reanalysis-era5-pressure-levels-monthly-means\",\n    target = basename(era5_file)\n  )\n  \n  # Need to set up user with\n  # ecmwfr::wf_set_key() \n  ecmwfr::wf_request(request, path = dirname(era5_file))\n}\n\n\n\n\n\nShow code\n\nhgt <- ReadNetCDF(era5_file, vars = c(hgt = \"z\")) %>% \n  setnames(c(\"latitude\", \"longitude\"),\n           c(\"lat\", \"lon\")) %>% \n  .[, lon := ConvertLongitude(lon)] %>%   # put longitude between 0 and 360\n  .[, hgt := hgt/9.8] %>% \n  .[, hgt_a := hgt - mean(hgt), by = .(lon, lat, month(time))] %>% \n  .[, hgt_m := mean(hgt_a), by = .(lat, time)] %>% \n  .[, hgt_z := hgt_a - hgt_m]\n\n\n\nLet’s first compute the Southern Annular Mode (SAM) as the leading EOF of the 700 hPa geopotential height south of 20ºS.\n\n\nShow code\n\nsam <- hgt %>% \n  copy() %>% \n  .[, hgt := hgt_a*sqrt(cos(lat*pi/180))] %>% \n  EOF(hgt ~ time | lon+ lat, n = 1, data = .)\n\n\n\n\n\nShow code\n\nsam$right %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = hgt, fill = ..level..)) +\n  quick_map +\n  coord_polar() +\n  scale_fill_divergent_discretised(guide = \"none\")\n\n\n\n\nFigure 4: Spatial pattern of the leading EOF of 700 hPa monthly geopotential height anomalies (AKA Southern Annular Mode, AKA SAM).\n\n\n\nAnd now let’s compute localised SAM indices. At each longitude I will take a section of the data located between 45º West and 45º East of it and project the geopotential height field onto the corresponding SAM pattern (in Figure 4). The result is one “local SAM index” for each longitude.\n\n\nShow code\n\nlon_width <- 90\nlon_halfwidth <- lon_width/2\n\n# \"extended\" version of geopotential height \nhgt2 <- sam$right %>% \n  copy() %>% \n  setnames(\"hgt\", \"EOF\") %>% \n  hgt[., on = .NATURAL] %>% \n  qwrap(lon = c(0, 360) ~ c(-lon_halfwidth, 360 + lon_halfwidth))\n\n# For each longitude, compute EOF using a segment of lon_width width\n# centred in that longitude.\nlon_eofs <- lapply(unique(hgt$lon), function(base_lon) {\n  hgt2 %>% \n    .[lon %between% (base_lon + c(-lon_halfwidth, lon_halfwidth))] %>% \n    .[, .(eof = weighted.mean(hgt_a*EOF, cos(lat*pi/180))),\n      by = time] %>% \n    .[, base_lon := base_lon] %>% \n    .[]\n}) %>% \n  rbindlist()\n\n\n\nFollowing a similar argument from my analysis of the proposed wave-4 pattern, if the SAM pattern in Figure 4 was a statistical artifact formed by the combination of independent localised events, then the local SAM indices shouldn’t be correlated between each other at all. So let’s compute the pairwise correlation at each longitude.\n\n\nShow code\n\nlon_eofs %>% \n  widyr::pairwise_cor(base_lon, time, eof) %>% \n  ggplot(aes(item1, item2)) +\n  geom_contour_fill(aes(z = correlation, fill = ..level..), na.fill = 1) +\n  geom_contour2(aes(z = correlation), size = 0.2) +\n  geom_text_contour(aes(z = correlation, stroke.color = ..level..), color = \"black\",\n                    stroke = 0.2) +\n  scale_fill_divergent(\"Correlation\", \n                       super = ScaleDiscretised) +\n  scale_color_divergent(aesthetics = \"stroke.color\", guide = \"none\") +\n  scale_x_longitude() +\n  scale_y_longitude() +\n  coord_equal() \n\n\n\n\nFigure 5: Pairwise correlation between localised SAM indices.\n\n\n\nAnd form Figure 5 I feel that it’s clear that the local SAM indices are fairly well correlated. There is, though, a correlation minimum between ~120W and 60E. 120W coincides with the Amundsen Sea Low and where most of the wave activity related to El Niño Southern Oscillation is located, so is not surprising that this region appears notable.\nAnother test would be to compute the correlation map of each localised SAM index with the whole geopotential height field. Again, if the SAM was not global, then I would not expect to find a (relatively) complete SAM pattern associated with the localised indices.\n\n\nShow code\n\ncor_pattern <- lon_eofs %>% \n  .[base_lon %in% rev(seq(0, 360, length.out = 7))[-1]] %>% \n  .[hgt, on = \"time\", allow.cartesian = TRUE] %>% \n  .[, cor(hgt_a, eof), by = .(lon, lat, base_lon)]\n\n\n\n\n\nShow code\n\nbase_lons <- unique(cor_pattern$base_lon)\nwrap_ <- function(lon) {\n   lon <- ifelse(lon <= 0, lon + 360, lon)\n   lon <- ifelse(lon >= 360, lon - 360, lon)\n   lon\n}\n\nlims <- data.table(base_lon = unique(lon_eofs$base_lon)) %>% \n   .[, side1 := wrap_(base_lon + 45)] %>% \n   .[, side2 := wrap_(base_lon - 45)] %>% \n   .[base_lon %~% base_lons] \n\n\ncor_pattern %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = V1, fill = ..level..), breaks = AnchorBreaks(exclude = 0)) +\n  geom_contour2(data = periodic(sam$right, lon = c(0, 360)),\n                aes(z = hgt), size = 0.2, breaks = AnchorBreaks(0, 0.01)) +\n  geom_vline(data = lims, aes(xintercept = side1)) +\n  geom_vline(data = lims, aes(xintercept = side2)) +\n  quick_map +\n  scale_fill_divergent(super = ScaleDiscretised) +\n  coord_polar() +\n  facet_wrap(.~base_lon, labeller = labeller(base_lon = LonLabel)) +\n  theme(axis.text = element_blank())\n\n\n\n\nFigure 6: Correlation maps between 700 hPa monthly geopotential height anomalies and localised SAM indices at differnet centre longidutes. Black contours indicate the original SAM pattern and black lines delineate the longitudes used to compute each localised SAM index.\n\n\n\nFigure 6 shows the correlation patterns for the local SAM indices with central longitudes of 0°, 60°E, 120°E, 180°, 120°W, and 60°W. The original SAM pattern is overlaid with contours for comparison. Although with some differences, I’d argue that in all cases the classic SAM pattern is clearly visible.\nNull hypothesis\nI now wonder… how would the null hypothesis of no global SAM looked under these same methods? I don’t have CKV data handy, but I can run simulations.\nFirst, replace the zonally varying SAM with it’s zonal mean component. Then, use that pattern to simulate a global, zonally symmetric, geopotential field. Finally, multiply that field with a localised Gaussian function with a random central longitude and 45º standard deviation. An example field is shown in Figure 7.\n\n\nShow code\n\nmean_eof <- copy(sam)\n\nmean_eof$right[, hgt := mean(hgt), by = lat]\n\nzonal_amplitude <- function(lon, central_lon) {\n  amplitude <- suppressWarnings(circular::dwrappednormal(lon*pi/180, mu = central_lon[1]*pi/180, sd = 45*pi/180))\n  amplitude[amplitude <= 0.05] <- 0\n  amplitude/max(amplitude)\n}\n\n# Grid\nsimulation <- CJ(lon = unique(hgt$lon), \n                 lat = unique(hgt$lat),\n                 time = unique(hgt$time)) %>% \n  predict(mean_eof)[., on = .NATURAL] %>% \n  setnames(\"hgt\", \"annular\") %>% \n  na.omit() %>% \n  .[, central_lon := runif(1, 0, 360), by = time] %>% \n  .[, zonal_amplitude := zonal_amplitude(lon, central_lon), by = .(time)] %>% \n  .[, hgt := zonal_amplitude*annular/sqrt(cos(lat*pi/180))]\n\n\n\n\n\nShow code\n\nsimulation[time == unique(time)[1]] %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = annular)) +\n  quick_map +\n  scale_fill_divergent(guide = \"none\") +\n  coord_polar() +\n  labs(subtitle = \"Zonally symmetric anomaly\") +\n  \n  simulation[time == unique(time)[1]] %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = zonal_amplitude)) +\n  quick_map +\n  scale_fill_divergent(guide = \"none\") +\n  coord_polar() +\n  labs(subtitle = \"Localisation function\") + \n  \n  \n  simulation[time == unique(time)[1]] %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = hgt)) +\n  quick_map +\n  scale_fill_divergent(guide = \"none\") +\n  coord_polar() +\n  labs(subtitle = \"Final geopotential field\") + \n\n\n  plot_layout(ncol = 3) & theme(axis.text = element_blank())\n\n\n\n\nFigure 7: Example of a single simulated geopotential height field. The zonally symmetric SAM pattern (left) is multiplied by the localisation function (middle), which reulsts in a localised pattern of positive and negative anomalies.\n\n\n\nCKV’s contention is that the leading EOF of an atmosphere consisting solely on localised patterns such as the last row of Figure 7 will appear as a single annular pattern. Let’s see first if that’s correct.\n\n\nShow code\n\nsim_eof <- simulation %>% \n  .[lat >= -85] %>% \n  .[, hgt := Anomaly(hgt)*sqrt(cos(lat*pi/180)), by = .(lon, lat, month(time))] %>% \n  .[, EOF(hgt ~ time | lon + lat, n = 1)]\n\n\n\n\n\nShow code\n\nsim_eof$right %>%\n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = hgt)) +\n  quick_map +\n  scale_fill_divergent(guide = \"none\") +\n  coord_polar()\n\n\n\n\nFigure 8: Leading EOF of simulated geopotential height fields.\n\n\n\nFigure 8 shows the leading EOF of the simulated data. And indeed, even thought by construction the data doesn’t have an annular mode, one appears as the leading EOF plain as day. Now let’s put them through the same process as the real data. Get the localised SAM indices and compute the pairwise correlation between them.\n\n\nShow code\n\n# \"extended\" version of geopotential height \nhgt2 <- sim_eof$right %>% \n  copy() %>% \n  setnames(\"hgt\", \"EOF\") %>% \n  simulation[., on = .NATURAL] %>% \n  qwrap(lon = c(0, 360) ~ c(-lon_halfwidth, 360 + lon_halfwidth))\n\n# For each longitude, compute EOF using a segment of lon_width width\n# centred in that longitude.\nlon_eofs_sim <- lapply(unique(hgt$lon), function(base_lon) {\n  hgt2 %>% \n    .[lon %between% (base_lon + c(-lon_halfwidth, lon_halfwidth))] %>% \n    .[, .(eof = weighted.mean(hgt*EOF, cos(lat*pi/180))),\n      by = time] %>% \n    .[, base_lon := base_lon] %>% \n    .[]\n}) %>% \n  rbindlist()\n\n\n\n\n\n\nShow code\n\nlon_eofs_sim %>% \n  widyr::pairwise_cor(base_lon, time, eof) %>% \n  ggplot(aes(item1, item2)) +\n  geom_contour_fill(aes(z = correlation, fill = ..level..), na.fill = 1) +\n  geom_contour2(aes(z = correlation), size = 0.2) +\n  geom_text_contour(aes(z = correlation, stroke.color = ..level..), color = \"black\",\n                    stroke = 0.2) +\n  scale_fill_divergent(\"Correlation\",\n                       super = ScaleDiscretised) +\n  scale_color_divergent(aesthetics = \"stroke.color\", guide = \"none\") +\n  scale_x_longitude() +\n  scale_y_longitude() +\n  coord_equal() \n\n\n\n\nFigure 9: Same as Figure 5 but for the simulated fields.\n\n\n\nComparing Figure 9 with Figure 5, the results of the simulation are very different! As expected, the correlation between indices decreases rather rapidly between adjacent longitudes, and it’s near zero for the antipodes.\nConslusion\nI think that CKV does bring up an important point. The fact that one can compute a global EOF doesn’t mean that the observed pattern is indeed global. EOF is a statistical method not constrained by physics and as the simulated data shows, you can very well obtain an annular mode with data that doesn’t vary annularly.\nHowever, in the real atmosphere, the Southern Annular Mode does appear as a globally coherent annular mode. The SAM is safe.\nDownload code\nClick on this button to get the code that generated this document:\n\n\n Download code\n\n\n\n\n\nCash, Benjamin A., Paul J. Kushner, and Geoffrey K. Vallis. 2002. “The Structure and Composition of the Annular Modes in an Aquaplanet General Circulation Model.” J. Atmos. Sci. 59 (23): 3399–3414. https://doi.org/10.1175/1520-0469(2002)059<3399:TSACOT>2.0.CO;2.\n\n\n———. 2005. “Zonal Asymmetries, Teleconnections, and Annular Patterns in a GCM.” J. Atmos. Sci. 62 (1): 207–19. https://doi.org/10.1175/JAS-3361.1.\n\n\nSenapati, Balaji, Mihir K. Dash, and Swadhin K. Behera. 2021. “Global Wave Number-4 Pattern in the Southern Subtropical Sea Surface Temperature.” Scientific Reports 11 (1): 142. https://doi.org/10.1038/s41598-020-80492-x.\n\n\n\n\n",
    "preview": "posts/2021-01-15-sam-symmetry/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-15-sao-and-jet/",
    "title": "The meridional structure of the Semiannual Oscillation",
    "description": "How to filter out extra-tropical variabilty to highlight the structure of the Semiannual Oscillation using just multiple linear regression.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-01-15",
    "categories": [
      "Semiannual Oscillation",
      "general circulation",
      "tropics",
      "stratosphere"
    ],
    "contents": "\nSome time ago, with a Journal Club I’m part of (shout-out to Go stratospheric!), we read “Representation of the equatorial stratopause semiannual oscillation in global atmospheric reanalyses” (Kawatani et al. 2020), a paper about the Semiannual Oscillation (SAO). One figure (Figure 1) piqued our interest. The SAO is supposed to be a characteristic of the tropical stratosphere and be… you know… semiannual. But in Figure 1, the more sticking feature is extra-tropical and with an annual cycle.\n\n\n\nShow code\n\nknitr::include_graphics(file.path(\"img\", \"sao-jet-fig8.png\"))\n\n\n\n\nFigure 1: Figure 8 from Kawatani et al. (2020): “Time–latitude sections of climatological mean annual cycle of the zonal mean zonal wind for (a) SABER, (b) MLS, and (c–i) each reanalysis at 1 hPa. The contour intervals are 10 m s−1. Climatology is calculated from 1980 to 2010 in the reanalyses, from 2002 to 2016 in SABER, and from 2005 to 2016 in MLS.”\n\n\n\nWe quickly realised that there’s more going on there than the SAO. Is there a way to filter out the rest of the variability and only look at the SAO-related variabiltiy?\nAs always, first thing first. Download the data and replicate the figure. Kawatani et al. (2020) use data from many many reanalyses, but I’m only going to use data from ERA5.\n\n\nShow code\n\nlibrary(magrittr)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(metR)\ntheme_set(theme_minimal(base_size = 10) + theme(panel.grid = element_blank()))\n\n\n\n\n\nShow code\n\n# This WILL take long. Go make yourself a cut of tea or brew some mate.\nera5_file <- here::here(\"_data\", \"era5-u.nc\")\n\nif (!file.exists(era5_file)) {\n  request <- list(\n    format = \"netcdf\",\n    product_type = \"monthly_averaged_reanalysis\",\n    variable = \"u_component_of_wind\",\n    pressure_level = c(\"1\", \"2\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\"),\n    year = c(\"1979\", \"1980\", \"1981\", \"1982\", \"1983\", \"1984\", \"1985\", \"1986\", \"1987\", \"1988\", \"1989\", \"1990\", \"1991\", \"1992\", \"1993\", \"1994\", \"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\"),\n    month = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"),\n    time = \"00:00\",\n    grid = c(\"2.5\", \"2.5\"),   # we don't need high resolution\n    dataset_short_name = \"reanalysis-era5-pressure-levels-monthly-means\",\n    target = basename(era5_file)\n  )\n  \n  # Need to set up user with\n  # ecmwfr::wf_set_key() \n  ecmwfr::wf_request(request, path = dirname(era5_file))\n}\n\n\n\n\n\nShow code\n\nera5 <- ReadNetCDF(era5_file)\n\n\n\n\n\nShow code\n\nera5 %>% \n  .[level == 1] %>% \n  .[, .(u = mean(u)), by = .(latitude, month(time), level)] %>% \n  ggplot(aes(month, latitude)) +\n  geom_contour_fill(aes(z = u, fill = ..level..), \n                    breaks = AnchorBreaks(0, binwidth = 15, exclude =  0)) +\n  geom_contour_tanaka(aes(z = u), \n                      breaks =  AnchorBreaks(0, binwidth = 15, exclude =  0)) +\n  scale_y_latitude() +\n  scale_x_continuous(breaks = 1:12, labels = month.abb, expand = c(0, 0)) +\n  scale_fill_divergent_discretised(\"U\") +\n  facet_wrap(~level, labeller = labeller(level = function(x) paste0(x, \" hPa\")))\n\n\n\n\nFigure 2: Seasonal cycle of zonal mean zonal wind at 1 hPa\n\n\n\nFigure 2 replicated Figure 1 extending the latitude range to the rest of the globe. It becomes even clearer that the main variability is in the extratropics.\nNow, the SAO not only is supposed to be in the tropics, but it’s also restricted to levels higher than ~3 hPa. Figure 3 shows what’s going on at levels bewteen 1 hPa and 30 hPa.\n\n\n\nShow code\n\nlast_plot() %+%\n  era5[, .(u = mean(u)), by = .(latitude, month(time), level)]\n\n\n\n\nFigure 3: Same as Figure 2 but for multiple levels.\n\n\n\nThe variability in the extratopics is there in the lower levels. So I though, why don’t try to remove that lower-level signal? After looking at Figure 3 and trying around, I settled into representing the lower levels with 5 hPa and 20 hPa.\nSo, at each gridpoint I fit the model\n\\[\nU_{1hPa} = \\alpha U_{5hPa} + \\beta U_{20hPa} + \\epsilon_{1hPa}\n\\]\nWhere \\(U\\) is the zonal mean zonal wind at each level and \\(\\epsilon_{1hPa}\\) is the residual wind that I’m interested in. That is, the variability of zonal mean zonal wind at 1 hPa that is not (linearly) explained by the variability of the zonal mean zonal wind at the lower levels. For those paying attention, this is (I think) essentially removing the equivalent barotropic component in the wind.\n\n\nShow code\n\nfiltered  <- era5 %>% \n  .[level %in% c(1, 5, 20)] %>% \n  # .[, .(u = mean(u)), by = .(time, level, latitude)] %>%\n  dcast(time + latitude + longitude ~ level, value.var = \"u\") %>% \n  .[, u_resid := resid(lm(`1` ~ `5` + `20`)), by = .(latitude, longitude)] \n\n\n\nThe seasonal cycle of the residual zonal mean zonal wind in Figure 4 shows that the filtering really works! Now it really looks like the equatorial phenomenon that it’s supposed to be, and the semiannual cycle is plainly for all to see. 🥳.\n\n\nShow code\n\nfiltered %>% \n  .[, .(u = mean(u_resid)), by = .(month(time), latitude)] %>% \n  ggplot(aes(month, latitude)) +\n  geom_contour_fill(aes(z = u, fill = ..level..), \n                    breaks = AnchorBreaks(0, binwidth = 10, exclude =  0)) +\n  geom_contour_tanaka(aes(z = u), \n                      breaks =  AnchorBreaks(0, binwidth = 10, exclude =  0)) +\n  scale_y_latitude() +\n  scale_x_continuous(breaks = 1:12, labels = month.abb, expand = c(0, 0)) +\n  scale_fill_divergent_discretised(\"U\")\n\n\n\n\nFigure 4: Seasonal cycle of the residual zonal mean zonal wind.\n\n\n\nAnd for completitude, Figure 5 shows timeseries of zonal mean zonal wind at the equator, and averaged polarward of 30°. In the equator, the residual zonal wind is virtually identical to the unfiltered zonal wind.\n\n\nShow code\n\nfiltered %>% \n  .[, zone := fcase(latitude > 30, \"north of 30°N\",\n                    latitude < -30, \"south of 30°S\",\n                    latitude == 0, \"equator\",\n                    default = NA)] %>% \n  na.omit() %>% \n  .[, .(u = mean(`1`),\n        u_resid = mean(u_resid)), by = .(time, zone)] %>% \n  melt(id.vars = c(\"time\", \"zone\")) %>% \n  ggplot(aes(time, value)) +\n  geom_line(aes(color = variable)) +\n  scale_color_brewer(NULL, palette = \"Dark2\", labels = c(u = \"U\", \n                                                         u_resid = \"Residual U\")) +\n  scale_y_continuous(NULL) +\n  facet_wrap(zone~., ncol = 1) \n\n\n\n\nFigure 5: Timeseries of zonal mean zonal wind and residual zonal wind in the equator, averaged north of 30°N and south of 30°S\n\n\n\nSo the conclusion, I think, is that the meridional structure was a function of the polar jets, and that looking at the SAO by looking at zonal mean zonal wind in the equator is fine.\nDownload code\nClick on this button to get the code that generated this document:\n\n\n Download code\n\n\n\n\n\nKawatani, Yoshio, Toshihiko Hirooka, Kevin Hamilton, Anne K. Smith, and Masatomo Fujiwara. 2020. “Representation of the Equatorial Stratopause Semiannual Oscillation in Global Atmospheric Reanalyses.” Atmospheric Chemistry and Physics 20 (14): 9115–33. https://doi.org/10.5194/acp-20-9115-2020.\n\n\n\n\n",
    "preview": "posts/2021-01-15-sao-and-jet/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-14-enso-and-regression-to-the-mean/",
    "title": "ENSO and regression to the mean",
    "description": "How a recent paper published in Nature was pray to regression to the mean.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-01-14",
    "categories": [
      "ENSO",
      "statistics"
    ],
    "contents": "\nOn a recent article, Cai et al. (2020) used a perturbed ensemble of 40 members to show that members with relatively low El Niño Southern Oscillation (ENSO) variability at the beginning of the simulated period have a greater increase in ENSO variability towards the end of the simulated period than members with relatively high ENSO variability. They take this as evidence of ENSO self-regulation which would, if real, have important implications for future projections of ENSO variability under global warming. Here, I show that their results are not surprising, are likely product of simple regression to the mean and have no physical interpretation.\nIntroduction\nThe Community Earth System Model Large Ensemble model (Kay et al. 2015) consists of 40 climate simulations (members) that ran from 1920 to 2100 with identical external forcing and physics. Their only difference is a small, round-off error, perturbation in their atmospheric initial conditions. Cai et al. (2020) computed, for each member, the variability (standard deviation) of an index of Eastern-Pacific ENSO they call “E-index” for the first and last 50 years of the simulation. Owing to the effect of the various initial conditions, the variability in the E-index varied between members. I show their main result in Figure 1, which reproduces their Figure 1b. It shows a negative correlation between the initial E-index variability and its future change, defined as the difference between the final and the initial E-index variability.\n\n\nShow code\n\nlibrary(magrittr)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(patchwork)\ntheme_set(theme_minimal(base_size = 10) + theme(panel.grid = element_blank()))\n\n\n\n\n\nShow code\n\nobs_paper <- fread(here::here(\"public_data\", \"enso-data-fig1.csv\")) %>% \n  .[, final := initial + change]\nmoments <- obs_paper[, .(mean_initial = mean(initial), \n                         sd_initial = sd(initial),\n                         mean_final = mean(final),\n                         sd_final = sd(final))]\n\n\n\n\n\nShow code\n\nset.seed(42)\nmembers <- 40\nB <- 10000\nsims_normal <- CJ(sim = seq_len(B), member = seq_len(members)) %>% \n  .[, `:=`(initial = rnorm(.N, moments$mean_initial, moments$sd_initial),\n           final   = rnorm(.N, moments$mean_final, moments$sd_final))] %>% \n  .[, change := final - initial]\n\n\n\n\n\n\nShow code\n\ncor <- obs_paper[, cor.test(initial, change)]\n\ncor_text <- with(cor, paste0(\"R = \", signif(estimate, 2), \"\\nP ~ \", signif(p.value, 2)))\n\nobs_paper[, rank := cut_number(initial, 3)]\n\nobs_paper %>% \n  ggplot(aes(initial, change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#0e9a83\") +\n  annotate(\"text\", label = cor_text, x = .51, y = 0, hjust = 0, size = 4) +\n  scale_x_continuous(\"Initial E-index variability (s.d.)\") +\n  scale_y_continuous(\"Change in E-index variability (s.d.)\") \n\n\n\n\nFigure 1: Reproduction of Figure 1b from Cai et al. (2020) based on digitised values. The horizontal axis shows the initial variability of their ENSO index for the first 50 years of simulation (1920 – 1969) and the vertical axis shows the difference between the variability of their ENSO index for the last 50 years of the simulation (2050 – 2099) and the first 50 years.\n\n\n\nFrom this negative correlation, Cai et al. (2020) conclude that “strikingly, in experiments with initially higher ENSO variability (…), [its] amplitude in the future a century later [is] systematically smaller, and vice versa.” They attribute this to a novel mechanism of self-regulation involving non-linear thermal damping through upper-ocean heat exchange and stratification.\nSimulations\nTo test how striking this result really is, I created \\(10^{4}\\) simulations of 40 pairs of random numbers representing the initial and final variability of the E-index for 40 hypothetical ensemble members. The random numbers were taken from independent normal distribution with \\(\\mu=0.8\\) and \\(\\sigma = 0.14\\) for the initial variability and \\(\\mu=0.93\\), \\(\\sigma = 0.072\\) for the final variability. These are the mean and standard deviation from the observed values derived from Figure 1.\nAs each simulated observation is independent from the rest, this simulations serve as a plausible model for the null hypothesis that the initial variability of ENSO does not influence it’s future change.\n\n\nShow code\n\ntwo_plots <- function(sim, obs, n = 1:10) {\n  fits <- sim[, metR::FitLm(change, initial, se = TRUE), by = sim] %>% \n    .[term != \"(Intercept)\"] %>% \n    .[, p_value := pt(abs(estimate)/std.error, df, lower.tail = FALSE), by = sim]\n  \n  obs_fits <- obs[, metR::FitLm(change, initial, se = TRUE)] %>% \n    .[term != \"(Intercept)\"] %>% \n    .[, p_value := pt(abs(estimate)/std.error, df, lower.tail = FALSE)]\n  \n  p_value <- fits[, mean(r.squared >= obs_fits$r.squared)]\n  \n  fit_density <- as.data.table(density(fits[term != \"(Intercept)\"]$r.squared)[c(\"x\", \"y\")])\n  \n  x_pval <- mean(fits[term != \"(Intercept)\"][r.squared >= obs_fits$r.squared]$r.squared)\n  y_pval <- mean(fit_density$y)\n  \n  sim[sim %in% 1:10] %>% \n    .[, change := change] %>% \n    ggplot(aes(initial, change)) +\n    geom_point(size = 0.4, alpha = 0.3) + \n    geom_point(data = obs, size = 0.7) +\n    geom_smooth(data = obs, method = \"lm\", se = FALSE, size = 1.4,\n                fullrange = TRUE, color = \"#0e9a83\") +\n    geom_smooth(method = \"lm\", aes(group = sim), se = FALSE, fullrange = TRUE, \n                color = \"black\", size = 0.1, alpha = 0.5) +\n    scale_x_continuous(\"Initial E-index variability (s.d.)\") +\n    scale_y_continuous(\"Change in E-index variability (s.d.)\") +\n  # coord_equal()  +\n  \n  fit_density %>% \n    ggplot(aes(x, y)) +\n    geom_area(data = ~.x[x >= obs_fits$r.squared], fill = \"#4d4d4d\") +\n    annotate(\"text\", size = 6, color = \"white\",\n             label = scales::percent(p_value), x = x_pval, y = y_pval) +\n    geom_line() +\n    geom_vline(xintercept = obs_fits$r.squared, color = \"#0e9a83\", size = 1.4) +\n    scale_x_continuous(\"r²\") +\n    scale_y_continuous(NULL) +\n    \n    \n    plot_annotation(tag_levels = \"a\")\n}\n\ncompute_pvalue <- function(sim, obs) {\n  fits <- sim[, metR::FitLm(change, initial, se = TRUE), by = sim] %>% \n    .[term != \"(Intercept)\"] %>% \n    .[, p_value := pt(abs(estimate)/std.error, df, lower.tail = FALSE), by = sim]\n  \n  obs_fits <- obs[, metR::FitLm(change, initial, se = TRUE)] %>% \n    .[term != \"(Intercept)\"] %>% \n    .[, p_value := pt(abs(estimate)/std.error, df, lower.tail = FALSE)]\n  \n  fits[, mean(r.squared >= obs_fits$r.squared)]\n}\n\n\n\n\n\n\nShow code\n\ntwo_plots(sims_normal, obs_paper)\n\n\n\n\nFigure 2: a. Ten simulated ensembles of 40 members drawn from random numbers (see description in text) in small dot and their linear regression in fine lines. The observed values from Cai et al. (2020) in big dots and the linear regression in think, green. b. Estimated probability density of the coefficient of determination (\\(r^2\\)) under the null hypothesis model. The vertical bar shows the observed \\(r^2\\) and the shaded area is the proportion of simulated samples from the null hypothesis model with a \\(r^2\\) equal or greater than the observed one.\n\n\n\nFigure 2.a shows a random sample of 10 of these simulations in small dots and their respective linear fit in fine lines. Comparing these random simulations with the observed values taken from Cai et al. (2020) – shown in big dots and thick, green line – puts into perspective the strength of their evidence. The spread of the simulated data is very similar to the real data and all 10 simulation have a steeper regression slope. Both sets are so similar that it would be virtually impossible to distinguish between the simulated and real data if not for the different size of the points.\n\n\nShow code\n\np_value <- compute_pvalue(sims_normal, obs_paper)\n\n\n\nTo quantify how surprising the observed effect actually is, Figure 2.b shows the estimated probability density of the coefficient of determination (\\(r^2\\)) from the linear fit of the 10^{4} simulations and the observed value as a vertical line. 74% of simulations show an \\(r^2\\) equal or greater than the observed one. This translates to a p-value of 0.74; significantly higher than \\(1.4\\times 10^{-12}\\), naively computed from the linear regression.\nThen, it’s evidently clear that under this model of the null hypothesis the observed negative correlation is not only unsurprising, but completely expected.\nThe fact that the initial value of a variable is negatively correlated with its “future change” is one of the manifestation of the general principle of regression to the mean first observed by Galton in 1889 (Francis Galton 1889). He noted that very tall parents tended to have relatively shorter children while tall children tended to have relatively shorter parents.\nIn a perturbed ensemble experiment some ensemble members will have higher initial ENSO variability than average, while other will have lower ENSO variability than average. If this initial ENSO variability is completely independent of further ENSO variability (e.g. the null hypothesis is true), then both groups are equally likely to have average variability further down the simulation. Thus, members which a high initial ENSO variability will tend to show – on average – a negative change in variability, an vice versa.\nFormal formula\nIt’s straightforward to show this effect mathematically. The correlation between two variables \\(x\\) and \\(z = y - x\\) can be written as:\n\\[\\begin{equation}\n\\tag{1}\n\\mathrm{cor}(x, y - x) = \\frac{\\mathrm{cov(x, y) - \\mathrm{var}(x)}}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y) + \\mathrm{var}(x)^2 - 2\\mathrm{var}(x)\\mathrm{cov}(x, y)}}\n\\end{equation}\\]\nIf \\(x\\) and \\(y\\) are independent random variables, \\(\\mathrm{cov}(x, y) = 0\\) vanish and Equation (1) simplifies to\n\\[\\begin{equation}\n\\tag{2}\n\\mathrm{cor}(x, y - x)= \\frac{-1}{\\sqrt{\\mathrm{var}(y)/\\mathrm{var}(x) +1}}\n\\end{equation}\\]\nThis final formula shows that the correlation between \\(x\\) and \\(y - x\\) is bound to be negative and its magnitude depends only on the relationship between their variances. That is, if the variance of \\(y\\) is greater than the variance of \\(x\\), then the correlation will be small, and vice versa.\nApplying Equation (2) to the issue at hand, \\(x\\) becomes the initial ENSO variability and \\(y\\), the final ENSO variability and \\(\\mathrm{var}(x)\\), \\(\\mathrm{var}(y)\\) are the ensemble spread in initial and final ENSO variability, respectively. This means that, even in the case of no linear relation between initial and final variability, any process – either driven by physics or models – which reduces ensemble spread will lead to an even stronger negative linear relationship between the variables.\n\n\nShow code\n\nalpha_obs <- moments[, sd_final/sd_initial]^2\npred <- -1/sqrt(alpha_obs + 1)\n\n\n\nIn the case of equal variances, Equation (2) predicts a correlation of \\(-1/\\sqrt{2} \\sim -0.71\\). The observed correlation of -0.86 (Figure 1) is stronger This is explained by the reduced ensemble spread in ENSO variability between the initial and final periods, as the final spread is around 29% of the initial spread. For this value, Equation (2) predicts a correlation of -0.88, which is very close (if a bit stronger) to the observed value.\n\nConclusion\nThe previous analysis puts the strength of Cai et al. (2020) evidence into perspective. That the change in ENSO variability is greater in ensemble members with initially low ENSO variability is not at all surprising. By simulating the null hypothesis, I show that a negative correlations as strong or stronger than the one observed by Cai et al. (2020) can be expected 74% of the time.\nThese strong negative correlation can be explained by regression towards the mean and reduced ensemble spread in the final period compared to the initial period. I don’t think it is possible to identify the source of the spread reduction. It might be a real response to the forcing scenario or inability of the ensemble to capture all the sources of variability. In particular, all members share the same physics and boundary conditions, therefore I believe it would not be unexpected if all members slowly converged to a similar state. Kay et al. (2015)’s Figure 2 does appear to show that the ensemble spread is slightly higher before 1970. Of note, recently Bengtsson and Hodges (2019) observed secular reductions of ensemble spread in mean surface temperature in an ensemble whose members where also all forced with the same radiative conditions.\nFailing to account for regression to the mean is a fallacy that affects many fields of science – including behavioural science (Kelly and Price 2005) and medicine (Chuang-Stein and Tong 1997) – and features prominently in popular culture, such as the “Sports Illustrated cover jinx” – the perception that an athlete’s performance will be “cursed” after appearing in the cover of Sports Illustrated (Goldacre 2008). Cai et al. (2020) is not the first paper nor will be the last to fall for it.\nDownload code\nClick on this button to get the code that generated this document:\n\n\n Download code\n\n\n\n\n\nBengtsson, L., and K. I. Hodges. 2019. “Can an Ensemble Climate Simulation Be Used to Separate Climate Change Signals from Internal Unforced Variability?” Clim Dyn 52 (5): 3553–73. https://doi.org/10.1007/s00382-018-4343-8.\n\n\nCai, Wenju, Benjamin Ng, Tao Geng, Lixin Wu, Agus Santoso, and Michael J. McPhaden. 2020. “Butterfly Effect and a Self-Modulating El Niño Response to Global Warming.” Nature 585 (7823): 68–73. https://doi.org/10.1038/s41586-020-2641-x.\n\n\nChuang-Stein, Christy, and Donald M Tong. 1997. “The Impact and Implication of Regression to the Mean on the Design and Analysis of Medical Investigations.” Statistical Methods in Medical Research 6 (2): 115–28. https://doi.org/10.1177/096228029700600203.\n\n\nFrancis Galton. 1889. Natural Inheritance.\n\n\nGoldacre, Ben. 2008. Bad Science. HarperCollins UK.\n\n\nKay, J. E., C. Deser, A. Phillips, A. Mai, C. Hannay, G. Strand, J. M. Arblaster, et al. 2015. “The Community Earth System Model (CESM) Large Ensemble Project: A Community Resource for Studying Climate Change in the Presence of Internal Climate Variability.” Bull. Amer. Meteor. Soc. 96 (8): 1333–49. https://doi.org/10.1175/BAMS-D-13-00255.1.\n\n\nKelly, Colleen, and Trevor D. Price. 2005. “Correcting for Regression to the Mean in Behavior and Ecology.” Am Nat 166 (6): 700–707. https://doi.org/10.1086/497402.\n\n\n\n\n",
    "preview": "posts/2021-01-14-enso-and-regression-to-the-mean/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-14-wave4/",
    "title": "Analysis of \"Global wave number-4 pattern in the southern subtropical sea surface temperature.\"",
    "description": "A recent paper claims to have detected a global wave-4 pattern in subtropical SSTs. But have they really?",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-01-14",
    "categories": [
      "zonal waves",
      "general circulation"
    ],
    "contents": "\nThis are some quick notes on Global wave number-4 pattern in the southern subtropical sea surface temperature (Senapati, Dash, and Behera 2021). The article claims to discover a wave-4 pattern in the southern Sea Surface Temperature (SST). Their basic method is to perform Empirical Orthogonal Function (EOF) to SST between 55°S and 20°S. They discard the first EOF as uninteresting because it’s “ENSO-like” (unsurprising) and focus on the second EOF, whose spatial pattern is in Figure 1.\n\n\n\n\nFigure 1: Panel a from Senapati, Dash, and Behera (2021)’s Figure 1: Spatial pattern of second leading EOF mode of SST anomaly over the region (20°S-55°S) from HadSST.\n\n\n\nFirst things first. Download the data and try to reproduce their figure.\n\n\nShow code\n\n# packages\nlibrary(magrittr)\nlibrary(data.table)\nlibrary(metR)\nlibrary(ggplot2)\n\n\n# fast detrend.\nDetrend <- function(y, x) {\n  nas <- is.na(y)\n  m <- mean(y, na.rm = TRUE)\n  if (!hasArg(x)) x <- seq_along(y)\n  y[!nas] <- .lm.fit(cbind(1, x[!nas]), y[!nas])$residuals\n  return(y + m)\n}\n\ntheme_set(theme_minimal() + \n            theme(panel.grid = element_blank()))\n# simple and dirty map\nmap <- ggplot2::map_data(\"world2\") %>%\n  subset(lat %between% c(-90, -0))\n\nquick_map <- list(geom_polygon(data = map,\n                               aes(long, lat, group = group), fill = \"white\", color = \"black\",\n                               size =0.2),\n                  scale_x_longitude(),\n                  scale_y_latitude(ticks = 10),\n                  coord_quickmap(ylim = c(-55, -20)))\n\n\n\n\n\nShow code\n\n# Download and decompress HadSST data\nhad_file <- here::here(\"_data\", \"hadsst.nc\")\nif (!file.exists(had_file)) {\n  hadsst <- \"https://www.metoffice.gov.uk/hadobs/hadisst/data/HadISST_sst.nc.gz\"\n  had_zip <- tempfile()\n  \n  download.file(hadsst, had_zip, mode = \"wb\")  \n  R.utils::gunzip(had_zip, had_file, remove = FALSE)\n}\n\n\n\n\n\nShow code\n\n# Read data and detrend\nsst <- ReadNetCDF(had_file, \n                   vars = c(\"sst\"),\n                   subset = list(latitude = c(-90, -0),\n                                 time = c(\"1979-01-01\", \"2018-12-31\"))) %>% \n  setnames(c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\")) %>% \n  na.omit() %>% \n  .[, lon := ConvertLongitude(lon)] %>% \n  .[, sst := Detrend(Anomaly(sst)), by = .(lon, lat, month(time))]\n\n\n\n\n\nShow code\n\neofs <- sst[lat %between% c(-55, -20)] %>%\n  copy() %>% \n  .[, sst := sst*sqrt(cos(lat*pi/180))] %>% \n  EOF(sst ~ time | lat + lon, n = 1:2, data = .)\n\n\n\n\n\n\nShow code\n\nggplot(eofs$right, aes(lon, lat)) +\n  geom_contour_fill(aes(z = -sst)) +\n  scale_fill_divergent(guide = \"none\") +\n  quick_map + \n  facet_wrap(PC~., ncol = 1)\n\n\n\n\nFigure 2: Spatial patterns of the frist and second leading EOFs of detrended monthly anomalies of SST between 55°S and 20°S weighted by the square root of the cosine of latitude.\n\n\n\nAnd indeed. The first EOF is kind of ENSO-like (not really full ENSO, because I’m missing the equatorial SSTs, which is where ENSO really shines) and the second EOF looks pretty much identical to the their Figure 1.a save the different prime meridian. The time series associated with the second EOF is also almost exactly the same.\n\n\n\nShow code\n\ncut(eofs, 2) %>% \n  .$left %>%\n  copy() %>% \n  .[, sst5 := frollmean(sst, 5, align = \"center\")] %>% \n  ggplot(aes(time, -sst)) +\n  geom_hline(yintercept = 0, size = 0.2, color = \"gray50\")  +\n  geom_line(aes(y = -sst5))  +\n  geom_line(color = \"gray\") +\n  \n  scale_y_continuous(NULL) +\n  scale_x_datetime(date_breaks = \"5 years\", date_labels = \"%Y\")\n\n\n\n\nFigure 3: Temporal pattern of the second EOF in gray with a 5-month running mean in black.\n\n\n\nOK, I’m on the right track.\nNow, my main concern with this paper is whether this pattern is actually, as the title of the paper says, a global pattern. EOF is a great technique for dimensionality reduction, but it’s too easy to end up with statistical patterns that are a mix of actual physical patterns or even just noise.\nThe authors agree, and they say that..\n\nIn order to examine the synchronization of the W4 pattern among all the basins, point correlation analysis has been performed. For this purpose, eight points [i(37.5°S, 173.5°W), ii(37.5°S, 133.5°W), iii(44.5°S, 90.5°W), iv(39.5°S, 40.5°W), v(29.5°S, 2.5°W), vi(41.5°S, 41.5°E), vii(30.5°S, 86.5°E), viii(35.5°S, 130.5°E)] corresponding to the loading centres are selected (marked by green dots, i-viii, in Fig. 1a). The time series of SST anomaly is computed at each grid point after removing the contributions of the first EOF mode (henceforth, reconstructed SST anomaly). Further, point correlation is performed for the time series at the loading centers (Fig. 1a) with the reconstructed SST anomaly (Fig. 2a–h corresponding respectively to points (i) to (viii) of Fig. 1a).\n\nThe problem, IMHO, is that their Figure 2 doesn’t really show as much synchronisation as they claim. First, let’s reproduce it here.\n\n\nShow code\n\n# Define points of interest\npoints <- tibble::tribble(~lat, ~lon,\n                          -37.5, -173.5,\n                          -37.5, -133.5,\n                          -44.5, -90.5,\n                          -39.5, -40.5, \n                          -29.5, -2.5, \n                          -41.5, 41.5,\n                          -30.5, 86.5,\n                          -35.5, 130.5) %>% \n  as.data.table() %>% \n  .[, lon := ConvertLongitude(lon)] %>% \n  .[, id := tolower(as.roman(seq_len(.N)))] %>% \n  .[, sign := rep(c(-1, 1), 4)]   # sign of original correlation\n\n\n\n\n\nShow code\n\n# Reconstruct SST from leading EOF  and add it to \n# original data                     \nsst_reconstruct <- predict(eofs, n = 1) %>% \n  setnames(\"sst\", \"sst_reconstructed\")\n\nsst <- sst[sst_reconstruct, on = .NATURAL]\nrm(sst_reconstruct)\n\n\n\n\n\nShow code\n\n# Compute correlation maps of  points of interest,\n# both with the orignial sst and sst with filtered EOF1\n# (for compatison)\ncorrs <- sst[points, on = .NATURAL] %>% \n  .[, \":=\"(lon = NULL, lat = NULL)] %>% \n  setnames(c(\"sst\", \"sst_reconstructed\"), c(\"ref\", \"ref_reconstructed\")) %>% \n  .[sst, on = \"time\", allow.cartesian = TRUE] %>% \n  .[, .(correlation = cor(sst, ref),\n        correlation_reconstructed = cor(sst - sst_reconstructed, ref - ref_reconstructed)),\n    by = .(lon, lat, id, sign)]\n\n\n\n\n\n\nShow code\n\ncorrs %>% \n  melt(measure.vars = c(\"correlation\", \"correlation_reconstructed\")) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = -value*sign), \n                    breaks = AnchorBreaks(anchor = 0, binwidth = 0.1)) +\n  quick_map +\n  geom_point(data = copy(points)[, id := NULL], shape = 21, fill = NA) +\n  geom_point(data = copy(points), shape = 21, color = \"black\", fill =  \"#0e9a83\") +\n  scale_fill_divergent() +\n  facet_grid(id~variable, labeller = labeller(variable = c(correlation = \"SST\", \n                                                           correlation_reconstructed = \"SST - EOF1\")))\n\n\n\n\nFigure 4: Correlation maps of SST and SST with the first EOF filterred out with the corresponding SST at each one of the points of interest. Compare with Figure 2 of Senapati, Dash, and Behera (2021). The sign of the correlation is flipped for the even points as to preserve always the same sign and allow for easier comparison among maps.\n\n\n\nI purposely computed the two versions. The panels on the right should be a reproduction of Senapati, Dash, and Behera (2021)’s Figure 2. It’s hard to compare them because of the different colour palettes and scale limits (the authors chose parameters that enhanced small correlations), but I personally don’t see much coherency. The first three rows do show high correlations between the three points in the Pacific, which in the unfiltered data looks very much like the well-known PSA pattern that appears as a response to El Niño Southern Oscillation.\nAnother way to look at it is with by plotting \\(r^2\\) which directly quantifies the degree of (linear) dependence between points.\n\n\n\nShow code\n\ncorrs %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = correlation_reconstructed^2, \n                        fill = ..level..), breaks = seq(.1, 1, by = 0.1)) +\n  quick_map +\n  geom_point(data = copy(points)[, id := NULL], shape = 21, fill = NA) +\n  geom_point(data = copy(points), shape = 21, color = \"black\", fill =  \"#de3e80\") +\n  scale_fill_viridis_c(oob = scales::squish,\n                       super = ScaleDiscretised) +\n  facet_grid(id~.)\n\n\n\n\nFigure 5: Coefficient of determination (\\(r^2\\)) computed from the correlations with the filtered SST in Figure 4. Contours only show areas with \\(r^2 > 0.1\\).\n\n\n\nAgain, aside from the points in the Pacific, there is not a lot of long-range relationships between points. As I see it, I strongly suspect that this PC2 pattern is not really robustly global.\n\n\nShow code\n\nset.seed(42)\nN <- 500\nrandom_points <- unique(corrs[, .(lon, lat)]) %>% \n  .[sample(.N, N), ] %>% \n  setkey(lon, lat)\n\nrandom_cors <- vapply(seq_len(N), function(i) {\n  r <- random_points[i, ] \n  ref <- sst[lat == r$lat & lon == r$lon, sst - sst_reconstructed]\n  sst[, ref_data := ..ref, by = .(lon, lat)]\n  sst[, .(cor(sst - sst_reconstructed, ref_data)), by = .(lon, lat)] %>% \n    .[, min(V1)]\n}, numeric(1))\n\n\n\nOk, but how could I quantify this vague impression that these maps don’t show a lot of long-range teleconnections? What I’m going to do is to compute 500 correlation maps like Figure 4 but using random points. For each map, I’ll take the absolute value of the minimum (negative) correlation as the level of connectivity.\n\n\n\nShow code\n\ncorrs %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = -correlation_reconstructed*sign), \n                    breaks = AnchorBreaks(anchor = 0, binwidth = 0.1)) +\n  geom_contour2(aes(z = abs(correlation)), breaks = quantile(abs(random_cors), .5), size = 0.25) +\n  quick_map +\n  geom_point(data = copy(points)[, id := NULL], shape = 21, fill = NA) +\n  geom_point(data = copy(points), shape = 21, color = \"black\", fill =  \"#0e9a83\") +\n  scale_fill_divergent() +\n  facet_grid(id~., labeller = labeller(variable = c(correlation = \"SST\", \n                                                           correlation_reconstructed = \"SST - EOF1\")))\n\n\n\n\nFigure 6: Same as Figure 4 but only for the reconstructed SST. Added in black contours, the 0.5 quantile of random correlations derived from 500 correlation maps with random points.\n\n\n\nFigure 6 repeats Figure 4 but it marks the 50th percentile of the minimum correlation values derived from the bootstrap procedure, which is 0.45. 95% of random points had minimum correlations with absolute values larger than 0.66. With this in mind, correlations of the order of \\(\\pm 0.45\\) are not surprising and what would be expected by chance alone. This, of course, is only a crude measure since it doesn’t take into account the size or pattern of the correlations, but I think that it should give some perspective to the real significance to the correlation levels shown here.\nLet’s try something else. From the correlation maps above (and previous knowledge), it’s pretty obvious that the Pacific sector does behave somewhat coherently. So what I’m going to do is to split the spatial pattern into the Pacific basin (between 150°E and 290°E) and the Atlantic-Indian basins (the rest of the hemisphere). Then, I’m going to project each pattern onto the corresponding SST fields to get two indices. If the patterns is really coherent in time, then both indices must be strongly correlated.\n\n\nShow code\n\nseries <- eofs$right %>% copy() %>% \n  .[PC == \"PC2\"] %>% \n  .[, basin := ifelse(lon %between% c(150, 290), \"pacific\", \"atlantic\")] %>% \n  setnames(\"sst\", \"EOF\") %>% \n  .[sst, on = c(\"lon\", \"lat\")] %>% \n  .[, weighted.mean(sst*EOF, cos(lat*pi/180)), by = .(time, basin)]\n\n\n\n\n\nShow code\n\n# Relationship between the two indices. \nseries %>% \n  dcast(time ~ basin, value.var = \"V1\") %>% \n  ggplot(aes(pacific, atlantic)) +\n  geom_point() +\n  geom_label(data = ~.x[, .(cor(pacific, atlantic))], \n            aes(label = paste0(\"cor= \", signif(V1, 2))), \n            x = -0.0035, y = 0.001, size = 7) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(\"Pacific index\") +\n  scale_y_continuous(\"Atlantic-Indian index\")\n\n\n\n\nFigure 7: Relationship between the Pacific index and Atlantic-Indian index.\n\n\n\nI mean… A correlation of 0.44 is not nothing, but it’s also not a lot.\nLet’s do the correlation map of each index. Again, if the pattern is really global, then the correlation map of the Pacific Index should also show th .e Atlantic-Indian pattern and vice versa.\n\n\nShow code\n\npatterns <- eofs$left[PC == \"PC2\"] %>% \n  setnames(c(\"sst\", \"PC\"), c(\"V1\", \"basin\")) %>% \n  rbind(., series, use.names = TRUE) %>% \n  .[sst, on = \"time\", allow.cartesian = TRUE] %>% \n  .[, .(correlation = cor(V1, sst)), by = .(lon, lat, basin)] \n\nlines <- CJ(lon = c(150, 290),\n            basin = c(\"pacific\", \"atlantic\"))\n\n\n\n\n\nShow code\n\npatterns %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = -correlation, fill = ..level..), \n                    breaks = AnchorBreaks()) +\n  quick_map + \n  geom_vline(data = lines, aes(xintercept = lon)) +\n  scale_fill_divergent_discretised(limits = c(-1, 1)) +\n  facet_wrap(basin~., ncol = 1, labeller = labeller(basi = c(pacific = \"Pacific\", \n                                                             atlantic = \"Atlantic-Indian\")))\n\n\n\n\nFigure 8: Correlation patterns with the Pacific index, the Atlantic-Indian index and the PC2.\n\n\n\nAgain… kinda? For the Atlantic-Indian index, the Pacific signal is barely there and it’s actually completely missing in the Western Pacific. And for the Pacific index, the there is some signal in the Indian ocean, but barely any signal in the Atlantic.\nNow one last test. To extend this analysis, I’ll do the same computation but for every longitude. That is, for each longitude, take a 140º wide section of SST centred in that longitude and project the corresponding wave-4 pattern onto it to get a time-varying index. The result, then, is one “local wave-4 index” for each longitude.\n\n\nShow code\n\nlon_width <- diff(c(150, 290))\nlon_halfwidth <- lon_width/2\n\n# \"extended\" version of geopotential height \n# \nsst2 <- eofs$right %>% copy() %>% \n  .[PC == \"PC2\"] %>% \n  setnames(\"sst\", \"EOF\") %>% \n  sst[., on = .NATURAL] %>% \n  ggperiodic::qwrap(lon = c(0, 360) ~ c(-lon_halfwidth, 360 + lon_halfwidth))\n\n# For each longitude, compute EOF using a segment of lon_width width\n# centered in that longitude.\nlon_eofs <- lapply(unique(sst$lon), function(base_lon) {\n  sst2 %>% \n    .[lon %between% (base_lon + c(-lon_halfwidth, lon_halfwidth))] %>% \n    .[, .(eof = weighted.mean(sst*EOF, cos(lat*pi/180))),\n      by = time] %>% \n    .[, base_lon := base_lon] %>% \n    .[]\n}) %>% \n  rbindlist()\n\n\n\n\n\nShow code\n\nk <- lon_eofs %>% \n  widyr::pairwise_cor(base_lon, time, eof) %>% \n  as.data.table() %>% \n  .[, correlation := 1 - abs(correlation)] %>% \n  dcast(item1 ~ item2, value.var = \"correlation\") %>% \n  .[, -1] %>% \n  as.dist() %>% \n  hclust() %>% \n  cutree(3)\n\nsections <- data.table(lon = as.numeric(names(k)), k = k)\ncuts <- sections[c(0, diff(k)) != 0]\n\nlabel_k <- c(\"1\" = \"East Pacific & Atlantic\", \n             \"2\" = \"Indian\",\n             \"3\" = \"West Pacific\")\n\n\ncuts_lon <- LonLabel(cuts$lon)\ncuts_lon_round <- LonLabel(round(cuts$lon/5)*5)\n\n\n\n\n\nShow code\n\nlon_eofs %>% \n  widyr::pairwise_cor(base_lon, time, eof) %>% \n  ggplot(aes(item1, item2)) +\n  geom_contour_fill(aes(z = correlation, fill = ..level..), na.fill = 1) +\n  geom_contour2(aes(z = correlation), size = 0.2) +\n  geom_text_contour(aes(z = correlation, stroke.color = ..level..), color = \"black\",\n                    stroke = 0.2) +\n  scale_fill_divergent(\"Correlation\",\n                       super = ScaleDiscretised) +\n  scale_color_divergent(aesthetics = \"stroke.color\", guide = \"none\") +\n  scale_x_longitude() +\n  scale_y_longitude() +\n  coord_equal() \n\n\n\n\nFigure 9: Pairwise correlation of “local wave-4” indices.\n\n\n\nFigure 9 shows the pairwise correlation between all indices. Correlation drop rapidly with distance, but there are three clearly defined regions of high correlation that could represent various teleconnected areas. Perhaps not coincidentally, they correspond approximately to the three oceanic basins. The Indian between 0º and 120ºE, Western Pacific between 120ºE and 120ºW and Eastern Pacific and Atlantic between 120ºW and 0º.\nBased on these correlations, we use hierarchical clustering with 1 - abs(correlation) as distance measure to classify each longitude into each of 3 groups. The clusters are flanked by the longitudes 17.5°E, 106.5°E, and 120.5°W, which agree well with the visual interpretation of Figure 9. Using this classification, I now create three indices by again projecting the corresponding wave-4 pattern into SST anomalies.\n\n\nShow code\n\nseries_k <- eofs$right %>% copy() %>% \n  .[PC == \"PC2\"] %>% \n  .[sections, on = \"lon\"] %>% \n  setnames(\"sst\", \"EOF\") %>% \n  .[sst, on = c(\"lon\", \"lat\")] %>% \n  .[, .(value = weighted.mean(sst*EOF, cos(lat*pi/180))), by = .(time, k)] %>% \n  .[, value := as.numeric(scale(value)), by = .(k)]\n\n\n\n\n\nShow code\n\npatterns_k <- series_k %>% \n  .[sst, on = \"time\", allow.cartesian = TRUE] %>% \n  .[, .(correlation = cor(value, sst)), by = .(lon, lat, k)] \n\n\n\n\n\n\nShow code\n\npatterns_k %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = -correlation, fill = ..level..), \n                    breaks = AnchorBreaks(0)) +\n  quick_map + \n  geom_raster(data = unique(patterns_k[, .(lon, lat)])[sections, on = \"lon\"],\n              alpha = 0.2) +\n  scale_fill_divergent_discretised(\"Correlation\", limits = c(-1, 1)) +\n  facet_wrap(k~., ncol = 1, labeller = labeller(k = label_k))\n\n\n\n\nFigure 10: Correlation maps between SST and each of the three basin-dependend wave-4 index. Overlayed in gray, the tree distinct areas of shared variability identified in Figure 9 by hierarchical clustering and selecting 3 clusters.\n\n\n\nCorrelation maps between SST anomalies and each of the three indices are shown in Figure 10. Inside the area used to define each index correlation are high and the pattern is well defined, as expected by construction. However, outside those areas, there is very little signal.\n🤷. Take it as you will. From what I’ve seen here, I remain unconvinced that this is a global pattern of SST.\nDownload code\nClick on this button to get the code that generated this document:\n\n\n Download code\n\n\n\n\n\nSenapati, Balaji, Mihir K. Dash, and Swadhin K. Behera. 2021. “Global Wave Number-4 Pattern in the Southern Subtropical Sea Surface Temperature.” Scientific Reports 11 (1): 142. https://doi.org/10.1038/s41598-020-80492-x.\n\n\n\n\n",
    "preview": "posts/2021-01-14-wave4/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 403
  }
]
