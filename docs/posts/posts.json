[
  {
    "path": "posts/2024-04-18-MJO-RMM/",
    "title": "An index for the MJO using complex Empirical Orthogonal Functions",
    "description": "An early exploration of using cEOF to characterise the Maden-Julian Oscillation.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2024-04-29",
    "categories": [
      "ceof",
      "mjo"
    ],
    "contents": "\nThe Madden-Julian Oscillation is a tropical oscillation located mainly over the Indian and western Pacific oceans.\nIt’s not a standing oscillation, but instead it’s more like a propagating wave of enhanced and reduced convection that moves eastward.\n\n\n\n\nFigure 1: MJO schematic. From https://www.climate.gov/news-features/blogs/enso/what-mjo-and-why-do-we-care.\n\n\n\nDue to its propagating nature, it cannot be reproduced by a single EOF so MJO indices use two EFOs.\nThe Real-time Multivariate MJO (RMM) index is made up of the first leading EOFs of Outgoing Longwave Radiation, 200hPa zonal wind and 850 hPa zonal wind anomalies in the tropics.\nThey do some filtering of the time series to remove the seasonal cycle, short-scale fluctuations and the impact of El Niño-Southern Oscillation too.\nI think complex Empirical Orthogonal Functions (Horel 1984) might be a great fit for this kind of index, because they can naturally represent propagating patterns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe methods used to derive the RMM index are listed in Wheeler and Hendon (2004).\nIn short they are, for each variable averaged between 15ºS and 15ºN,\nremove the annual cycle, estimated as the waves 0 through 3 of daily means,\nremove the linear effect of ENSO using the monthly ONI interpolated to daily values,\nremove the 120 day running mean, and\nnormalise by the global standard deviation.\nHere I reproduce the method to the best of my ability.\nThe only difference is that, due to what data easily available to me, I will be using the 1994–2024 period instead of the 1979–2001 to define the annual cycle, remove the (linear) effect of ENSO and compute the EOFs.\nThe dataset has a few missing values, which I impute using DINEOF (Alvera-Azcárate et al. 2011).\n\n\n\nTo compute the cEOF, an extra fifth step is to “enrich” the original data by applying the Hilbert transform.\nIn the literature they usually apply this step in the time domain: considering the signal as an oscillation in time.\nIn this case, because this is a longitudinally-propagating wave, I’m computing this in the zonal domain: considering the signal as an oscillation in space.\nThe same way that EOFs are only defined up to a change in sign, cEOFs are only defined up to a rotation in the complex plane.\nAny rotation is equally “real” but to make it comparable with the RMM index, I will rotate the leading cEOF so that both indices are maximally correlated.\nTo compute the correlation between two bivariate indices I treat them as vectors, and compute their correlation as the mean cosine of the difference between their phases weighted by the product of their amplitudes.\nInstead of labelling each component as the “Real” and “Imaginary” part, I use the angle between each and the positive real line.\nSo the real part is the 0º phase and the imaginary part is the 90º phase.\nHere, the RMM1 index is aligned with the 0º phase and the RMM2 with the 90º phase.\n\n\n\n\n\n\nThe spatial pattern of the cEOF is shown in Figure 2, in which the map is shown only for reference, the vertical coordinates are arbitrary and the dark band indicates the area in which the variables were averaged.\n\n\n\n\nFigure 2: Spatial patterns of the leading cEOF of OLR, 850 hPa zonal wind and 200 hPa zonal wind anomalies.\n\n\n\nCompare this figure with Wheeler and Hendon (2004)’s Figure 1.\nThe MJO in its 0º phase is characterised by increased convection over Indonesia (around 120ºE), which is evident by the OLR minimum, convergence at the lower levels (negative slope in the 850 hPa zonal wind) and divergence at upper levels (positive slope in the 200 hPa zonal wind) and reduced convection in the western Indian ocean and Africa as evidenced by the inverse signal.\nThis is equivalent to the phases 4 and 5 in the RMM diagram.\nIn its 90º phase, the enhanced convection is over the Indian ocean, with drier conditions east of Indonesia.\nThe amplitude of the signal, particularly for OLR, is maximum over the Indian ocean and Indonesia, with little to no signal in the eastern Pacific, South America and the Atlantic Ocean (Fig. 3).\n\n\n\n\nFigure 3: Amplitude of the OLR component of the cEOF.\n\n\n\nA buttery smooth and oddly satisfying animation of the evolution of the cEOF shows how the wet and dry sections travel around the tropics (Fig. 4).\n\n\n\n\nFigure 4: Animation showing all the phases of the cEOF.\n\n\n\nThe correlation between the RMM index and the cEOF is 0.96, so they are essentially the same indices.\nFigure 5 show the trajectory of the two indices between March 6th 2024 and March 31st 2024 with arrows showing the difference.\n\n\n\n\n\n\n\nFigure 5: Sample trajectory in the RMM/cEOF phase space between March 6th 2024 and March 31st 2024. Black arrows indicate the difference between the two indices.\n\n\n\nThey are almost identical up to an arbitrary scale factor.\nThis scale factor comes up because the RMM1 is scaled so that each component has unit standard deviation in the climatological period, but I scaled the cEOF so that its amplitude has unit standard deviation1.\n\n\n\nThe arbitrary constant makes it hard to compare the amplitude of each index.\nThe BOM uses amplitude equal to 1 to more or less define when the MJO is active, but that same cut-off is not necessarily useful for the cEOF index.\nThere are a few ways of getting an equivalent threshold for the cEOF index.\nRun a linear regression of the cEOF amplitude as a function of RMM amplitude and use the cEOF amplitude that corresponds to 1 RMM amplitude.\nDo the same but using orthogonal regression, which might be more appropriate in this case because this procedure should be symmetrical.\nCompute the quantile corresponding to the RMM threshold and use the value of that quantile in the cEOF amplitude series.\nThankfully the three approaches give you basically the same answer (this might not be a coincidence), but I kind of like the 3rd one better.\nFor the record, the quantile associated with 1 RMM amplitude is 0.38, which translates to a threshhold value of 0.69 in cEOF amplitude.\nFigure 6 shows the relationship between the amplitude of each index and a line of best fit using orthogonal regression for each month and the variance explained by the line (this is actually the first Principal Component of the two series).\n\n\n\n\n\n\n\nFigure 6: Relationship between RMM amplitude and cEOF amplitude for each month. The blue line shows the orthogonal regression line, whose explained variance is shown as text, and the horizonal and vertical black lines indicate the 0.38 quantile of each series (computed for the whole period and not for each month).\n\n\n\nAlthough both variables are clearly highly correlated, there are some differences which make a sizeable proportion of days “active” by the cEOF definition but not “active” by the RMM definition and vice versa, especially in the boreal winter months.\nAn important characteristic of the MJO is its intraseasonal timescale of between 30 and 80 days.\nWheeler and Hendon (2004) computes the spectra of each RMM index, but with I can just compute the spectrum of the complex cEOF.\nThe three spectra are very similar (Fig. 7).\n\n\n\n\nFigure 7: Smoothed power spectra of the RMM1, RMM2 and cEOF indices scaled to unit area.\n\n\n\nThe main difference is that the cEOF spectrum is a bit more concentrated in the intraseasonal range and doesn’t have that “bump”around 200 days that is visible in both the RMM1 and RMM2 indices.\nI don’t know if that’s something to do with the method or with the change in climatological period.\nIn any case, I think this also highlights one advantage of treating a bivariate index as a complex signal, since it allows you to study a single spectrum for the whole signal.\n\n\n\n\n\n\n\n\n\nFinally, Figure 8 shows the regression between OLR and the MJO using three different methods.\nBoth the “RMM”and “cEOF”panels are the regression of OLR anomalies with the respective index amplitude for each “pizza slice” phase (i.e. the 1 phase encompases phases from -180º to -135º).\nA problem with this method is that each panel discards a lot of information since only around 12% of observations fall into each slice.\nThe “cEOF - linear” panels are obtained by first computing the multivariate linear regression of OLR with the 0º and 90º phases of the complex series and then using a linear combination to compute the regression associated with any other phase.\nThis method assumes that the relationship between OLR and the MJO is linear in every phase and that the total effect of the MJO can be linearly divided into the effect of two orthogonal phases (i.e. that the 0º phase is equal an oposite to the 180º phase and that the 45º phase is the combined effect of the 0º phase and the 90º phase in equal measure).\nThe big advantage of this method is that it uses al the information available.\n\n\n\n\nFigure 8: Linear regression between each index and OLR for different phases. The “pizza slice” used to construct each panel or the phase represented by each panel is shown by the small inset in the lower-left corner of each panel.\n\n\n\nAll three methods give more or less the same result that are (naturally) consistent with the cEOF description above; the “wet patch” moves east, reaching its maximum intensity over the Indian ocean and then being replaced by a “dry patch” with a similar behaviour.\nThere is some differences in the small details, but it’s not trivial to know how much of that-small scale structure is just sampling noise.\nThe linear method has the advantage of resulting in smoother patterns with less noise and more large-scale signal.\nOn the other hand, the linear method seems to exaggerate the intensity of the dry patch in phases 6-7, since this has to be equal and opposite to the wet patch in phases 2-3 by construction.\nThe linear method also creates buttery smooth and oddly satisfying animations.\n\n\n\n\nFigure 9: Animation of OLR regression on the difference phases of the cEOF.\n\n\n\n\n\n\nAlvera-Azcárate, A., A. Barth, D. Sirjacobs, F. Lenartz, and J. M. Beckers. 2011. “Data Interpolating Empirical Orthogonal Functions (DINEOF): A Tool for Geophysical Data Analyses.” Mediterranean Marine Science 12 (3): 5. https://doi.org/10.12681/mms.64.\n\n\nHorel, J. D. 1984. “Complex Principal Component Analysis: Theory and Examples.” Journal of Applied Meteorology and Climatology 23 (12): 1660–73. https://doi.org/10.1175/1520-0450(1984)023<1660:CPCATA>2.0.CO;2.\n\n\nWheeler, Matthew C., and Harry H. Hendon. 2004. “An All-Season Real-Time Multivariate MJO Index: Development of an Index for Monitoring and Prediction.” Monthly Weather Review 132 (8): 1917–32. https://doi.org/10.1175/1520-0493(2004)132<1917:aarmmi>2.0.co;2.\n\n\nThe variance can be generalised to complex signals naturally by considering it as the mean squared distance between each point and the average point.↩︎\n",
    "preview": "posts/2024-04-18-MJO-RMM/img/mjo.png",
    "last_modified": "2024-05-03T11:29:21+10:00",
    "input_file": {},
    "preview_width": 1000,
    "preview_height": 677
  },
  {
    "path": "posts/2023-06-01-stop-composites/",
    "title": "Stop using composites",
    "description": "Composites are bad. Use regressions instead.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2023-06-02",
    "categories": [
      "statistics"
    ],
    "contents": "\n\n\n\nComposites are a very common method used in atmospheric.\nTake a continuous index, discretise it into two categories defined by some threshold and then compute the mean anomaly for each category.\nMany researchers use this to show the “typical” expected value for “events”, such as the effect of El Niño events on precipitation.\nSometimes it’s also used to try to find non-linear effects by comparing composites for positive and negative events (e.g. El Niño and La Niña events).\nFigure 1 is an example.\nIt shows composites of wintertime precipitation anomalies for El Niño and La Niña years, defined as winters with mean Sea Surface Temperatures in the Niño 3.4 region (that’s the ONI index) than 0.5 or lower than -0.5 respectively.\n\n\n\n\n\nFigure 1: Observed (1979 – 2019) austral winter composites of precipitation anomalies for La Niña and El Niño winters.\n\n\n\nThey seem to indicate that precipitation anomalies associated with La Niña are somewhat waaker and less extensive.\nThis would suggest a non-linear effect in which the effect of negative Niño events is not the same as the negative of the positive Niño events.\nHowever, this is an illusion.\nI am not a fan of composites for three reasons:\nBy selecting “events” using a threshold, composites discard a lot of perfectly good data.\nEstimation of non-linear effects is much harder and noisier than of linear effects, so you need a lot of data. But we usually don’t have as much data as we want and, even then, this method is not data-efficient (related to 1).\nResults can be very sensitive to the chosen thresholds.\nThe mean value of a variable conditioned on an event depends on the mean intensity of the events.\nComposites are data-inefficient\nTable 1 which shows the number of years in each phase.\n\n\nTable 1: Number of years in each phase of ENSO\n  Phase\n      N\n    Neutral\n29El Niño\n6La Niña\n6\n\nThe original wintertime ONI index has 41 data points (years), but only selecting years that are larger than 0.5 or lower than -0.5 throws away more than 2/3 of the data.\nThis is terribly data-inefficient.\nThe composites of El Niño and La Niña years in Figure 1 are based on only 6 years each.\nThis is extremely tiny data, which translates to huge uncertainty in the computed means.\nTo illustrate this, Figure 2 plots maps of the lower and upper values for the 95% confidence intervals from a t-test.\nAccording to these composites, both El Niño and La Niña are compatible with zero signal in the precipitation in the central Pacific.\n\n\n\n\n\nFigure 2: Lower and upper values for 95% confidence intervals from a t-test comparing the mean precipitation anomaly in El Niño and La Niña years.\n\n\n\nNon-linear effects are harder to estimate than linear effects\nIf these composites have a hard time detecting the (gigantic) zero-order effect of El Niño, even less can they detect any non-linear effects.\nIndeed, the differences between the composites are not statistically significant.\nFigure 3 shows the result of a t-test on precipitation anomalies during El Niño years and negative precipitation anomalies during La Niña years to test if the effect of El Niño is not significantly different from the negative effect of La Niña.\nThe differences are barely significant at very few locations and the significance completely vanishes if one controls for multiple comparisons (Walker 1914; Wilks 2016).\n\n\n\n\n\nFigure 3: Difference between the mean precipitation anomaly in El Niño winters and the negative precipitation anomaly in La Niña winters. Crosses indicate raw p-values lower than 0.01.\n\n\n\nThis indicates that, using composites, precipitation anomalies during El Niño are almost indistinguishable to precipitation anomalies during La Niña reversed in sign.\nComposites depend on an arbitrarily-chosen threshold\nThe ±0.5 threshold used to define El Niño and La Niña years is arbitrary.\nThere’s no reason to think that the atmosphere cares that much if the ONI is 0.45 vs. 0.55.\nFigure 4 shows the same composites as Figure 1 but using ±0.1 instead of ±0.5 to define the phases.\nThese composites seem to suggest that precipitation anomalies are actually more sensitive to La Niña than to El Niño.\n\n\n\n\n\nFigure 4: Same as Figure 1 but for El Niño and La Niña defined using a ±0.1 threshold.\n\n\n\nChanging the threshold to ±0.3, however, now suggests no difference (Fig. 5).\n\n\n\n\n\nFigure 5: Same as Figure 1 but for El Niño and La Niña defined using a ±0.3 thershold.\n\n\n\nComposites don’t actually show the relationsip between variables\nTechnically what these plots are showing are, for each cell in the map, the expected value of precipitation anomalies conditioned on the ONI index falling into the El Niño or La Niña categories.\nCrucially, it doesn’t tell us how much precipitation increases or decreases when the ONI increases or decreases by 1 unit.\nThis is shown in cartoon form in Figure 6.\n\n\n\n\n\nFigure 6: Sketch showing a hypothetical linear relationship between ONI and precipitation anomalies at some point and the values of the mean precipitation anomalies for year with ONI greater than 0.5 or lower than -0.5.\n\n\n\nIn this fake example, the ONI index has larger mean magnitude in El Niño years than in La Niña years.\nIn other words, the typical El Niño is stronger than the typical La Niña.\nSo even though the relationship between precipitation and the ONI index is linear, the typical precipitation anomaly during El Niño is stronger than the typical precipitation anomaly during La Niña.\nTable 2 shows the mean mean magnitude of the ONI index during El Niño and La Niña years.\nEvidently the ONI index has greater mean magnitude during El Niño than La Niña.\nTherefore, composites using La Niña years will have smaller values (in magnitude) than composites using El Niño years even if the relationship were linear.\n\n\nTable 2: Mean ONI value for each phase. \n  Phase\n      ONI\n    El Niño\n1.16La Niña\n−0.89\n\nFigure 7 illustrates this effect on fake composites.\nHere I’ve created fake precipitation fields so that they have a perfectly linear relationship with the ONI index.\nHowever, the composite of La Niña years shows weaker anomalies than the composites of the El Niño years.\nIn other words, these plots suggest a non-linear effect even though the relationship is linear by construction.\n\n\n\n\n\n\n\n\nFigure 7: Same as Figure 1 but from synthetic precipitation anomalies that are perfectly linearly related with ONI by construction.\n\n\n\nThis problem can be corrected by normalising the composites by the absolute value of the mean ONI for each phase, like I did in Figure 8 using real precipitation fields.\nIn this figure, the map show the expected increase or decrease in precipitation due to a unit change in the ONI index in La Niña or El Niño years and suggest that precipitation anomalies are not more sensible to El Niño than La Niña.\n\n\n\n\n\nFigure 8: Same as Figure 1 but the composites are divided by the absolute value of the mean ONI in each phase.\n\n\n\nUse piecewise regression instead\nComposites throw away a lot of precious and perfectly good data, can’t actually detect non-linear effects unless they are really strong, and don’t actually show the relationship between the variables. So what’s the alternative?\nAn alternative is to compute simple linear regressions or correlations using all the data and don’t bother with trying to find non-linearities in relatively small datasets.\nThe sooner we accept that in many cases we only (barely) have enough data to estimate first-order effects, the better.\nBut if you really want to look for non-linearity, then fit a piecewise regression with the breakpoint at zero.\nThis will give you a slope for negative values and a slope for positive values.\nEach slope will be computed using all the (positive or negative) values and the line will be continuous at zero.\nSo, in this case, piecewise regression will give you an estimate of the change in precipitation when the ONI changes in one unit for positive values (~ El Niño) and negative values (~ La Niña) without discarding precious data, depending on arbitrarily chosen thresholds, nor suffering from the mean value issue.\nPiecewise regression is also not hard to compute when the breakpoint is fixed, like in this example.\nYou just need to fit the linear regression:\n\\[\npp \\sim ONI + (ONI - ONI_0)\\times I_{ONI\\le ONI_0}\n\\]\nWhere, \\(ONI_0\\) is the breakpoint value (0, in this case) and \\(I_{ONI\\le ONI_0}\\) is an indicator function that is 1 when \\(ONI\\le ONI_0\\) and 0 when is not (this translates simply to ONI <= ONI0 in any statistical software).\nUsing this formulation, the first regression coefficient corresponds to positive ONI and the second regression coefficient corresponds to the difference between positive ONI and negative ONI.\nThis means that piecewise regression directly computes the statistical significance of the difference between the slopes, providing you with a number to detect those pesky non-linear effects.\n(Although, again, you probably don’t have enough data to detect them and, since piecewise regression fits more parameters, estimates will be noisier than simple linear regression estimates; you really need to look out for type S and type M errors.)\nFigure 9 shows the result of piecewise regression applied to the relationship between ONI and precipitation (in that plot, I’ve flipped the sign of the La Niña values to match the composites).\n\n\n\n\n\nFigure 9: Linear regression slopes for positive and negative ONI values using a piecewise linear model with fixed breakpoint at ONI = 0. Crosses indicate raw p-values lower than 0.01 (yep, there’s only one).\n\n\n\nThis figure shows that the effect of La Niña on precipitation is of similar magnitude to the effect of El Niño.\nThe difference between the two signs of ONI is basially not significant even before ajustinf for multiple comparisons.\ntl;dr\nSo, composites throw away perfectly good data, can’t detect non-linear effects reliably, are not always robust to threshold choice and don’t actually show the relationship between the variables.\nDon’t use them.\nIn most cases, it’s better to use simple linear regressions (or correlations) to study first-order linear effects, as you probably don’t have enough data to reliably detect non-linearities. I\nIf you really want to check for non-linear effects, piecewise regression uses all the data and will even provide statistical significance of the difference.\n\n\n\nWalker, Sir Gilbert Thomas. 1914. Correlation in Seasonal Variations of Weather, III: On the Criterion for the Reality of Relationships Or Periodicities. Meteorological Office.\n\n\nWilks, D. S. 2016. “‘The Stippling Shows Statistically Significant Grid Points’: How Research Results Are Routinely Overstated and Overinterpreted, and What to Do about It.” Bulletin of the American Meteorological Society 97 (12): 2263–73. https://doi.org/10.1175/BAMS-D-15-00267.1.\n\n\n\n\n",
    "preview": "posts/2023-06-01-stop-composites/stop-composites_files/figure-html5/composites1-1.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 864
  },
  {
    "path": "posts/2022-09-20-uwind-sinewave/",
    "title": "Mini replication of \"Subseasonal Vacillations in the [boreal] Winter Stratosphere\"",
    "description": "Fun fitting a sine model",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2022-09-20",
    "categories": [
      "stratospheric circulation"
    ],
    "contents": "\nLast week on the Journal Club I participate in we read Subseasonal Vacillations in the [boreal] Winter Stratosphere (Hardiman et al. (2020)).\nThe data and methods were not too complicated and I had some small issues with some of the decisions so I was delighted to once again delve into a paper and try to reproduce it.\nThe underlining theory is that, due to the interaction between the zonal mean flow and the zonal waves, the stratospheric jet “vacillates” (d.h. oscillates) in a relatively regular way.\nFrom that, the authors posit that a simple sine wave model could predict the jet strength.\nThe conclusions are not very strong in that while the sine wave model is a relatively better fit than a linear model, it doesn’t have increased predictive power.\nHere I’ll replicate these methods with some minor changes and explore the results.\n\n\n\nFirst, I downloaded 00:00 UTC zonal wind hourly data from ERA5 at 10 hPa between between 55ºN and 65ºN.\nFigure 1 shows the timeseries, which has a very strong seasonal cycle.\n\n\n\n\nFigure 1: Daily zonal mean zonal wind at 10 hPa, averaged between 55ºN and 65ºN.\n\n\n\nTo remove the seasonal cycle, Hardiman et al. (2020) removed the climatological mean following MacLachlan et al. (2015):\n\nUsing the climatological (i.e., average across all years) daily U data, a rolling 61 day mean is formed (i.e., ±30 days), weighted with the function\n\\[w=\\mathrm e^{\\frac{-d^2}{100}}\\]\nwhere d is the lag/lead in days from the central day (i.e., an exponential decay on a time scale of 10 days). This climatological daily rolling mean is then removed from the daily winds for each year, producing the anomalous daily wind field.\n\nThe idea here is that the seasonal cycle computed by just the daily averages can be noisy due to sampling variability.\nSo they smooth it out with a weighted rolling mean function.\nThis might be fine, but I’m usually not a fan of rolling mean for this because it can have issues at the boundaries.\nNot only the mean is computed from fewer datapoints, but there is no assurance that the annual cycle to be periodic (the 31st of December needs to be almost equal to the 1st of January).\nMy preferred solution is to use Fourier (I dare you to find a problem that’s not solved with Fourier).\nJust filter out the higher wavenumbers to get a smooth and periodic seasonal cycle.\nIn this case, I filtered out wavenumbers greater than 3.\n\n\n\n\n\n\n\nFigure 2: Seasonal cycle of zonal mean zonal wind at 10 hPa, averaged between 55ºN and 65ºN. Raw cycle in read and a smoothed version in blue.\n\n\n\nAs you can see in Figure 2, the raw climatology is not super noisy (I’ve seen noisier) but the smoothed-out version is much better.\nThe difference is specially important in the boreal Winter months, which is the ones we are interested in!\nThe choice of maximum wavenumber does change the smooth slightly (of course).\nI chose 3 just by eye, but I suspect one could get to a more objective and automatic number using cross-validation (I dare you to find a problem that’s not solved with cross-validation).\n\n\n\nNext, after removing this smooth climatology, Hardiman et al. (2020) fits a sine wave model to each year’s November-March period.\nInitially they fit the model\n\\[\nU = \\bar{U} + A\\sin(\\mathit{freq}(\\mathit{time} + \\mathit{phase}))\n\\]\nin which \\(\\bar{U}\\) is the mean zonal wind anomaly of that winter (what they call “offset”), \\(A\\) is the amplitude of the vacillation, \\(\\mathit{freq}\\), its frequency and \\(\\mathir{phase}\\), its phase.\nAfter fitting this model, they then decide to opt for a simpler model with only \\(\\bar{U}\\) and \\(\\mathit{phase}\\) as free parameters and \\(A\\) and \\(\\mathit{freq}\\) fixed as \\(12\\ ms^{−1}\\) and 120 days respectively, which are approximatedly the mean values from the 4-parameter fit.\nI honestly don’t really get why they do this.\nWhile it’s important to be mindful of the fact that the goodness of fit improves monotonically with the number of parameters, I don’t see why 4 parameters is too much and 2 is fine.\nConsidering that each winter has between 151 and 152 observations (depending on the leap-ness of the year), 4 parameters doesn’t seem to be anywhere near the overfitting danger-zone.\nBesides, since they fix the two other parameters based on the same data, it’s not even technically true that they are only fitting 2 parameters!\nFrom a physical argument I would believe that fixing the frequency is desirable if one knows the approximate timescale of the phenomenon.\nBut the change amplitude is a fundamental aspect of the vacillations; the main idea of this process is that a weaker jet will have stronger oscillations.\nThey even test this with their 4-parameter fit.\nSo I don’t really understand why they decided to keep it fixed.\nI will ignore this parameter restriction and instead fit the full 4-parameter model.\nThe only restriction is that the maximum period I’m allowing is 300 days.\nGetting the sine wave fit was a bit of a challenge.\nAt first I was trying to use the nls() function, which performs Nonlinear Least Squares, but it not only errored out plenty of times due to numerical issues, but even when it ran without errors it would return nonsensical fits.\nIn the end, I decided to linearise the sine wave model for a fixed frequency, fit this linear model for a relatively large set of frequencies, and keep the model with the lowest Residual Sums of Squares.\nBelow is the R code I came up with, for which I’m not entirely proud, but also not completely ashamed.\n\n\nfit_sin <- function(x, time) {\n  t <- seq_along(x)\n  \n  # Try a bunch of periods and 120 (from the authors)\n  # (linear grid probably not the best)\n  periods <- 150*seq(1/10, 2, length.out = 200)\n  periods <- c(periods, 120)\n  frequencies <- 2*pi/periods\n  \n  models <- list()\n  for (k in seq_along(frequencies)) {\n    models[[k]] <- .lm.fit(cbind(1, cos(frequencies[k]*t), sin(frequencies[k]*t)), x)\n  }\n  \n  # Chose the one with lower RMSE\n  rmse <- vapply(models, function(model) sd(resid(model)), numeric(1))\n  model <- models[[which.min(rmse)]]\n  model120 <- models[[length(periods)]]\n  \n  # Return for the best and the 120-day model:\n  #   a model element with the fitted parameters\n  #   a fit element with fitted values, residuals and stuff. \n  resid <- resid(model)\n  resid120 <- resid(model120)\n  list(model = data.table::data.table(mean = coef(model)[1], \n                                      amplitude = sqrt(coef(model)[2]^2 + coef(model)[3]^2),\n                                      frequency = frequencies[which.min(rmse)],\n                                      period = periods[which.min(rmse)],\n                                      phase =  atan(coef(model)[3]/coef(model)[2])),\n       fit = data.table::data.table(\n         pred = x - resid,\n         resi = resid,\n         x = x,\n         t = seq_along(x),\n         time = time\n       ),\n       model120 = data.table::data.table(mean = coef(model120)[1], \n                                         amplitude = sqrt(coef(model120)[2]^2 + coef(model120)[3]^2),\n                                         frequency = frequencies[length(periods)],\n                                         period = periods[length(periods)],\n                                         phase =  atan(coef(model120)[3]/coef(model120)[2])),\n       fit120 = data.table::data.table(\n         pred = x - resid120,\n         resi = resid120,\n         x = x,\n         t = seq_along(x),\n         time = time\n       )\n       \n  )\n}\n\n\n\n\n\nFigure 3 shows the first nine years with their fit in blue.\nIn some years, such as 1979and 1981 the jet does show oscillations consistent with the sine wave fit.\nIn other years, like 1981 of 1985, the amplitude of the oscillation is minimal and not a very good fit.\n\n\n\nFigure 3: (ref:examples-cap)\n\n\n\nThe dotted line show the fit if the frequency is kept fixed at 120 days but the amplitude is allows to vary (so a 3-parameter fit).\nBoth lines feature a similar fit to the data since they only diverge when the amplitude of the wave is low.\nIn those year, the sinusoidal is more or less a straight line and thus the period makes little difference to the fit.\nCompared with Hardiman et al. (2020)’s Figure 2 reproduced below, their 2-parameter fit is not super bad for the years with reasonable oscillations, but it overfits strong vacillations in the “flat” years.\nThis is mainly due to the fixed amplitude, as discussed before.\n\n\n\n\nFigure 4: Same as Figure 3 but from Hardiman et al. (2020).\n\n\n\n\n\n\n\n\nFor completeness, Figure 5 shows all fits for all years.\nYou can sort the years chronologically or by the difference between the two fits.\nHonestly, the fixed-frequency model is not super bad except for a few cases.\nBut that’s not mind-blowing, since the fixed number was actually based on the full 4-parameter model.\n\n\n\n\nFigure 5: Same as Figure 3 but for all years. Ordered chronologically or by the difference between the two fits.\n\n\n\nWeave-mean flow interaction theory predicts that a weaker jet should be more wavy than a stronger jet.\nTherefore, there should be a negative correlation between the mean zonal wind zonal mean and the amplitude of the vacillations.\nHardiman et al. (2020) claim to have found this negative correlation:\n\n(…) across all years, the offset of these sine waves is negatively correlated to their amplitude (correlation coefficient =−0.36 […]).\n\nFigure 6 shows the relationship between the mean zonal wind and the amplitude in the 4-parameter model.\nThere seems to be a relationship, but is not as simple as the model predicts.\nIs not just a monotonic decrease in amplitude with increase in jet strength, but for some critical value of jet stength, the amplitude is relatively constant or even increasing; although there are not a lot of datapoints to make the relationship very clear.\n\n\n\nFigure 6: (ref:u-amplitude-cap)\n\n\n\n\n\n\nAs far as the correlation goes, the correlation between the mean zonal wind and the sine amplitude is -0.36 (CI: -0.60 – -0.06), which is consistent with Hardiman et al. (2020).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHardiman, S. C., A. A. Scaife, N. J. Dunstone, and L. Wang. 2020. “Subseasonal Vacillations in the Winter Stratosphere.” Geophysical Research Letters 47 (9): e2020GL087766. https://doi.org/https://doi.org/10.1029/2020GL087766.\n\n\nMacLachlan, C., A. Arribas, K. A. Peterson, A. Maidens, D. Fereday, A. A. Scaife, M. Gordon, et al. 2015. “Global Seasonal Forecast System Version 5 (GloSea5): A High-Resolution Seasonal Forecast System.” Quarterly Journal of the Royal Meteorological Society 141 (689): 1072–84. https://doi.org/https://doi.org/10.1002/qj.2396.\n\n\n\n\n",
    "preview": "posts/2022-09-20-uwind-sinewave/uwind-sinewave_files/figure-html5/timeseries-1.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-05-31-wave3-goyal/",
    "title": "Comment on \"A new zonal wave 3 index for the Southern Hemisphere\"",
    "description": "How to measure a wave that is not just a wave.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2022-06-02",
    "categories": [
      "wave 3",
      "general circulation"
    ],
    "contents": "\n\n\n\nMy PhD protect is (ostensibly) about the zonal wave 3 (ZW3) in the Southern Hemisphere.\nA zonal wave 3 is a short way of saying that a wave with a wavelenth 1/3rd of the circumference of the Earth at that latitude; which in essense means that on each latitude circle there are 3 maximums and 3 minimums.\nSo how do you measure the activity of this kind of wave in the atmosphere?\nTraditionally, one would compute the Fourier spectrum of geopotential height (or meridional wind) at some latitude or latitude band and look at the amplitude and phase of the 3rd wavenumber.\nAlternatively, Raphael (2004) took the mean value of geopotential anomalies in the climatological location of the three maximums.\nBoth approaches have the problem that assume that the wave 3 is a perfectly homogeneous wave with constant amplitude and phase in all longitudes.\nRaphael (2004)’s index is even more restrictive.\nBecause it’s based on fixed locations, it actually measures the amplitude of the wave 3 that is projected into the direction of the climatological wave (this might be an useful concept, though).\nHowever, wave 3 activity is not such a clean wave at all.\nMany studies and simulations show that the atmospheric circulation that is wave 3-like is much more intense over the South Pacific than over the southern Indian Ocean and that the waves don’t stick to a constant latitude.\nA recent paper by lead author Rishav Goyal proposes a different approach (which is very similar to my own, which will hopefully be available in a scientific journal near you soon).\nInstead of imposing the shape of the wave 3, Goyal et al. (2022) derive the suitable wave 3 pattern from the data using Empirical Orthogonal Functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: First two EOFs of meridional wind between 70°S and 40°S (shaded, positive values in red, negative in blue). The pure wave-3 field is shown in thin black contours. In parenthesis, the variance explained by each EOF. Compare with the top panel in Figure 2 of Goyal et al. (2022).\n\n\n\nAs you can see in Figure 1, the first two EOFs of meridional wind anomalies are basically a wave 3 pattern but with all the important deviations from a neat homogeneous wave that we want.\nThe amplitude is greater in the Pacific Ocean than in the Indian Ocean, the phase changes with latitude and the centres are not exactly where they would be otherwise.\nThis is essentially right, and something that is consistent with previous studies.\nThis is not the first time EOF was applied to the wave 3 pattern.\nYuan and Li (2008) did the same, but importantly, they used only one EOF to characterise the zonal wave 3 when you need two.\nThis kind of thing happens all the time in atmospheric sciences.\nSince EOFs can only capture standing oscillations, any pattern that moves needs two or more EOFs to be described.\nGoyal et al. (2022) does use two of them and furthermore argues that since they are clearly the two orthogonal phases of a wave 3-like pattern, one can combine the time series of each EOF into a single wave-like index\n\\[\nZW3 = \\sqrt{EOF1^2 + EOF2^2} \\cos\\left(\\tan^{-1} \\left ( \\frac{EOF2}{EOF1} \\right ) \\right )\n\\]\nSo, basically, you get the magnitude of the overall activity of the wave 3 as \\(\\sqrt{EOF1^2 + EOF2^2}\\) and the phase –the location– with \\(\\tan^{-1} \\left ( \\frac{EOF2}{EOF1} \\right )\\).\nI think this very neat and the way to go.\nIt combines the convenient mathematical formulation of a wave with the flexibility of describing something that is not a pure sine wave. But I think there’s a small detail that can be improved upon.\nIn theory, one can combine any two random time series that way, but the idea is that this works only if the two series actually represent the two orthogonal phases of a wave-like thing.\nSo the validity of this new index relies heavily on the patterns seen in Figure 1 being orthogonal.\nThey certainly look to be, but in detail, I think that they are not exactly.\nIf one plots the magnitude \\(\\sqrt{EOF1^1 + EOF^2}\\) (Figure 2 panel a) it looks like the amplitude changes spatially on a scale similar to the wavelength of the wave, particularly in the Indian sector.\nI think this is not ideal and shows that the patterns obtained with EOF are only approximately suitable for the task.\nThe panel b in Figure 2 shows how I think it should look. The amplitude is modulated on a spatial scale larger than the wave itself, creating a smoother field.\n\n\n\n\nFigure 2: Panel a: Spatial magnitude of the combined EOF magnitude (\\(\\sqrt{EOF1^2 + EOF2^2}\\)). Panel b: Spatial magnitude of the complex EOF.\n\n\n\nAs you might’ve guessed by the title Figure 2b, I think a slightly better approach might be to use complex EOF (cEOF).\nComplex EOFs were first described for the climate sciences by Horel (1984) as far as I can tell and are a very good fit for this problem.\nInstead of relying on chance that the two leading EOFs are orthogonal, one can create EOFs that are orthogonal by construction.\nTo do this, one has to move into the complex plane (which is always delight) by computing the analytic function of the variable, which is a complex number in which the real part is the same variable and the imaginary part is the Hilbert transform of the variable.\nTo give an idea of what it looks like, Figure 3 shows a (semi)randomly-selected meridional wind field in shading with it’s Hilbert transform in black contours.\n\n\n\n\n\n\n\nFigure 3: Monthly mean meridional wind (shaded, positive values in red, negative in blue) for July 1995. In black contour lines, its Hilbert transform (positive values in solid line, negatives in dashed).\n\n\n\nYou can see that the real field has a wave-like structure and that the imaginary field has the same structure but rotated 90º (the maximum amplitude of the imaginary field is exactly in the zeroes of the real field).\nAnd now for the magic reveal, Figure 4 shows the real and imaginary parts of the cEOF of meridional wind.\n\n\n\n\nFigure 4: Real and imaginary components of the complex EOF of meridional wind between 70°S and 40°S (shaded, positive values in red, negatives in blue).\n\n\n\nThey look very similar to Goyal et al. (2022)’s first two EOFs but they are truly orthogonal so there’s no ambiguity of whether it makes sense to calculate the amplitude and phase and the magnitude vaires smoothly in space (as seen before in Figure Figure 2 b)\n\n\n\nNow, to be fair, this is a small detail and the difference is not huge.\nFor instance, the correlation between the amplitude of Goyal et al. (2022)’s index and the equivalent complex index is a whooping 0.98. I think that using one or the other for research will not (should not, at any case) make a lot of difference.\n\n\n\nGoyal, Rishav, Martin Jucker, Alex Sen Gupta, and Matthew H. England. 2022. “A New Zonal Wave 3 Index for the Southern Hemisphere.” Journal of Climate -1 (May): 1–25. https://journals.ametsoc.org/view/journals/clim/aop/JCLI-D-21-0927.1/JCLI-D-21-0927.1.xml.\n\n\nHorel, J. D. 1984. “Complex Principal Component Analysis: Theory and Examples.” Journal of Applied Meteorology and Climatology 23 (12): 1660–73. https://doi.org/10.1175/1520-0450(1984)023<1660:CPCATA>2.0.CO;2.\n\n\nRaphael, M. N. 2004. “A Zonal Wave 3 Index for the Southern Hemisphere.” Geophysical Research Letters 31 (23). https://doi.org/10.1029/2004GL020365.\n\n\nYuan, Xiaojun, and Cuihua Li. 2008. “Climate Modes in Southern High Latitudes and Their Impacts on Antarctic Sea Ice.” Journal of Geophysical Research 113 (C6): C06S91. https://doi.org/10.1029/2006JC004067.\n\n\n\n\n",
    "preview": "posts/2022-05-31-wave3-goyal/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-09-asymsam/",
    "title": "Sources and impacts of the zonally asymmetric component of the Southern Annular Mode",
    "description": "Explanation of our recent paper on asymmetric and symmetric parts of the Southern Annular Mode.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-08-10",
    "categories": [
      "SAM",
      "ENSO",
      "general circulation"
    ],
    "contents": "\nToday I got my first paper published! 🎉\nAlong with my PhD advisors, Leandro Díaz and Carolina Vera, we wrote “Assessment of zonally symmetric and asymmetric components of the Southern Annular Mode using a novel approach” (because it wouldn’t be an academic publication if the title was shorter), and this post is a general explanation of the main methods and takeaways of the paper.\nWhat’s the deal with the SAM\nClimatologists love to think of the climate as a series of different modes. That is, take all the incomprehensible complexity of the evolving climate and distil it to a handful of phenomena that we can understand. For example, instead of having to think about all the variability of the Equatorial Pacific Ocean, which is a 3D field with multiple variables, you can think about the El Niño–Southern Oscillation (ENSO) phenomenon, which can be characterised by a single time series. Other members of the “Climate Oscillations Hall of Fame,” and their respective acronyms, are the Indian Ocean Dipole (IOD), the Northern Annular Mode (NAM), the North Atlantic Oscillation (NAO) and, important for this paper, the Southern Annular Mode (SAM).\nThe SAM describes an oscillating pattern of alternating low and high pressure anomalies over Antarctica and in the middle latitudes. To have a clear picture, the typical field of pressure anomalies when the SAM is on it’s positive phase looks like Figure 1.\n\n\n\n\nFigure 1: Typical pressure anomalies of the positive phase of the SAM. Lower pressure than usual over the Antarctic and higher pressure than usual in the mid-latitudes.\n\n\n\nAs you can see, this pattern lives up to its name in that the positive pressure anomalies form a ring (or, for the hoity-toity scientists, an annulus) around the negative pressure anomalies. In fact, almost every paper on the topic starts the introduction with a sentence along the lines of “The SAM is approximately zonally symmetric…” (Fogt and Marshall 2020) –where “zonally symmetric” means that it doesn’t depend on longitude. The word “approximately” is doing a lot of heavy lifting: the zonally symmetric ring is clearly deformed by zonally asymmetric anomalies.\nMost papers basically ignore these deviations from zonal symmetry and think of the SAM as zonally symmetric. This is fine as a first order approximation, but many aspects of the SAM are actually tied to its asymmetric nature. For example, the SAM is associated with anomalies in meridional (north–south) wind, which is not possible for a zonally symmetric pattern. And these meridional wind anomalies clearly are related to impacts in precipitation in over South America (Silvestri and Vera 2009) and temperature over the Antarctic Peninsula (Fogt, Jones, and Renwick 2012). These anomalies are also conspicuously similar to the effect of ENSO on these higher latitudes (e.g. Clem and Fogt 2013).\nThe understanding of the SAM as zonally symmetric also influences the theories behind the positive trend of the SAM index seen during the austral summer. Simulations show that increased concentrations of greenhouse gases and changes in stratospheric ozone combine to produce a zonally symmetric change of lower pressures over the poles and higher pressures at lower latitudes (Figure 2 from Arblaster and Meehl (2006)). Since these changes are similar to the positive phase of the SAM, then it’s only logical to identify this simulated changes with the observed SAM trend. But, of course, this only works for a zonally symmetric SAM.\n\n\n\n\nFigure 2: Trends in sea level pressure for the 1958–1999 period simulated with climate models that include various forcings. From Arblaster and Meehl (2006).\n\n\n\nIn essence, all evidence point to the fact that the asymmetric part of the SAM can have different sources of variability, different impacts, and different trends. So what we wanted to do is to try to separate the SAM into two indices, and index for the symmetric part of the SAM, and another for the asymmetric part of the SAM. The hope being that by studying these indices we could understand better the relationships between other parts of the climate to each part of the SAM.\nHow to divide the SAM\nWhat we did was, for each vertical level, to take the classical SAM pattern that we all know and love (Fig. 3a) and create two derived fields. The zonal mean field, which represents the zonally symmetric SAM (Fig. 3c) and then the difference between the full SAM field and the zonal mean, which represents the zonally asymmetric SAM (Fig. 3b). This figure really shows that the magnitude of the zonal anomalies is on par with the magnitude of the zonal mean!\n\n\n\n\nFigure 3: Spatial patterns of the first EOF of 700\\ hPa geopotential height for 1979 – 2018 period. (a) Full field, (b) zonally asymmetric component and (c) zonally symmetric component. Arbitrary units; positive values in blue and negative values in red.\n\n\n\nWe then projected monthly field into these three fields (again, for each vertical level) to obtain three time series, one for the classic “full” SAM, one for the asymmetric SAM (A-SAM) and one for the symmetric SAM (S-SAM).\nAs a sanity check, Figure 4 shows the regression of 700 hPa geopotential height and the three indices (the ones between A-SAM and S-SAM are actually the coefficients of multiple regression, check the paper for the details). And it works! The regression pattern of the full SAM index is the now-familiar SAM pattern. The pattern associated with the S-SAM is much more zonally symmetric and the one associated with the A-SAM is zonally asymmetric.\n\n\n\n\nFigure 4: Regression of 700 hPa geopotential height (meters) with (column a) SAM, (column b) A-SAM, and (column c) S-SAM for the 1979 – 2018 period.\n\n\n\nThe two faces of the SAM\nWith these indices at hand, we can now study the behaviours that are unique of the asymmetric or the symmetric SAM. For example, before I talked about the positive trend in the SAM index. Figure 5 show linear trends (in standard deviations per decade) for each index at each vertical level of the atmosphere.\n\n\n\n\nFigure 5: Linear trends (in standard deviations per decade) at each level for annual (row 1) and seasonal values (rows 2 to 5) for the period 1979 – 2018 and for the (column a) SAM index, (column b) A-SAM index, and (column c) S-SAM index. Shading indicates the 95% confidence interval from a t-distribution.\n\n\n\nNot surprisingly we detect the already-known positive trend of the SAM index, which is only present in the summer (and a but in autumn) and only in the troposphere (Fig. 5 column a). But what we also show is that only the symmetric part of the SAM has experienced a increase towards positive values (Fig. 5 column c), while the asymmetric part of the SAM appears to have suffered no long term (linear) trends ((Fig. 5 column b). Another added value of our paper is the computation of trends along many vertical levels. Most studies look at trends for a single SAM index defined using either 700 hPa geopotential height or sea level pressure. Here we show that the positive trend actually reaches its maximum near 100 hPa, which is much higher in the atmosphere.\n\n\n\n\nFigure 6: Linear trends (in percent per decade) of the variance explained by A-SAM and S-SAM at each level and for each trimester for the period 1979 – 2018.\n\n\n\nAnother interesting question is whether the SAM has become more or less asymmetric. Fogt, Jones, and Renwick (2012) suggested that the SAM is becoming more symmetric in summer and autumn between 1960 and 2000. In our study we use data between 1979 and 2018 (data before 1979 is highly suspect due to the lack of satellite observations) so it’s not terribly comparable, but we reach the opposite conclusion. Figure 6 shows linear trends of the explained variance of each index at each vertical level. In summer, the variance explained by the A-SAM in lower levels has increased by about 2% per decade. Honestly, this is not strong evidence by itself, so more research is needed™.\nAll that (and more!) indicates that the asymmetric part of the SAM behaves differently from the symmetric part. But we also show that they have different surface impacts.\n\n\n\n\nFigure 7: Regression of summer mean 2-metre temperature anomalies (Kelvin) from ERA5 with SAM, A-SAM and S-SAM for the 1979 – 2018 period. Black contours indicate areas with p-value smaller than 0.05 controlling for False Detection Rate. Note that the colour scale cuts-off at \\(\\pm0.6 \\mathrm{K}\\) to highlight mid-latitudes and tropics features at the expense of the higher values in polar regions.\n\n\n\nPanel a.1 in Figure 7 shows the regression between summer mean temperature and the classic SAM index. Compare it with panels b.1 and c.1, which show the same but for the A-SAM and S-SAM index. In most areas, you can see a relatively clean separation of the SAM relation ship into the asymmetric and symmetric component. For instance, in the Antarctic region positive values of the SAM are associated with negative temperature anomalies in an incomplete ring surrounding the continent. This ring is mostly explained by the S-SAM, while the A-SAM relationship with temperature is not so strong in these high latitudes.\nYou can look at the maps and find other differences (maps for other seasons are available on the paper), but I here I want to highlight what happens in the tropical oceans. Both in the Pacific and Indian ocean, there is no significant relationship between the SAM and air temperature, but there is a relationship between the A-SAM and temperatures there. That’s interesting to me, because it suggests that in some regions, there is an effect of the A-SAM that is masked by the S-SAM and thus is not detectable by using the full SAM index, which mixes the two.\nIf “temperature of the tropical Pacific” made you think of ENSO, you are not alone. It is known that ENSO is somewhat correlated with the SAM, but did you know know that there is no correlation between the zonally symmetric part of the SAM and ENSO in any season? If you look at Table 1, now you know! In fact, Table 1 also shows that the correlation with ENSO is higher for the A-SAM index than for the SAM index in all seasons except DJF.\n\n\nTable 1: Correlation between SAM indices and the Oceanic Niño Index. p-values corrected for False Detection Rate in parenthesis. In bold, correlations with p-value smaller than 0.05.\n\n\n\n\n\nCorrelation\n\n\n\n\nPartial correlation\n\n\n\n\n\nSAM\n\n\nA-SAM\n\n\nS-SAM\n\n\nYear\n\n\n-0.17 (0.001)\n\n\n-0.26 (<0.001)\n\n\n0.02 (0.775)\n\n\nDJF\n\n\n-0.31 (0.002)\n\n\n-0.30 (0.003)\n\n\n-0.17 (0.115)\n\n\nMAM\n\n\n-0.07 (0.530)\n\n\n-0.26 (0.011)\n\n\n0.14 (0.192)\n\n\nJJA\n\n\n0.01 (0.900)\n\n\n-0.14 (0.192)\n\n\n0.11 (0.300)\n\n\nSON\n\n\n-0.25 (0.014)\n\n\n-0.42 (<0.001)\n\n\n0.05 (0.686)\n\n\nThis is exciting because it says that if you want to study the relationship between the SAM and ENSO, you should probably look only at the S-SAM.\nMore SAM\nIf this piqued your interest, go read the rest of the paper (click here for a free, but DRMd copy). It covers other issues, such as the cross-correlation among levels, vertical regressions, and the relationship with precipitation. But overall, the point is that not only it’s possible to split the Southern Annular Mode into a zonally symmetric and a zonally asymmetric part, but it seems to be both physically meaningful and statistically useful.\nIf you want to use this on your research, you can get all the code that produced the paper on this repository and get the data for the three SAM indices here.\n\n\n\nArblaster, Julie M., and Gerald A. Meehl. 2006. “Contributions of External Forcings to Southern Annular Mode Trends.” Journal of Climate 19 (12): 2896–2905. https://doi.org/10.1175/JCLI3774.1.\n\n\nClem, Kyle R., and Ryan L. Fogt. 2013. “Varying Roles of ENSO and SAM on the Antarctic Peninsula Climate in Austral Spring.” Journal of Geophysical Research: Atmospheres 118 (20): 11, 481–11, 492. https://doi.org/10.1002/jgrd.50860.\n\n\nFogt, Ryan L., Julie M. Jones, and James Renwick. 2012. “Seasonal Zonal Asymmetries in the Southern Annular Mode and Their Impact on Regional Temperature Anomalies.” Journal of Climate 25 (18): 6253–70. https://doi.org/10.1175/JCLI-D-11-00474.1.\n\n\nFogt, Ryan L., and Gareth J. Marshall. 2020. “The Southern Annular Mode: Variability, Trends, and Climate Impacts Across the Southern Hemisphere.” WIREs Climate Change 11 (4): e652. https://doi.org/10.1002/wcc.652.\n\n\nSilvestri, Gabriel, and Carolina Vera. 2009. “Nonstationary Impacts of the Southern Annular Mode on Southern Hemisphere Climate.” Journal of Climate 22 (22): 6142–48. https://doi.org/10.1175/2009JCLI3036.1.\n\n\n\n\n",
    "preview": "posts/2021-08-09-asymsam/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 471,
    "preview_height": 471
  },
  {
    "path": "posts/2021-01-15-sam-symmetry/",
    "title": "Does the Southern Annular Mode exist?",
    "description": "Scavenging the literature on the Southern Annular Mode I came across two papers from the early 2000's which seem to question whether this mode is actually a global phenomenon and not a statistical artifact.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-01-15",
    "categories": [
      "SAM",
      "statistics",
      "general circulation"
    ],
    "contents": "\nIntroduction\nIn “The Structure and Composition of the Annular Modes in an Aquaplanet General Circulation Model” (Cash, Kushner, and Vallis 2002) and “Zonal Asymmetries, Teleconnections, and Annular Patterns in a GCM” (Cash, Kushner, and Vallis 2005) (CKV from now on), the authors analyse simulations on an aquaplanet model (in the former) and with some zonal asymmetries (in the latter). In particular, they study the configuration of their “annular modes” or, rather, their leading EOF. Their conclusion is that annular modes as seen from the leading EOF are a statistical artifact and that, in fact, they actually describe localised events.\nIf this is correct and is also valid for the real atmosphere (as opposed to their simple models), then it throws a monkey wrench to any attempts to understand the Souther Annular Mode –it doesn’t even exist as a physically meaningful entity.\nHere, I use reanalysis data to show that, while CKV raises important points, the concept of the global SAM is safe.\nCKV analysis\nCKV ran an aquaplanet model and they point out that the first EOF of sea level pressure describes an almost perfect annular mode.\n\n\n\n\nFigure 1: Figure 4 from Cash, Kushner, and Vallis (2002): Leading EOF of winter season surface pressure. (a) Normalized, nondimensional EOF of the zonal-mean surface pressure. (b) Regression map of zonal-mean surface pressure against the principal component of the leading EOF. Amplitude is the response to 1 std dev in the principal component. (Units: mb.) (c) As in (b), except for the zonally varying pressure. Solid lines are positive, dashed lines are negative, and a heavy contour denotes the zero line. Contour interval is 2 mb\n\n\n\nThe problem is that when the look at particular events of high / low values of EOF1, they are nowhere near “annular.”\n\n\n\n\nFigure 2: Excerpt from Figure 7 from Cash, Kushner, and Vallis (2002): High- and low-index annular-mode events. (a), (b), (c) High-index events. (d), (e), (f ) Low-index events. Events displayed are 3-day averages about the day that the projection coefficient attains its maximum value during the event. Solid contours are positive, dashed contours are negative, and a heavy contour denotes the zero line. Contour interval is 5 mb\n\n\n\nInspired by this disconnect between the structure of the EOF and the individual cases, they try a different approach. They go back to using teleconnection maps (correlation between SLP at one point and the rest of the globe) based on various latitudes. They conclude that negative correlations are maximised for points at 65º and 35º (Figure 2)\n\n\n\n\nFigure 3: SLP de casos seleccionados con eventos positivos (izquierda) y negativos (derecha)\n\n\n\nThese maps look like “zonally localised versions” of the global annular mode. From this (and other results), CKV conclude that\n\nThe low-frequency variability of the model is thus characterized by meridional dipoles in the sea level pressure, with centers near 35º and 65º latitude, and a zonal scale of 60º to 90º. Because these events are distributed uniformly in longitude in the aquaplanet model, they are represented in an EOF analysis as a single, zonally uniform pattern.\n\nStrong stuff.\nFurther analysis\nCKV’s proposal is intriguing, does the real atmosphere behaves thusly? The problem they put forward is very similar to the problem I had with Senapati, Dash, and Behera (2021) here so I’ll use similar methods.\n\n\nShow code\n\nlibrary(data.table)\nlibrary(magrittr)\nlibrary(metR)\nlibrary(ggplot2)\nlibrary(ggperiodic)\nlibrary(patchwork)\n\ntheme_set(theme_minimal() + \n            theme(panel.grid = element_blank()))\n\n# simple and dirty map\nmap <- ggplot2::map_data(\"world2\") %>%\n  subset(lat %between% c(-90, -10))\n\nquick_map <- list(geom_polygon(data = map,\n                               aes(long, lat, group = group), fill = NA, color = \"black\",\n                               size = 0.2),\n                  scale_x_longitude(),\n                  scale_y_latitude())\n\n\n\n\n\nShow code\n\n# This WILL take long. Go make yourself a cup of tea or brew some mate.\nera5_file <- here::here(\"_data\", \"era5-hgt.nc\")\n\nif (!file.exists(era5_file)) {\n  request <- list(\n    format = \"netcdf\",\n    product_type = \"monthly_averaged_reanalysis\",\n    variable = \"geopotential\",\n    pressure_level = \"700\",\n    year = c(\"1979\", \"1980\", \"1981\", \"1982\", \"1983\", \"1984\", \"1985\", \"1986\", \"1987\", \"1988\", \"1989\", \"1990\", \"1991\", \"1992\", \"1993\", \"1994\", \"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\"),\n    month = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"),\n    time = \"00:00\",\n    area = c(-20, -180, -90, 180),\n    grid = c(\"2.5\", \"2.5\"),   # we don't need high resolution\n    dataset_short_name = \"reanalysis-era5-pressure-levels-monthly-means\",\n    target = basename(era5_file)\n  )\n  \n  # Need to set up user with\n  # ecmwfr::wf_set_key() \n  ecmwfr::wf_request(request, path = dirname(era5_file))\n}\n\n\n\n\n\nShow code\n\nhgt <- ReadNetCDF(era5_file, vars = c(hgt = \"z\")) %>% \n  setnames(c(\"latitude\", \"longitude\"),\n           c(\"lat\", \"lon\")) %>% \n  .[, lon := ConvertLongitude(lon)] %>%   # put longitude between 0 and 360\n  .[, hgt := hgt/9.8] %>% \n  .[, hgt_a := hgt - mean(hgt), by = .(lon, lat, month(time))] %>% \n  .[, hgt_m := mean(hgt_a), by = .(lat, time)] %>% \n  .[, hgt_z := hgt_a - hgt_m]\n\n\n\nLet’s first compute the Southern Annular Mode (SAM) as the leading EOF of the 700 hPa geopotential height south of 20ºS.\n\n\nShow code\n\nsam <- hgt %>% \n  copy() %>% \n  .[, hgt := hgt_a*sqrt(cos(lat*pi/180))] %>% \n  EOF(hgt ~ time | lon+ lat, n = 1, data = .)\n\n\n\n\n\nShow code\n\nsam$right %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = hgt, fill = ..level..)) +\n  quick_map +\n  coord_polar() +\n  scale_fill_divergent_discretised(guide = \"none\")\n\n\n\n\nFigure 4: Spatial pattern of the leading EOF of 700 hPa monthly geopotential height anomalies (AKA Southern Annular Mode, AKA SAM).\n\n\n\nAnd now let’s compute localised SAM indices. At each longitude I will take a section of the data located between 45º West and 45º East of it and project the geopotential height field onto the corresponding SAM pattern (in Figure 4). The result is one “local SAM index” for each longitude.\n\n\nShow code\n\nlon_width <- 90\nlon_halfwidth <- lon_width/2\n\n# \"extended\" version of geopotential height \nhgt2 <- sam$right %>% \n  copy() %>% \n  setnames(\"hgt\", \"EOF\") %>% \n  hgt[., on = .NATURAL] %>% \n  qwrap(lon = c(0, 360) ~ c(-lon_halfwidth, 360 + lon_halfwidth))\n\n# For each longitude, compute EOF using a segment of lon_width width\n# centred in that longitude.\nlon_eofs <- lapply(unique(hgt$lon), function(base_lon) {\n  hgt2 %>% \n    .[lon %between% (base_lon + c(-lon_halfwidth, lon_halfwidth))] %>% \n    .[, .(eof = weighted.mean(hgt_a*EOF, cos(lat*pi/180))),\n      by = time] %>% \n    .[, base_lon := base_lon] %>% \n    .[]\n}) %>% \n  rbindlist()\n\n\n\nFollowing a similar argument from my analysis of the proposed wave-4 pattern, if the SAM pattern in Figure 4 was a statistical artifact formed by the combination of independent localised events, then the local SAM indices shouldn’t be correlated between each other at all. So let’s compute the pairwise correlation at each longitude.\n\n\nShow code\n\nlon_eofs %>% \n  widyr::pairwise_cor(base_lon, time, eof) %>% \n  ggplot(aes(item1, item2)) +\n  geom_contour_fill(aes(z = correlation, fill = ..level..), na.fill = 1) +\n  geom_contour2(aes(z = correlation), size = 0.2) +\n  geom_text_contour(aes(z = correlation, stroke.color = ..level..), color = \"black\",\n                    stroke = 0.2) +\n  scale_fill_divergent(\"Correlation\", \n                       super = ScaleDiscretised) +\n  scale_color_divergent(aesthetics = \"stroke.color\", guide = \"none\") +\n  scale_x_longitude() +\n  scale_y_longitude() +\n  coord_equal() \n\n\n\n\nFigure 5: Pairwise correlation between localised SAM indices.\n\n\n\nAnd form Figure 5 I feel that it’s clear that the local SAM indices are fairly well correlated. There is, though, a correlation minimum between ~120W and 60E. 120W coincides with the Amundsen Sea Low and where most of the wave activity related to El Niño Southern Oscillation is located, so is not surprising that this region appears notable.\nAnother test would be to compute the correlation map of each localised SAM index with the whole geopotential height field. Again, if the SAM was not global, then I would not expect to find a (relatively) complete SAM pattern associated with the localised indices.\n\n\nShow code\n\ncor_pattern <- lon_eofs %>% \n  .[base_lon %in% rev(seq(0, 360, length.out = 7))[-1]] %>% \n  .[hgt, on = \"time\", allow.cartesian = TRUE] %>% \n  .[, cor(hgt_a, eof), by = .(lon, lat, base_lon)]\n\n\n\n\n\nShow code\n\nbase_lons <- unique(cor_pattern$base_lon)\nwrap_ <- function(lon) {\n   lon <- ifelse(lon <= 0, lon + 360, lon)\n   lon <- ifelse(lon >= 360, lon - 360, lon)\n   lon\n}\n\nlims <- data.table(base_lon = unique(lon_eofs$base_lon)) %>% \n   .[, side1 := wrap_(base_lon + 45)] %>% \n   .[, side2 := wrap_(base_lon - 45)] %>% \n   .[base_lon %~% base_lons] \n\n\ncor_pattern %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = V1, fill = ..level..), breaks = AnchorBreaks(exclude = 0)) +\n  geom_contour2(data = periodic(sam$right, lon = c(0, 360)),\n                aes(z = hgt), size = 0.2, breaks = AnchorBreaks(0, 0.01)) +\n  geom_vline(data = lims, aes(xintercept = side1)) +\n  geom_vline(data = lims, aes(xintercept = side2)) +\n  quick_map +\n  scale_fill_divergent(super = ScaleDiscretised) +\n  coord_polar() +\n  facet_wrap(.~base_lon, labeller = labeller(base_lon = LonLabel)) +\n  theme(axis.text = element_blank())\n\n\n\n\nFigure 6: Correlation maps between 700 hPa monthly geopotential height anomalies and localised SAM indices at differnet centre longidutes. Black contours indicate the original SAM pattern and black lines delineate the longitudes used to compute each localised SAM index.\n\n\n\nFigure 6 shows the correlation patterns for the local SAM indices with central longitudes of 0°, 60°E, 120°E, 180°, 120°W, and 60°W. The original SAM pattern is overlaid with contours for comparison. Although with some differences, I’d argue that in all cases the classic SAM pattern is clearly visible.\nNull hypothesis\nI now wonder… how would the null hypothesis of no global SAM looked under these same methods? I don’t have CKV data handy, but I can run simulations.\nFirst, replace the zonally varying SAM with it’s zonal mean component. Then, use that pattern to simulate a global, zonally symmetric, geopotential field. Finally, multiply that field with a localised Gaussian function with a random central longitude and 45º standard deviation. An example field is shown in Figure 7.\n\n\nShow code\n\nmean_eof <- copy(sam)\n\nmean_eof$right[, hgt := mean(hgt), by = lat]\n\nzonal_amplitude <- function(lon, central_lon) {\n  amplitude <- suppressWarnings(circular::dwrappednormal(lon*pi/180, mu = central_lon[1]*pi/180, sd = 45*pi/180))\n  amplitude[amplitude <= 0.05] <- 0\n  amplitude/max(amplitude)\n}\n\n# Grid\nsimulation <- CJ(lon = unique(hgt$lon), \n                 lat = unique(hgt$lat),\n                 time = unique(hgt$time)) %>% \n  predict(mean_eof)[., on = .NATURAL] %>% \n  setnames(\"hgt\", \"annular\") %>% \n  na.omit() %>% \n  .[, central_lon := runif(1, 0, 360), by = time] %>% \n  .[, zonal_amplitude := zonal_amplitude(lon, central_lon), by = .(time)] %>% \n  .[, hgt := zonal_amplitude*annular/sqrt(cos(lat*pi/180))]\n\n\n\n\n\nShow code\n\nsimulation[time == unique(time)[1]] %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = annular)) +\n  quick_map +\n  scale_fill_divergent(guide = \"none\") +\n  coord_polar() +\n  labs(subtitle = \"Zonally symmetric anomaly\") +\n  \n  simulation[time == unique(time)[1]] %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = zonal_amplitude)) +\n  quick_map +\n  scale_fill_divergent(guide = \"none\") +\n  coord_polar() +\n  labs(subtitle = \"Localisation function\") + \n  \n  \n  simulation[time == unique(time)[1]] %>% \n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = hgt)) +\n  quick_map +\n  scale_fill_divergent(guide = \"none\") +\n  coord_polar() +\n  labs(subtitle = \"Final geopotential field\") + \n\n\n  plot_layout(ncol = 3) & theme(axis.text = element_blank())\n\n\n\n\nFigure 7: Example of a single simulated geopotential height field. The zonally symmetric SAM pattern (left) is multiplied by the localisation function (middle), which reulsts in a localised pattern of positive and negative anomalies.\n\n\n\nCKV’s contention is that the leading EOF of an atmosphere consisting solely on localised patterns such as the last row of Figure 7 will appear as a single annular pattern. Let’s see first if that’s correct.\n\n\nShow code\n\nsim_eof <- simulation %>% \n  .[lat >= -85] %>% \n  .[, hgt := Anomaly(hgt)*sqrt(cos(lat*pi/180)), by = .(lon, lat, month(time))] %>% \n  .[, EOF(hgt ~ time | lon + lat, n = 1)]\n\n\n\n\n\nShow code\n\nsim_eof$right %>%\n  periodic(lon = c(0, 360)) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = hgt)) +\n  quick_map +\n  scale_fill_divergent(guide = \"none\") +\n  coord_polar()\n\n\n\n\nFigure 8: Leading EOF of simulated geopotential height fields.\n\n\n\nFigure 8 shows the leading EOF of the simulated data. And indeed, even thought by construction the data doesn’t have an annular mode, one appears as the leading EOF plain as day. Now let’s put them through the same process as the real data. Get the localised SAM indices and compute the pairwise correlation between them.\n\n\nShow code\n\n# \"extended\" version of geopotential height \nhgt2 <- sim_eof$right %>% \n  copy() %>% \n  setnames(\"hgt\", \"EOF\") %>% \n  simulation[., on = .NATURAL] %>% \n  qwrap(lon = c(0, 360) ~ c(-lon_halfwidth, 360 + lon_halfwidth))\n\n# For each longitude, compute EOF using a segment of lon_width width\n# centred in that longitude.\nlon_eofs_sim <- lapply(unique(hgt$lon), function(base_lon) {\n  hgt2 %>% \n    .[lon %between% (base_lon + c(-lon_halfwidth, lon_halfwidth))] %>% \n    .[, .(eof = weighted.mean(hgt*EOF, cos(lat*pi/180))),\n      by = time] %>% \n    .[, base_lon := base_lon] %>% \n    .[]\n}) %>% \n  rbindlist()\n\n\n\n\n\n\nShow code\n\nlon_eofs_sim %>% \n  widyr::pairwise_cor(base_lon, time, eof) %>% \n  ggplot(aes(item1, item2)) +\n  geom_contour_fill(aes(z = correlation, fill = ..level..), na.fill = 1) +\n  geom_contour2(aes(z = correlation), size = 0.2) +\n  geom_text_contour(aes(z = correlation, stroke.color = ..level..), color = \"black\",\n                    stroke = 0.2) +\n  scale_fill_divergent(\"Correlation\",\n                       super = ScaleDiscretised) +\n  scale_color_divergent(aesthetics = \"stroke.color\", guide = \"none\") +\n  scale_x_longitude() +\n  scale_y_longitude() +\n  coord_equal() \n\n\n\n\nFigure 9: Same as Figure 5 but for the simulated fields.\n\n\n\nComparing Figure 9 with Figure 5, the results of the simulation are very different! As expected, the correlation between indices decreases rather rapidly between adjacent longitudes, and it’s near zero for the antipodes.\nConslusion\nI think that CKV does bring up an important point. The fact that one can compute a global EOF doesn’t mean that the observed pattern is indeed global. EOF is a statistical method not constrained by physics and as the simulated data shows, you can very well obtain an annular mode with data that doesn’t vary annularly.\nHowever, in the real atmosphere, the Southern Annular Mode does appear as a globally coherent annular mode. The SAM is safe.\nDownload code\nClick on this button to get the code that generated this document:\n\n\n Download code\n\n\n\n\n\nCash, Benjamin A., Paul J. Kushner, and Geoffrey K. Vallis. 2002. “The Structure and Composition of the Annular Modes in an Aquaplanet General Circulation Model.” J. Atmos. Sci. 59 (23): 3399–3414. https://doi.org/10.1175/1520-0469(2002)059<3399:TSACOT>2.0.CO;2.\n\n\n———. 2005. “Zonal Asymmetries, Teleconnections, and Annular Patterns in a GCM.” J. Atmos. Sci. 62 (1): 207–19. https://doi.org/10.1175/JAS-3361.1.\n\n\nSenapati, Balaji, Mihir K. Dash, and Swadhin K. Behera. 2021. “Global Wave Number-4 Pattern in the Southern Subtropical Sea Surface Temperature.” Scientific Reports 11 (1): 142. https://doi.org/10.1038/s41598-020-80492-x.\n\n\n\n\n",
    "preview": "posts/2021-01-15-sam-symmetry/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-15-sao-and-jet/",
    "title": "The meridional structure of the Semiannual Oscillation",
    "description": "How to filter out extra-tropical variabilty to highlight the structure of the Semiannual Oscillation using just multiple linear regression.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-01-15",
    "categories": [
      "Semiannual Oscillation",
      "general circulation",
      "tropics",
      "stratosphere"
    ],
    "contents": "\nSome time ago, with a Journal Club I’m part of (shout-out to Go stratospheric!), we read “Representation of the equatorial stratopause semiannual oscillation in global atmospheric reanalyses” (Kawatani et al. 2020), a paper about the Semiannual Oscillation (SAO). One figure (Figure 1) piqued our interest. The SAO is supposed to be a characteristic of the tropical stratosphere and be… you know… semiannual. But in Figure 1, the more sticking feature is extra-tropical and with an annual cycle.\n\n\n\nShow code\n\nknitr::include_graphics(file.path(\"img\", \"sao-jet-fig8.png\"))\n\n\n\n\nFigure 1: Figure 8 from Kawatani et al. (2020): “Time–latitude sections of climatological mean annual cycle of the zonal mean zonal wind for (a) SABER, (b) MLS, and (c–i) each reanalysis at 1 hPa. The contour intervals are 10 m s−1. Climatology is calculated from 1980 to 2010 in the reanalyses, from 2002 to 2016 in SABER, and from 2005 to 2016 in MLS.”\n\n\n\nWe quickly realised that there’s more going on there than the SAO. Is there a way to filter out the rest of the variability and only look at the SAO-related variabiltiy?\nAs always, first thing first. Download the data and replicate the figure. Kawatani et al. (2020) use data from many many reanalyses, but I’m only going to use data from ERA5.\n\n\nShow code\n\nlibrary(magrittr)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(metR)\ntheme_set(theme_minimal(base_size = 10) + theme(panel.grid = element_blank()))\n\n\n\n\n\nShow code\n\n# This WILL take long. Go make yourself a cut of tea or brew some mate.\nera5_file <- here::here(\"_data\", \"era5-u.nc\")\n\nif (!file.exists(era5_file)) {\n  request <- list(\n    format = \"netcdf\",\n    product_type = \"monthly_averaged_reanalysis\",\n    variable = \"u_component_of_wind\",\n    pressure_level = c(\"1\", \"2\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\"),\n    year = c(\"1979\", \"1980\", \"1981\", \"1982\", \"1983\", \"1984\", \"1985\", \"1986\", \"1987\", \"1988\", \"1989\", \"1990\", \"1991\", \"1992\", \"1993\", \"1994\", \"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\"),\n    month = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"),\n    time = \"00:00\",\n    grid = c(\"2.5\", \"2.5\"),   # we don't need high resolution\n    dataset_short_name = \"reanalysis-era5-pressure-levels-monthly-means\",\n    target = basename(era5_file)\n  )\n  \n  # Need to set up user with\n  # ecmwfr::wf_set_key() \n  ecmwfr::wf_request(request, path = dirname(era5_file))\n}\n\n\n\n\n\nShow code\n\nera5 <- ReadNetCDF(era5_file)\n\n\n\n\n\nShow code\n\nera5 %>% \n  .[level == 1] %>% \n  .[, .(u = mean(u)), by = .(latitude, month(time), level)] %>% \n  ggplot(aes(month, latitude)) +\n  geom_contour_fill(aes(z = u, fill = ..level..), \n                    breaks = AnchorBreaks(0, binwidth = 15, exclude =  0)) +\n  geom_contour_tanaka(aes(z = u), \n                      breaks =  AnchorBreaks(0, binwidth = 15, exclude =  0)) +\n  scale_y_latitude() +\n  scale_x_continuous(breaks = 1:12, labels = month.abb, expand = c(0, 0)) +\n  scale_fill_divergent_discretised(\"U\") +\n  facet_wrap(~level, labeller = labeller(level = function(x) paste0(x, \" hPa\")))\n\n\n\n\nFigure 2: Seasonal cycle of zonal mean zonal wind at 1 hPa\n\n\n\nFigure 2 replicated Figure 1 extending the latitude range to the rest of the globe. It becomes even clearer that the main variability is in the extratropics.\nNow, the SAO not only is supposed to be in the tropics, but it’s also restricted to levels higher than ~3 hPa. Figure 3 shows what’s going on at levels bewteen 1 hPa and 30 hPa.\n\n\n\nShow code\n\nlast_plot() %+%\n  era5[, .(u = mean(u)), by = .(latitude, month(time), level)]\n\n\n\n\nFigure 3: Same as Figure 2 but for multiple levels.\n\n\n\nThe variability in the extratopics is there in the lower levels. So I though, why don’t try to remove that lower-level signal? After looking at Figure 3 and trying around, I settled into representing the lower levels with 5 hPa and 20 hPa.\nSo, at each gridpoint I fit the model\n\\[\nU_{1hPa} = \\alpha U_{5hPa} + \\beta U_{20hPa} + \\epsilon_{1hPa}\n\\]\nWhere \\(U\\) is the zonal mean zonal wind at each level and \\(\\epsilon_{1hPa}\\) is the residual wind that I’m interested in. That is, the variability of zonal mean zonal wind at 1 hPa that is not (linearly) explained by the variability of the zonal mean zonal wind at the lower levels. For those paying attention, this is (I think) essentially removing the equivalent barotropic component in the wind.\n\n\nShow code\n\nfiltered  <- era5 %>% \n  .[level %in% c(1, 5, 20)] %>% \n  # .[, .(u = mean(u)), by = .(time, level, latitude)] %>%\n  dcast(time + latitude + longitude ~ level, value.var = \"u\") %>% \n  .[, u_resid := resid(lm(`1` ~ `5` + `20`)), by = .(latitude, longitude)] \n\n\n\nThe seasonal cycle of the residual zonal mean zonal wind in Figure 4 shows that the filtering really works! Now it really looks like the equatorial phenomenon that it’s supposed to be, and the semiannual cycle is plainly for all to see. 🥳.\n\n\nShow code\n\nfiltered %>% \n  .[, .(u = mean(u_resid)), by = .(month(time), latitude)] %>% \n  ggplot(aes(month, latitude)) +\n  geom_contour_fill(aes(z = u, fill = ..level..), \n                    breaks = AnchorBreaks(0, binwidth = 10, exclude =  0)) +\n  geom_contour_tanaka(aes(z = u), \n                      breaks =  AnchorBreaks(0, binwidth = 10, exclude =  0)) +\n  scale_y_latitude() +\n  scale_x_continuous(breaks = 1:12, labels = month.abb, expand = c(0, 0)) +\n  scale_fill_divergent_discretised(\"U\")\n\n\n\n\nFigure 4: Seasonal cycle of the residual zonal mean zonal wind.\n\n\n\nAnd for completitude, Figure 5 shows timeseries of zonal mean zonal wind at the equator, and averaged polarward of 30°. In the equator, the residual zonal wind is virtually identical to the unfiltered zonal wind.\n\n\nShow code\n\nfiltered %>% \n  .[, zone := fcase(latitude > 30, \"north of 30°N\",\n                    latitude < -30, \"south of 30°S\",\n                    latitude == 0, \"equator\",\n                    default = NA)] %>% \n  na.omit() %>% \n  .[, .(u = mean(`1`),\n        u_resid = mean(u_resid)), by = .(time, zone)] %>% \n  melt(id.vars = c(\"time\", \"zone\")) %>% \n  ggplot(aes(time, value)) +\n  geom_line(aes(color = variable)) +\n  scale_color_brewer(NULL, palette = \"Dark2\", labels = c(u = \"U\", \n                                                         u_resid = \"Residual U\")) +\n  scale_y_continuous(NULL) +\n  facet_wrap(zone~., ncol = 1) \n\n\n\n\nFigure 5: Timeseries of zonal mean zonal wind and residual zonal wind in the equator, averaged north of 30°N and south of 30°S\n\n\n\nSo the conclusion, I think, is that the meridional structure was a function of the polar jets, and that looking at the SAO by looking at zonal mean zonal wind in the equator is fine.\nDownload code\nClick on this button to get the code that generated this document:\n\n\n Download code\n\n\n\n\n\nKawatani, Yoshio, Toshihiko Hirooka, Kevin Hamilton, Anne K. Smith, and Masatomo Fujiwara. 2020. “Representation of the Equatorial Stratopause Semiannual Oscillation in Global Atmospheric Reanalyses.” Atmospheric Chemistry and Physics 20 (14): 9115–33. https://doi.org/10.5194/acp-20-9115-2020.\n\n\n\n\n",
    "preview": "posts/2021-01-15-sao-and-jet/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-14-enso-and-regression-to-the-mean/",
    "title": "ENSO and regression to the mean",
    "description": "How a recent paper published in Nature was pray to regression to the mean.",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-01-14",
    "categories": [
      "ENSO",
      "statistics"
    ],
    "contents": "\nOn a recent article, Cai et al. (2020) used a perturbed ensemble of 40 members to show that members with relatively low El Niño Southern Oscillation (ENSO) variability at the beginning of the simulated period have a greater increase in ENSO variability towards the end of the simulated period than members with relatively high ENSO variability. They take this as evidence of ENSO self-regulation which would, if real, have important implications for future projections of ENSO variability under global warming. Here, I show that their results are not surprising, are likely product of simple regression to the mean and have no physical interpretation.\nIntroduction\nThe Community Earth System Model Large Ensemble model (Kay et al. 2015) consists of 40 climate simulations (members) that ran from 1920 to 2100 with identical external forcing and physics. Their only difference is a small, round-off error, perturbation in their atmospheric initial conditions. Cai et al. (2020) computed, for each member, the variability (standard deviation) of an index of Eastern-Pacific ENSO they call “E-index” for the first and last 50 years of the simulation. Owing to the effect of the various initial conditions, the variability in the E-index varied between members. I show their main result in Figure 1, which reproduces their Figure 1b. It shows a negative correlation between the initial E-index variability and its future change, defined as the difference between the final and the initial E-index variability.\n\n\nShow code\n\nlibrary(magrittr)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(patchwork)\ntheme_set(theme_minimal(base_size = 10) + theme(panel.grid = element_blank()))\n\n\n\n\n\nShow code\n\nobs_paper <- fread(here::here(\"public_data\", \"enso-data-fig1.csv\")) %>% \n  .[, final := initial + change]\nmoments <- obs_paper[, .(mean_initial = mean(initial), \n                         sd_initial = sd(initial),\n                         mean_final = mean(final),\n                         sd_final = sd(final))]\n\n\n\n\n\nShow code\n\nset.seed(42)\nmembers <- 40\nB <- 10000\nsims_normal <- CJ(sim = seq_len(B), member = seq_len(members)) %>% \n  .[, `:=`(initial = rnorm(.N, moments$mean_initial, moments$sd_initial),\n           final   = rnorm(.N, moments$mean_final, moments$sd_final))] %>% \n  .[, change := final - initial]\n\n\n\n\n\n\nShow code\n\ncor <- obs_paper[, cor.test(initial, change)]\n\ncor_text <- with(cor, paste0(\"R = \", signif(estimate, 2), \"\\nP ~ \", signif(p.value, 2)))\n\nobs_paper[, rank := cut_number(initial, 3)]\n\nobs_paper %>% \n  ggplot(aes(initial, change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#0e9a83\") +\n  annotate(\"text\", label = cor_text, x = .51, y = 0, hjust = 0, size = 4) +\n  scale_x_continuous(\"Initial E-index variability (s.d.)\") +\n  scale_y_continuous(\"Change in E-index variability (s.d.)\") \n\n\n\n\nFigure 1: Reproduction of Figure 1b from Cai et al. (2020) based on digitised values. The horizontal axis shows the initial variability of their ENSO index for the first 50 years of simulation (1920 – 1969) and the vertical axis shows the difference between the variability of their ENSO index for the last 50 years of the simulation (2050 – 2099) and the first 50 years.\n\n\n\nFrom this negative correlation, Cai et al. (2020) conclude that “strikingly, in experiments with initially higher ENSO variability (…), [its] amplitude in the future a century later [is] systematically smaller, and vice versa.” They attribute this to a novel mechanism of self-regulation involving non-linear thermal damping through upper-ocean heat exchange and stratification.\nSimulations\nTo test how striking this result really is, I created \\(10^{4}\\) simulations of 40 pairs of random numbers representing the initial and final variability of the E-index for 40 hypothetical ensemble members. The random numbers were taken from independent normal distribution with \\(\\mu=0.8\\) and \\(\\sigma = 0.14\\) for the initial variability and \\(\\mu=0.93\\), \\(\\sigma = 0.072\\) for the final variability. These are the mean and standard deviation from the observed values derived from Figure 1.\nAs each simulated observation is independent from the rest, this simulations serve as a plausible model for the null hypothesis that the initial variability of ENSO does not influence it’s future change.\n\n\nShow code\n\ntwo_plots <- function(sim, obs, n = 1:10) {\n  fits <- sim[, metR::FitLm(change, initial, se = TRUE), by = sim] %>% \n    .[term != \"(Intercept)\"] %>% \n    .[, p_value := pt(abs(estimate)/std.error, df, lower.tail = FALSE), by = sim]\n  \n  obs_fits <- obs[, metR::FitLm(change, initial, se = TRUE)] %>% \n    .[term != \"(Intercept)\"] %>% \n    .[, p_value := pt(abs(estimate)/std.error, df, lower.tail = FALSE)]\n  \n  p_value <- fits[, mean(r.squared >= obs_fits$r.squared)]\n  \n  fit_density <- as.data.table(density(fits[term != \"(Intercept)\"]$r.squared)[c(\"x\", \"y\")])\n  \n  x_pval <- mean(fits[term != \"(Intercept)\"][r.squared >= obs_fits$r.squared]$r.squared)\n  y_pval <- mean(fit_density$y)\n  \n  sim[sim %in% 1:10] %>% \n    .[, change := change] %>% \n    ggplot(aes(initial, change)) +\n    geom_point(size = 0.4, alpha = 0.3) + \n    geom_point(data = obs, size = 0.7) +\n    geom_smooth(data = obs, method = \"lm\", se = FALSE, size = 1.4,\n                fullrange = TRUE, color = \"#0e9a83\") +\n    geom_smooth(method = \"lm\", aes(group = sim), se = FALSE, fullrange = TRUE, \n                color = \"black\", size = 0.1, alpha = 0.5) +\n    scale_x_continuous(\"Initial E-index variability (s.d.)\") +\n    scale_y_continuous(\"Change in E-index variability (s.d.)\") +\n  # coord_equal()  +\n  \n  fit_density %>% \n    ggplot(aes(x, y)) +\n    geom_area(data = ~.x[x >= obs_fits$r.squared], fill = \"#4d4d4d\") +\n    annotate(\"text\", size = 6, color = \"white\",\n             label = scales::percent(p_value), x = x_pval, y = y_pval) +\n    geom_line() +\n    geom_vline(xintercept = obs_fits$r.squared, color = \"#0e9a83\", size = 1.4) +\n    scale_x_continuous(\"r²\") +\n    scale_y_continuous(NULL) +\n    \n    \n    plot_annotation(tag_levels = \"a\")\n}\n\ncompute_pvalue <- function(sim, obs) {\n  fits <- sim[, metR::FitLm(change, initial, se = TRUE), by = sim] %>% \n    .[term != \"(Intercept)\"] %>% \n    .[, p_value := pt(abs(estimate)/std.error, df, lower.tail = FALSE), by = sim]\n  \n  obs_fits <- obs[, metR::FitLm(change, initial, se = TRUE)] %>% \n    .[term != \"(Intercept)\"] %>% \n    .[, p_value := pt(abs(estimate)/std.error, df, lower.tail = FALSE)]\n  \n  fits[, mean(r.squared >= obs_fits$r.squared)]\n}\n\n\n\n\n\n\nShow code\n\ntwo_plots(sims_normal, obs_paper)\n\n\n\n\nFigure 2: a. Ten simulated ensembles of 40 members drawn from random numbers (see description in text) in small dot and their linear regression in fine lines. The observed values from Cai et al. (2020) in big dots and the linear regression in think, green. b. Estimated probability density of the coefficient of determination (\\(r^2\\)) under the null hypothesis model. The vertical bar shows the observed \\(r^2\\) and the shaded area is the proportion of simulated samples from the null hypothesis model with a \\(r^2\\) equal or greater than the observed one.\n\n\n\nFigure 2.a shows a random sample of 10 of these simulations in small dots and their respective linear fit in fine lines. Comparing these random simulations with the observed values taken from Cai et al. (2020) – shown in big dots and thick, green line – puts into perspective the strength of their evidence. The spread of the simulated data is very similar to the real data and all 10 simulation have a steeper regression slope. Both sets are so similar that it would be virtually impossible to distinguish between the simulated and real data if not for the different size of the points.\n\n\nShow code\n\np_value <- compute_pvalue(sims_normal, obs_paper)\n\n\n\nTo quantify how surprising the observed effect actually is, Figure 2.b shows the estimated probability density of the coefficient of determination (\\(r^2\\)) from the linear fit of the 10^{4} simulations and the observed value as a vertical line. 74% of simulations show an \\(r^2\\) equal or greater than the observed one. This translates to a p-value of 0.74; significantly higher than \\(1.4\\times 10^{-12}\\), naively computed from the linear regression.\nThen, it’s evidently clear that under this model of the null hypothesis the observed negative correlation is not only unsurprising, but completely expected.\nThe fact that the initial value of a variable is negatively correlated with its “future change” is one of the manifestation of the general principle of regression to the mean first observed by Galton in 1889 (Francis Galton 1889). He noted that very tall parents tended to have relatively shorter children while tall children tended to have relatively shorter parents.\nIn a perturbed ensemble experiment some ensemble members will have higher initial ENSO variability than average, while other will have lower ENSO variability than average. If this initial ENSO variability is completely independent of further ENSO variability (e.g. the null hypothesis is true), then both groups are equally likely to have average variability further down the simulation. Thus, members which a high initial ENSO variability will tend to show – on average – a negative change in variability, an vice versa.\nFormal formula\nIt’s straightforward to show this effect mathematically. The correlation between two variables \\(x\\) and \\(z = y - x\\) can be written as:\n\\[\\begin{equation}\n\\tag{1}\n\\mathrm{cor}(x, y - x) = \\frac{\\mathrm{cov(x, y) - \\mathrm{var}(x)}}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y) + \\mathrm{var}(x)^2 - 2\\mathrm{var}(x)\\mathrm{cov}(x, y)}}\n\\end{equation}\\]\nIf \\(x\\) and \\(y\\) are independent random variables, \\(\\mathrm{cov}(x, y) = 0\\) vanish and Equation (1) simplifies to\n\\[\\begin{equation}\n\\tag{2}\n\\mathrm{cor}(x, y - x)= \\frac{-1}{\\sqrt{\\mathrm{var}(y)/\\mathrm{var}(x) +1}}\n\\end{equation}\\]\nThis final formula shows that the correlation between \\(x\\) and \\(y - x\\) is bound to be negative and its magnitude depends only on the relationship between their variances. That is, if the variance of \\(y\\) is greater than the variance of \\(x\\), then the correlation will be small, and vice versa.\nApplying Equation (2) to the issue at hand, \\(x\\) becomes the initial ENSO variability and \\(y\\), the final ENSO variability and \\(\\mathrm{var}(x)\\), \\(\\mathrm{var}(y)\\) are the ensemble spread in initial and final ENSO variability, respectively. This means that, even in the case of no linear relation between initial and final variability, any process – either driven by physics or models – which reduces ensemble spread will lead to an even stronger negative linear relationship between the variables.\n\n\nShow code\n\nalpha_obs <- moments[, sd_final/sd_initial]^2\npred <- -1/sqrt(alpha_obs + 1)\n\n\n\nIn the case of equal variances, Equation (2) predicts a correlation of \\(-1/\\sqrt{2} \\sim -0.71\\). The observed correlation of -0.86 (Figure 1) is stronger This is explained by the reduced ensemble spread in ENSO variability between the initial and final periods, as the final spread is around 29% of the initial spread. For this value, Equation (2) predicts a correlation of -0.88, which is very close (if a bit stronger) to the observed value.\n\nConclusion\nThe previous analysis puts the strength of Cai et al. (2020) evidence into perspective. That the change in ENSO variability is greater in ensemble members with initially low ENSO variability is not at all surprising. By simulating the null hypothesis, I show that a negative correlations as strong or stronger than the one observed by Cai et al. (2020) can be expected 74% of the time.\nThese strong negative correlation can be explained by regression towards the mean and reduced ensemble spread in the final period compared to the initial period. I don’t think it is possible to identify the source of the spread reduction. It might be a real response to the forcing scenario or inability of the ensemble to capture all the sources of variability. In particular, all members share the same physics and boundary conditions, therefore I believe it would not be unexpected if all members slowly converged to a similar state. Kay et al. (2015)’s Figure 2 does appear to show that the ensemble spread is slightly higher before 1970. Of note, recently Bengtsson and Hodges (2019) observed secular reductions of ensemble spread in mean surface temperature in an ensemble whose members where also all forced with the same radiative conditions.\nFailing to account for regression to the mean is a fallacy that affects many fields of science – including behavioural science (Kelly and Price 2005) and medicine (Chuang-Stein and Tong 1997) – and features prominently in popular culture, such as the “Sports Illustrated cover jinx” – the perception that an athlete’s performance will be “cursed” after appearing in the cover of Sports Illustrated (Goldacre 2008). Cai et al. (2020) is not the first paper nor will be the last to fall for it.\nDownload code\nClick on this button to get the code that generated this document:\n\n\n Download code\n\n\n\n\n\nBengtsson, L., and K. I. Hodges. 2019. “Can an Ensemble Climate Simulation Be Used to Separate Climate Change Signals from Internal Unforced Variability?” Clim Dyn 52 (5): 3553–73. https://doi.org/10.1007/s00382-018-4343-8.\n\n\nCai, Wenju, Benjamin Ng, Tao Geng, Lixin Wu, Agus Santoso, and Michael J. McPhaden. 2020. “Butterfly Effect and a Self-Modulating El Niño Response to Global Warming.” Nature 585 (7823): 68–73. https://doi.org/10.1038/s41586-020-2641-x.\n\n\nChuang-Stein, Christy, and Donald M Tong. 1997. “The Impact and Implication of Regression to the Mean on the Design and Analysis of Medical Investigations.” Statistical Methods in Medical Research 6 (2): 115–28. https://doi.org/10.1177/096228029700600203.\n\n\nFrancis Galton. 1889. Natural Inheritance.\n\n\nGoldacre, Ben. 2008. Bad Science. HarperCollins UK.\n\n\nKay, J. E., C. Deser, A. Phillips, A. Mai, C. Hannay, G. Strand, J. M. Arblaster, et al. 2015. “The Community Earth System Model (CESM) Large Ensemble Project: A Community Resource for Studying Climate Change in the Presence of Internal Climate Variability.” Bull. Amer. Meteor. Soc. 96 (8): 1333–49. https://doi.org/10.1175/BAMS-D-13-00255.1.\n\n\nKelly, Colleen, and Trevor D. Price. 2005. “Correcting for Regression to the Mean in Behavior and Ecology.” Am Nat 166 (6): 700–707. https://doi.org/10.1086/497402.\n\n\n\n\n",
    "preview": "posts/2021-01-14-enso-and-regression-to-the-mean/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-14-wave4/",
    "title": "Analysis of \"Global wave number-4 pattern in the southern subtropical sea surface temperature.\"",
    "description": "A recent paper claims to have detected a global wave-4 pattern in subtropical SSTs. But have they really?",
    "author": [
      {
        "name": "Elio Campitelli",
        "url": "https://eliocamp.github.io/"
      }
    ],
    "date": "2021-01-14",
    "categories": [
      "zonal waves",
      "general circulation"
    ],
    "contents": "\nThis are some quick notes on Global wave number-4 pattern in the southern subtropical sea surface temperature (Senapati, Dash, and Behera 2021). The article claims to discover a wave-4 pattern in the southern Sea Surface Temperature (SST). Their basic method is to perform Empirical Orthogonal Function (EOF) to SST between 55°S and 20°S. They discard the first EOF as uninteresting because it’s “ENSO-like” (unsurprising) and focus on the second EOF, whose spatial pattern is in Figure 1.\n\n\n\n\nFigure 1: Panel a from Senapati, Dash, and Behera (2021)’s Figure 1: Spatial pattern of second leading EOF mode of SST anomaly over the region (20°S-55°S) from HadSST.\n\n\n\nFirst things first. Download the data and try to reproduce their figure.\n\n\nShow code\n\n# packages\nlibrary(magrittr)\nlibrary(data.table)\nlibrary(metR)\nlibrary(ggplot2)\n\n\n# fast detrend.\nDetrend <- function(y, x) {\n  nas <- is.na(y)\n  m <- mean(y, na.rm = TRUE)\n  if (!hasArg(x)) x <- seq_along(y)\n  y[!nas] <- .lm.fit(cbind(1, x[!nas]), y[!nas])$residuals\n  return(y + m)\n}\n\ntheme_set(theme_minimal() + \n            theme(panel.grid = element_blank()))\n# simple and dirty map\nmap <- ggplot2::map_data(\"world2\") %>%\n  subset(lat %between% c(-90, -0))\n\nquick_map <- list(geom_polygon(data = map,\n                               aes(long, lat, group = group), fill = \"white\", color = \"black\",\n                               size =0.2),\n                  scale_x_longitude(),\n                  scale_y_latitude(ticks = 10),\n                  coord_quickmap(ylim = c(-55, -20)))\n\n\n\n\n\nShow code\n\n# Download and decompress HadSST data\nhad_file <- here::here(\"_data\", \"hadsst.nc\")\nif (!file.exists(had_file)) {\n  hadsst <- \"https://www.metoffice.gov.uk/hadobs/hadisst/data/HadISST_sst.nc.gz\"\n  had_zip <- tempfile()\n  \n  download.file(hadsst, had_zip, mode = \"wb\")  \n  R.utils::gunzip(had_zip, had_file, remove = FALSE)\n}\n\n\n\n\n\nShow code\n\n# Read data and detrend\nsst <- ReadNetCDF(had_file, \n                   vars = c(\"sst\"),\n                   subset = list(latitude = c(-90, -0),\n                                 time = c(\"1979-01-01\", \"2018-12-31\"))) %>% \n  setnames(c(\"longitude\", \"latitude\"), c(\"lon\", \"lat\")) %>% \n  na.omit() %>% \n  .[, lon := ConvertLongitude(lon)] %>% \n  .[, sst := Detrend(Anomaly(sst)), by = .(lon, lat, month(time))]\n\n\n\n\n\nShow code\n\neofs <- sst[lat %between% c(-55, -20)] %>%\n  copy() %>% \n  .[, sst := sst*sqrt(cos(lat*pi/180))] %>% \n  EOF(sst ~ time | lat + lon, n = 1:2, data = .)\n\n\n\n\n\n\nShow code\n\nggplot(eofs$right, aes(lon, lat)) +\n  geom_contour_fill(aes(z = -sst)) +\n  scale_fill_divergent(guide = \"none\") +\n  quick_map + \n  facet_wrap(PC~., ncol = 1)\n\n\n\n\nFigure 2: Spatial patterns of the frist and second leading EOFs of detrended monthly anomalies of SST between 55°S and 20°S weighted by the square root of the cosine of latitude.\n\n\n\nAnd indeed. The first EOF is kind of ENSO-like (not really full ENSO, because I’m missing the equatorial SSTs, which is where ENSO really shines) and the second EOF looks pretty much identical to the their Figure 1.a save the different prime meridian. The time series associated with the second EOF is also almost exactly the same.\n\n\n\nShow code\n\ncut(eofs, 2) %>% \n  .$left %>%\n  copy() %>% \n  .[, sst5 := frollmean(sst, 5, align = \"center\")] %>% \n  ggplot(aes(time, -sst)) +\n  geom_hline(yintercept = 0, size = 0.2, color = \"gray50\")  +\n  geom_line(aes(y = -sst5))  +\n  geom_line(color = \"gray\") +\n  \n  scale_y_continuous(NULL) +\n  scale_x_datetime(date_breaks = \"5 years\", date_labels = \"%Y\")\n\n\n\n\nFigure 3: Temporal pattern of the second EOF in gray with a 5-month running mean in black.\n\n\n\nOK, I’m on the right track.\nNow, my main concern with this paper is whether this pattern is actually, as the title of the paper says, a global pattern. EOF is a great technique for dimensionality reduction, but it’s too easy to end up with statistical patterns that are a mix of actual physical patterns or even just noise.\nThe authors agree, and they say that..\n\nIn order to examine the synchronization of the W4 pattern among all the basins, point correlation analysis has been performed. For this purpose, eight points [i(37.5°S, 173.5°W), ii(37.5°S, 133.5°W), iii(44.5°S, 90.5°W), iv(39.5°S, 40.5°W), v(29.5°S, 2.5°W), vi(41.5°S, 41.5°E), vii(30.5°S, 86.5°E), viii(35.5°S, 130.5°E)] corresponding to the loading centres are selected (marked by green dots, i-viii, in Fig. 1a). The time series of SST anomaly is computed at each grid point after removing the contributions of the first EOF mode (henceforth, reconstructed SST anomaly). Further, point correlation is performed for the time series at the loading centers (Fig. 1a) with the reconstructed SST anomaly (Fig. 2a–h corresponding respectively to points (i) to (viii) of Fig. 1a).\n\nThe problem, IMHO, is that their Figure 2 doesn’t really show as much synchronisation as they claim. First, let’s reproduce it here.\n\n\nShow code\n\n# Define points of interest\npoints <- tibble::tribble(~lat, ~lon,\n                          -37.5, -173.5,\n                          -37.5, -133.5,\n                          -44.5, -90.5,\n                          -39.5, -40.5, \n                          -29.5, -2.5, \n                          -41.5, 41.5,\n                          -30.5, 86.5,\n                          -35.5, 130.5) %>% \n  as.data.table() %>% \n  .[, lon := ConvertLongitude(lon)] %>% \n  .[, id := tolower(as.roman(seq_len(.N)))] %>% \n  .[, sign := rep(c(-1, 1), 4)]   # sign of original correlation\n\n\n\n\n\nShow code\n\n# Reconstruct SST from leading EOF  and add it to \n# original data                     \nsst_reconstruct <- predict(eofs, n = 1) %>% \n  setnames(\"sst\", \"sst_reconstructed\")\n\nsst <- sst[sst_reconstruct, on = .NATURAL]\nrm(sst_reconstruct)\n\n\n\n\n\nShow code\n\n# Compute correlation maps of  points of interest,\n# both with the orignial sst and sst with filtered EOF1\n# (for compatison)\ncorrs <- sst[points, on = .NATURAL] %>% \n  .[, \":=\"(lon = NULL, lat = NULL)] %>% \n  setnames(c(\"sst\", \"sst_reconstructed\"), c(\"ref\", \"ref_reconstructed\")) %>% \n  .[sst, on = \"time\", allow.cartesian = TRUE] %>% \n  .[, .(correlation = cor(sst, ref),\n        correlation_reconstructed = cor(sst - sst_reconstructed, ref - ref_reconstructed)),\n    by = .(lon, lat, id, sign)]\n\n\n\n\n\n\nShow code\n\ncorrs %>% \n  melt(measure.vars = c(\"correlation\", \"correlation_reconstructed\")) %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = -value*sign), \n                    breaks = AnchorBreaks(anchor = 0, binwidth = 0.1)) +\n  quick_map +\n  geom_point(data = copy(points)[, id := NULL], shape = 21, fill = NA) +\n  geom_point(data = copy(points), shape = 21, color = \"black\", fill =  \"#0e9a83\") +\n  scale_fill_divergent() +\n  facet_grid(id~variable, labeller = labeller(variable = c(correlation = \"SST\", \n                                                           correlation_reconstructed = \"SST - EOF1\")))\n\n\n\n\nFigure 4: Correlation maps of SST and SST with the first EOF filterred out with the corresponding SST at each one of the points of interest. Compare with Figure 2 of Senapati, Dash, and Behera (2021). The sign of the correlation is flipped for the even points as to preserve always the same sign and allow for easier comparison among maps.\n\n\n\nI purposely computed the two versions. The panels on the right should be a reproduction of Senapati, Dash, and Behera (2021)’s Figure 2. It’s hard to compare them because of the different colour palettes and scale limits (the authors chose parameters that enhanced small correlations), but I personally don’t see much coherency. The first three rows do show high correlations between the three points in the Pacific, which in the unfiltered data looks very much like the well-known PSA pattern that appears as a response to El Niño Southern Oscillation.\nAnother way to look at it is with by plotting \\(r^2\\) which directly quantifies the degree of (linear) dependence between points.\n\n\n\nShow code\n\ncorrs %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = correlation_reconstructed^2, \n                        fill = ..level..), breaks = seq(.1, 1, by = 0.1)) +\n  quick_map +\n  geom_point(data = copy(points)[, id := NULL], shape = 21, fill = NA) +\n  geom_point(data = copy(points), shape = 21, color = \"black\", fill =  \"#de3e80\") +\n  scale_fill_viridis_c(oob = scales::squish,\n                       super = ScaleDiscretised) +\n  facet_grid(id~.)\n\n\n\n\nFigure 5: Coefficient of determination (\\(r^2\\)) computed from the correlations with the filtered SST in Figure 4. Contours only show areas with \\(r^2 > 0.1\\).\n\n\n\nAgain, aside from the points in the Pacific, there is not a lot of long-range relationships between points. As I see it, I strongly suspect that this PC2 pattern is not really robustly global.\n\n\nShow code\n\nset.seed(42)\nN <- 500\nrandom_points <- unique(corrs[, .(lon, lat)]) %>% \n  .[sample(.N, N), ] %>% \n  setkey(lon, lat)\n\nrandom_cors <- vapply(seq_len(N), function(i) {\n  r <- random_points[i, ] \n  ref <- sst[lat == r$lat & lon == r$lon, sst - sst_reconstructed]\n  sst[, ref_data := ..ref, by = .(lon, lat)]\n  sst[, .(cor(sst - sst_reconstructed, ref_data)), by = .(lon, lat)] %>% \n    .[, min(V1)]\n}, numeric(1))\n\n\n\nOk, but how could I quantify this vague impression that these maps don’t show a lot of long-range teleconnections? What I’m going to do is to compute 500 correlation maps like Figure 4 but using random points. For each map, I’ll take the absolute value of the minimum (negative) correlation as the level of connectivity.\n\n\n\nShow code\n\ncorrs %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = -correlation_reconstructed*sign), \n                    breaks = AnchorBreaks(anchor = 0, binwidth = 0.1)) +\n  geom_contour2(aes(z = abs(correlation)), breaks = quantile(abs(random_cors), .5), size = 0.25) +\n  quick_map +\n  geom_point(data = copy(points)[, id := NULL], shape = 21, fill = NA) +\n  geom_point(data = copy(points), shape = 21, color = \"black\", fill =  \"#0e9a83\") +\n  scale_fill_divergent() +\n  facet_grid(id~., labeller = labeller(variable = c(correlation = \"SST\", \n                                                           correlation_reconstructed = \"SST - EOF1\")))\n\n\n\n\nFigure 6: Same as Figure 4 but only for the reconstructed SST. Added in black contours, the 0.5 quantile of random correlations derived from 500 correlation maps with random points.\n\n\n\nFigure 6 repeats Figure 4 but it marks the 50th percentile of the minimum correlation values derived from the bootstrap procedure, which is 0.45. 95% of random points had minimum correlations with absolute values larger than 0.66. With this in mind, correlations of the order of \\(\\pm 0.45\\) are not surprising and what would be expected by chance alone. This, of course, is only a crude measure since it doesn’t take into account the size or pattern of the correlations, but I think that it should give some perspective to the real significance to the correlation levels shown here.\nLet’s try something else. From the correlation maps above (and previous knowledge), it’s pretty obvious that the Pacific sector does behave somewhat coherently. So what I’m going to do is to split the spatial pattern into the Pacific basin (between 150°E and 290°E) and the Atlantic-Indian basins (the rest of the hemisphere). Then, I’m going to project each pattern onto the corresponding SST fields to get two indices. If the patterns is really coherent in time, then both indices must be strongly correlated.\n\n\nShow code\n\nseries <- eofs$right %>% copy() %>% \n  .[PC == \"PC2\"] %>% \n  .[, basin := ifelse(lon %between% c(150, 290), \"pacific\", \"atlantic\")] %>% \n  setnames(\"sst\", \"EOF\") %>% \n  .[sst, on = c(\"lon\", \"lat\")] %>% \n  .[, weighted.mean(sst*EOF, cos(lat*pi/180)), by = .(time, basin)]\n\n\n\n\n\nShow code\n\n# Relationship between the two indices. \nseries %>% \n  dcast(time ~ basin, value.var = \"V1\") %>% \n  ggplot(aes(pacific, atlantic)) +\n  geom_point() +\n  geom_label(data = ~.x[, .(cor(pacific, atlantic))], \n            aes(label = paste0(\"cor= \", signif(V1, 2))), \n            x = -0.0035, y = 0.001, size = 7) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(\"Pacific index\") +\n  scale_y_continuous(\"Atlantic-Indian index\")\n\n\n\n\nFigure 7: Relationship between the Pacific index and Atlantic-Indian index.\n\n\n\nI mean… A correlation of 0.44 is not nothing, but it’s also not a lot.\nLet’s do the correlation map of each index. Again, if the pattern is really global, then the correlation map of the Pacific Index should also show th .e Atlantic-Indian pattern and vice versa.\n\n\nShow code\n\npatterns <- eofs$left[PC == \"PC2\"] %>% \n  setnames(c(\"sst\", \"PC\"), c(\"V1\", \"basin\")) %>% \n  rbind(., series, use.names = TRUE) %>% \n  .[sst, on = \"time\", allow.cartesian = TRUE] %>% \n  .[, .(correlation = cor(V1, sst)), by = .(lon, lat, basin)] \n\nlines <- CJ(lon = c(150, 290),\n            basin = c(\"pacific\", \"atlantic\"))\n\n\n\n\n\nShow code\n\npatterns %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = -correlation, fill = ..level..), \n                    breaks = AnchorBreaks()) +\n  quick_map + \n  geom_vline(data = lines, aes(xintercept = lon)) +\n  scale_fill_divergent_discretised(limits = c(-1, 1)) +\n  facet_wrap(basin~., ncol = 1, labeller = labeller(basi = c(pacific = \"Pacific\", \n                                                             atlantic = \"Atlantic-Indian\")))\n\n\n\n\nFigure 8: Correlation patterns with the Pacific index, the Atlantic-Indian index and the PC2.\n\n\n\nAgain… kinda? For the Atlantic-Indian index, the Pacific signal is barely there and it’s actually completely missing in the Western Pacific. And for the Pacific index, the there is some signal in the Indian ocean, but barely any signal in the Atlantic.\nNow one last test. To extend this analysis, I’ll do the same computation but for every longitude. That is, for each longitude, take a 140º wide section of SST centred in that longitude and project the corresponding wave-4 pattern onto it to get a time-varying index. The result, then, is one “local wave-4 index” for each longitude.\n\n\nShow code\n\nlon_width <- diff(c(150, 290))\nlon_halfwidth <- lon_width/2\n\n# \"extended\" version of geopotential height \n# \nsst2 <- eofs$right %>% copy() %>% \n  .[PC == \"PC2\"] %>% \n  setnames(\"sst\", \"EOF\") %>% \n  sst[., on = .NATURAL] %>% \n  ggperiodic::qwrap(lon = c(0, 360) ~ c(-lon_halfwidth, 360 + lon_halfwidth))\n\n# For each longitude, compute EOF using a segment of lon_width width\n# centered in that longitude.\nlon_eofs <- lapply(unique(sst$lon), function(base_lon) {\n  sst2 %>% \n    .[lon %between% (base_lon + c(-lon_halfwidth, lon_halfwidth))] %>% \n    .[, .(eof = weighted.mean(sst*EOF, cos(lat*pi/180))),\n      by = time] %>% \n    .[, base_lon := base_lon] %>% \n    .[]\n}) %>% \n  rbindlist()\n\n\n\n\n\nShow code\n\nk <- lon_eofs %>% \n  widyr::pairwise_cor(base_lon, time, eof) %>% \n  as.data.table() %>% \n  .[, correlation := 1 - abs(correlation)] %>% \n  dcast(item1 ~ item2, value.var = \"correlation\") %>% \n  .[, -1] %>% \n  as.dist() %>% \n  hclust() %>% \n  cutree(3)\n\nsections <- data.table(lon = as.numeric(names(k)), k = k)\ncuts <- sections[c(0, diff(k)) != 0]\n\nlabel_k <- c(\"1\" = \"East Pacific & Atlantic\", \n             \"2\" = \"Indian\",\n             \"3\" = \"West Pacific\")\n\n\ncuts_lon <- LonLabel(cuts$lon)\ncuts_lon_round <- LonLabel(round(cuts$lon/5)*5)\n\n\n\n\n\nShow code\n\nlon_eofs %>% \n  widyr::pairwise_cor(base_lon, time, eof) %>% \n  ggplot(aes(item1, item2)) +\n  geom_contour_fill(aes(z = correlation, fill = ..level..), na.fill = 1) +\n  geom_contour2(aes(z = correlation), size = 0.2) +\n  geom_text_contour(aes(z = correlation, stroke.color = ..level..), color = \"black\",\n                    stroke = 0.2) +\n  scale_fill_divergent(\"Correlation\",\n                       super = ScaleDiscretised) +\n  scale_color_divergent(aesthetics = \"stroke.color\", guide = \"none\") +\n  scale_x_longitude() +\n  scale_y_longitude() +\n  coord_equal() \n\n\n\n\nFigure 9: Pairwise correlation of “local wave-4” indices.\n\n\n\nFigure 9 shows the pairwise correlation between all indices. Correlation drop rapidly with distance, but there are three clearly defined regions of high correlation that could represent various teleconnected areas. Perhaps not coincidentally, they correspond approximately to the three oceanic basins. The Indian between 0º and 120ºE, Western Pacific between 120ºE and 120ºW and Eastern Pacific and Atlantic between 120ºW and 0º.\nBased on these correlations, we use hierarchical clustering with 1 - abs(correlation) as distance measure to classify each longitude into each of 3 groups. The clusters are flanked by the longitudes 17.5°E, 106.5°E, and 120.5°W, which agree well with the visual interpretation of Figure 9. Using this classification, I now create three indices by again projecting the corresponding wave-4 pattern into SST anomalies.\n\n\nShow code\n\nseries_k <- eofs$right %>% copy() %>% \n  .[PC == \"PC2\"] %>% \n  .[sections, on = \"lon\"] %>% \n  setnames(\"sst\", \"EOF\") %>% \n  .[sst, on = c(\"lon\", \"lat\")] %>% \n  .[, .(value = weighted.mean(sst*EOF, cos(lat*pi/180))), by = .(time, k)] %>% \n  .[, value := as.numeric(scale(value)), by = .(k)]\n\n\n\n\n\nShow code\n\npatterns_k <- series_k %>% \n  .[sst, on = \"time\", allow.cartesian = TRUE] %>% \n  .[, .(correlation = cor(value, sst)), by = .(lon, lat, k)] \n\n\n\n\n\n\nShow code\n\npatterns_k %>% \n  ggplot(aes(lon, lat)) +\n  geom_contour_fill(aes(z = -correlation, fill = ..level..), \n                    breaks = AnchorBreaks(0)) +\n  quick_map + \n  geom_raster(data = unique(patterns_k[, .(lon, lat)])[sections, on = \"lon\"],\n              alpha = 0.2) +\n  scale_fill_divergent_discretised(\"Correlation\", limits = c(-1, 1)) +\n  facet_wrap(k~., ncol = 1, labeller = labeller(k = label_k))\n\n\n\n\nFigure 10: Correlation maps between SST and each of the three basin-dependend wave-4 index. Overlayed in gray, the tree distinct areas of shared variability identified in Figure 9 by hierarchical clustering and selecting 3 clusters.\n\n\n\nCorrelation maps between SST anomalies and each of the three indices are shown in Figure 10. Inside the area used to define each index correlation are high and the pattern is well defined, as expected by construction. However, outside those areas, there is very little signal.\n🤷. Take it as you will. From what I’ve seen here, I remain unconvinced that this is a global pattern of SST.\nDownload code\nClick on this button to get the code that generated this document:\n\n\n Download code\n\n\n\n\n\nSenapati, Balaji, Mihir K. Dash, and Swadhin K. Behera. 2021. “Global Wave Number-4 Pattern in the Southern Subtropical Sea Surface Temperature.” Scientific Reports 11 (1): 142. https://doi.org/10.1038/s41598-020-80492-x.\n\n\n\n\n",
    "preview": "posts/2021-01-14-wave4/distill-preview.png",
    "last_modified": "2024-05-03T10:28:44+10:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 403
  }
]
