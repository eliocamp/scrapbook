---
title: "Stop using composites"
description: |
Composites are bad. Use regressions instead. 
author:
  - name: Elio Campitelli
    url: https://eliocamp.github.io/
    affiliation: Centro de Investigaciones del Mar y la Atmósfera
    affiliation_url: http://www.cima.fcen.uba.ar/
date: "2023-06-02"
slug: stop-composites
draft: yes
categories:
  - statistics
output: 
  distill::distill_article: 
    self_contained: FALSE
    pandoc-args: "--wrap=none"
bibliography: bibliography.bib
compare_updates_url: https://github.com/eliocamp/scrapbook/blob/main/_posts/2023-06-01-stop-composites/stop-composites.Rmd
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(code_folding = FALSE, 
                      echo = FALSE, 
                      cache = TRUE, 
                      cache.extra = 1, 
                      fig.width = 8,
                      fig.align = "center")
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

knitr::opts_hooks$set(label = function(options) {
  if (is.null(options$fig.cap)) {
    options$fig.cap <- paste0("(ref:", options$label, "-cap)")
  }
  
  if (is.null(options$fig.alt)) {
    options$fig.alt <- paste0("(ref:", options$label, "-alt)")
  }
  options
})

library(metR)
library(data.table)
library(ggplot2)
library(magrittr)
library(patchwork)

theme_set(theme_minimal() +
            theme(panel.background = element_rect(fill = "#fafafa", color = NA),
                  legend.position = "bottom", 
                  legend.key.width = grid::unit(1, "null"),
                  legend.key.height = grid::unit(0.5, "line"), 
                  legend.frame = element_rect(colour = "black", linewidth = 0.15), 
                  legend.title.position = "top"))

map <- list(
  scale_x_continuous(name = NULL, expand = c(0, 0)),
  scale_y_continuous(name = NULL, expand = c(0, 0)),
  eliotesis::geom_qmap(),
  coord_sf()
)

scale_pp <- scale_fill_divergent_discretised(name = NULL, low = "#8E521C", high = "#00665E",
                                             guide = guide_colorsteps(barheight = 0.5,barwidth = 15)) 
```


```{r datos}
ersst <- function() {
  file <- here::here("_data/alternatives-regression/ersst.mon.Rds")
  
  if (file.exists(file)) {
    return(file)
  }
  dir.create(dirname(file), showWarnings = FALSE, recursive = TRUE)
  base_url <- "https://www.ncei.noaa.gov/pub/data/cmb/ersst/v5/netcdf/ersst.v5."
  years <- 1979:2020
  months <- formatC(1:12, width = 2, flag = "0")
  dates <- data.table::CJ(years, months)[, paste0(years, months)]
  
  urls <- paste0(base_url, dates, ".nc")
  files <- file.path("_data/alternatives-regression/temp", 
                     paste0("ersst_", dates, ".nc"))
  dir.create(dirname(files[1]), showWarnings = FALSE, recursive = TRUE)
  
  to_download <- !file.exists(files)
  
  res <- curl::multi_download(urls[to_download], files[to_download])
  
  data <- lapply(files, function(file) metR::ReadNetCDF(file, vars = c(t = "sst")))
  data <- data.table::rbindlist(data)
  data[, time := as.Date(lubridate::floor_date(time, "month"))]
  saveRDS(data, file)
  file
}

cmap <- function() {
  cmap_file <- here::here("_data/alternatives-regression/precip.mon.mean.nc")
  
  if (file.exists(cmap_file)) {
    return(cmap_file)
  }
  
  dir.create(dirname(cmap_file), showWarnings = FALSE, recursive = TRUE)
  cmap_url <- "https://downloads.psl.noaa.gov/Datasets/cmap/std/precip.mon.mean.nc"
  download.file(cmap_url, cmap_file, mode = "wb")  
  
  cmap_file
}

```

```{r pp}
sesa <- list(lon = c(-60.74, -48)+360,
             lat =  c(-34.6, -24))
pp <- cmap() |> 
  ReadNetCDF(vars = c(pp = "precip"), 
             subset = sesa) |> 
  _[, .(pp = weighted.mean(pp, cos(lat*pi/180))), by = .(time = as.Date(time))] |> 
  _[year(time) %between% c(1979, 2019)] |> 
  _[, .(pp = mean(pp)), by = .(time = seasonally(time))] |> 
  _[, pp_a := Anomaly(pp), by = season(time)] 

sesabox <- annotate(xmin = sesa$lon[1], xmax = sesa$lon[2],
                    ymin = sesa$lat[1], ymax = sesa$lat[2],
                    geom = "rect", 
                    fill = "#c061cb", alpha = 0.5,
                    colour = "#613583", linewidth = 0.1)
```

```{r sst}
sst <- ersst() |> 
  readRDS() |> 
  _[year(time) %between% c(1979, 2019)] |> 
  _[, .(t = mean(t, na.rm = TRUE)), by = .(time = seasonally(time), lon, lat)] |> 
  _[, t_a := Anomaly(t), by = .(lon, lat, season(time))]
```

```{r}
plot_regr <- function(data, variable = estimate) {
  data |> 
    ggplot(aes(lon, lat)) +
    geom_contour_fill(aes(z = {{ variable }}, fill = after_stat(level)), 
                      breaks = AnchorBreaks(exclude = 0)) +
    scale_fill_divergent_discretised(NULL) +
    map 
}
```


In atmospheric science it's very common to have a target variable and to try to understand how it is related to a spatial variable defined on a grid. 
For example, how is seasonal precipitation in southeastern South America affected by Global Sea Surface Temperatures? 

As far as I'm aware, the typical method to do this is to compute a linear regression (or correlation) at each gridpoint and then plot the coefficients on a map. 
The result is something like this:

```{r}
sst[pp, on = "time"] |> 
  _[, FitLm(t_a, pp_a), by = .(lon, lat)] |> 
  plot_regr() +
  sesabox
```
This pattern interpreted as that, on average, it rains more when Sea Surface Temperatures in the equatorial Pacific are higher than average and it rains less when temperatures are lower than average. 
The typical El Niño signal. 
Domain knowledge and model experiments help to support this conclusion, to elucidate the direction of causality () and also to understand that those signals in the southern Pacific are also caused by El Niño, and not directly causally linked to precipitation in southeastern South America. 

This is fine and dandy, but I've never been totally satisfied with this statistical method. 

First, since this is essentially fitting thousands of independent linear models, it's not trivial to compute the statistical significance of the whole pattern. 
In general, people compute the statistical significance at each gridpoint and hatch significant area, which falls victim of the multiple comparisons problem. 
It's possible to try to correct for multiple comparisons, but that comes with its own set of problems and also usually doesn't account for the problem of spatial autocorrelation (each gridpoint is not independent from the rest).

Second is the hidden assumption that a single spatial pattern is enough to understand the relationship. 

For example, look at that positive and negative signals in the Atlantic ocean. 
It would be tempting to conclude that a the temperature difference between those two regions is important in some way, but those two signals could easily be completely independent in time.
What I mean is that precipitation in the region might increase with positive temperature anomalies in the tropical Atlantic and negative temperature anomalies in the southern Atlantic and the gradient between the two regions might not play a role whatsoever. 

This can be illustrated with a synthetic dataset consisting on the time series of temperature at 50°S 150°E before 2000 and negative temperature at 50°S 100°E after 2000. 
The "gridpoint-wise" regression gives this: 

```{r}
y <- sst[lat %~% -50 & lon %~% c(150, 100)] |> 
  dcast(time ~ lon, value.var = "t_a") |> 
  _[year(time) <= 2000, pp_a := `150`] |>
  _[year(time) > 2000, pp_a :=  - `100`] |>
  _[, .(time, pp_a)]

points <- list(annotate(x = c(150, 100), y = -50,
                        geom = "point", shape = c("+", "-"), size = 5),
               annotate(x = c(150, 100), y = -50,
                        geom = "point", size = 5, shape = 21))
```

```{r}
sst[y, on = "time"] |> 
  _[, FitLm(t_a, pp_a), by = .(lon, lat)] |> 
  plot_regr() +
  points
```

Which might suggest that this fake time series is somehow associated with a big temperature gradient south of Australia. 


So, are there any alternative methods that can solve any of these issues? 

### Field significance 

One interesting propsal that tries to address the first issue is [DelSole]. 
The idea is to essentially turn the problem on its head and instead.
Instead of fitting multiple models of temperature as a function of precipitation like in the typical method, it tries to fit a single model of precipitation as a function of temperature. 
In other words, find the linear combination of gridpoints that best predict precipitation. 

The issue, of course, is that in most gridded dataset there are much more gridpoints that timesteps so it's an overdeteremined problem. 
The trick is then to apply some form of regularisation of variable selection. 

What [DelSole] proposes is to reduce the dimensionality by doing the fit in Principal Component space: compute only the first P Principal Components of the data with P less than the number of observations, fit the model, and then compute the spatial field. 
They propose using crossvalidation to select how many Principal Components to keep and derive a single statistic to compute the field significance. 

I implemented [DelSole] method in the lm2d package (which is very much not ready for public consumption)

```{r}
model <- sst[pp, on = "time"] |> 
  na.omit() |> 
  _[, t_a := t_a*sqrt(cos(lat*pi/180))] |>
  lm2d::lm2d(t_a ~ time | lon + lat, y = pp_a, 
             data = _)
```

The model used only `r model$summary$non_zero` principal components for the fit, and their linear combination looks like this

```{r}
model$field |> 
  plot_regr(pp_a/sqrt(cos(lat*pi/180))) +
  sesabox
```

This is very similar to the "point-wise" regression above. 
The model statistic is also highly significant ($p.value `r scales::pvalue(model$summary$p.value)`$). 

A limitation of this method is the use of Principal Components, which makes the whole regression field dependent on the domain. 
The regular grid in longitude by latitude is also poor for computing Principal Components because each point represents different areas; this is solved by proper weighting of each gridpoint (ref), but it's a further complication. 
There's also no guarantee that the Principal Component directions are the optimal directions to capture the relationship of interest. 
For example, the regression field of the fake data is this: 

```{r}
model <- sst[y, on = "time"] |> 
  na.omit() |> 
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)] |> 
  _[, t_a := t_a*sqrt(cos(lat*pi/180))] |>
  lm2d::lm2d(t_a ~ time | lon + lat, y = pp_a, 
             data = _)
```

```{r}
model$field |> 
  plot_regr(pp_a/sqrt(cos(lat*pi/180))) +
  points
```

Which is pretty much nonsense. 
Here, no Principal Component is all that correlated with the target variable so the [DeSolde] method chooses only the first principal component, which is ENSO. 

I believe there is some way around it by further weighting the variables by their correlation with the target variable (inspired by Supervised Principal Regression (https://hastie.su.domains/Papers/spca_JASA.pdf)).
But the result is still not great: 

```{r}
data <- sst[y, on = "time"] |> 
  na.omit() |> 
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)]

W <-  data |> 
  _[, .(W = cor(pp_a, t_a, use = "complete.obs")^2), by = .(lon, lat)]

model <- data[W, on = .NATURAL] |> 
  _[, t_a := t_a*sqrt(cos(lat*pi/180))*W] |>
  lm2d::lm2d(t_a ~ time | lon + lat, y = pp_a, 
             data = _) 

model$field[W, on = .NATURAL] |> 
  plot_regr(pp_a/(sqrt(cos(lat*pi/180))*W)) +
  points
```


Interestingly, an alternative implementation of the [DeSolde] method that uses lasso regression to select the principal components does a better job in this case: 

```{r}
sst[y, on = "time"] |> 
  na.omit() |> 
  lm2d::lm2d(t_a ~ time | lon + lat, y = pp_a, 
             method = lm2d::fit_lasso(),
             data = _) |> 
  _$field |>
  plot_regr(pp_a) +
  points
```

However, I'm not sure if the statistic derived by [DeSolde] is valid for this selection method.



## LASSO

So lasso regression works  well in Principal Component space, but why not use it in the original space?

Well, I tried it and the results are not great: 

```{r}
LassoLM <- function(data, formula, value.var, y,
                    kernel = "linear", ...) {
  M <- metR:::.tidy2matrix(data, formula, value.var = value.var)
  
  res <- glmnet::cv.glmnet(M$matrix, y, ...)
  
  list(model = res,
       coef = M$coldims[, estimate := as.vector(coef(res))[-1]][])
}
```

```{r}
model <- sst |> 
  na.omit() |> 
  LassoLM(time ~ lon + lat, value.var = "t_a", y = pp$pp_a, 
          alpha = 1) 

model$coef |> 
  plot_regr() +
  sesabox
```

As you can see, lasso essentially chose only three gridpoints and I suspect is pretty much overfitting even with the penalisation chosen by crossvalidation. 


## Perceptron

Yes, you read it right. 
I had this idea a few year ago when taking a Machine Learning course. 
The perceptron finds the hyperplane that separates two categories and an hyperplane can be defined as a direction in feature space, so why not look at that direction? 
Here the task is not strictly lienar regression, but classification between positive and negative values of the target variable. 
However, if the linear model holds, then that direction shoudl pretty close to the regression direction. 


```{r, PerceptronLm}
PerceptronLm <-  function(data, formula, value.var, y) {
  perceptron_train <- function(X, y, epochs = 100, lambda = 10) {
    X <- cbind(1, X)
    W <- rnorm(ncol(X))
    Ws <- vector("list", epochs)
    accuracy <- rep(NA,  epochs)
    
    for (e in seq_len(epochs)) {
      for (i in seq_len(nrow(X))) {
        y_hat <- sum(W * X[i, ])
        
        pred <- sign(as.numeric(y_hat >= 0) - 0.5)
        
        W <- W + lambda*(y[i] - pred)*X[i, ]
      }
      
      W <- W/sqrt(sum(W^2))
      y_hat <- X %*% matrix(W, ncol = 1)
      pred <- sign(as.numeric(y_hat >= 0) - 0.5)
      
      accuracy[e] <- mean((y - pred) == 0)
      Ws[[e]] <- W
      
      if (accuracy[e] == 1) break
      
    }
    
    return(Ws[[which.max(accuracy)]])
  }
  
  M <- metR:::.tidy2matrix(data, formula, value.var = value.var)
  
  p <- perceptron_train(M$matrix, sign(y))
  
  M$coldims[, estimate := p[-1]][]
  
}
```

The "perceptron regression" gives me this: 


```{r}
(sst |> 
   na.omit() |>
   PerceptronLm(time ~ lon + lat, value.var = "t_a", y = pp$pp_a) |> 
   plot_regr() +
   labs(title = "Precipitation") +
   sesabox) +
  
  (sst |> 
     na.omit() |>
     PerceptronLm(time ~ lon + lat, value.var = "t_a", y = y$pp_a) |> 
     plot_regr() +
     labs(title = "Fake data") +
     points
  )  +
  plot_layout(ncol = 1)
```

Which is, honestly, not too bad. 
I don't think this has any real merit, but it works as a stepping stone for a more serious option

## Support Vector Machine

Support Vector Machine (SVM) is similar to the perceptron and, when using a linear kernel, also returns an hyperplane that we can use as field regression. 
The result is this:

```{r}
SVM <- function(data, formula, value.var, y,
                kernel = "linear", ...) {
  M <- metR:::.tidy2matrix(data, formula, value.var = value.var)
  
  res <- e1071::svm(M$matrix, y, kernel = kernel, type = "nu-regression", ...)
  
  list(model = res,
       coef = M$coldims[, estimate := t(res$SV) %*% res$coefs][])
}
```

```{r}
clean_data <- sst |> 
  na.omit() |>
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)]
```


```{r}
(
  clean_data |> 
    SVM(time ~ lon + lat, value.var = "t_a", y = pp$pp_a, 
        kernel = "linear", nu = 1e-5) |> 
    _$coef |> 
    plot_regr() +
    labs(title = "Precipitation") +
    sesabox) +
  
  (clean_data |> 
     SVM(time ~ lon + lat, value.var = "t_a", y = y$pp_a, 
         kernel = "linear", nu = 1e-5) |> 
     _$coef |> 
     plot_regr()  +
     labs(title = "Fake data") +
     points
  )  +
  plot_layout(ncol = 1)
```

Although when I say "the result", I'm lying a bit, because for nu-regression I need to chose a nu value. 
I might be able to chose it based on crossvalidation, but the e1071 package I'm using to fit the SMV doesn't come with that out of the box and I'm not going to implement all that just for a blogpost. 

In any case, the results are also not terrible.
The precipitation example seems to exaggerate higher latitude features compared with gridpoint-wise regression, but since there is no ground truth, that very well could be an issue with the latter method.
The fake data example is pretty neat as it recovers basically what would be expected. 

You could also use SVM in Principal Component space: 


## Sparse Partial Least Squares

In principle this method 

```{r}
SPLSLm <- function(data, formula, value.var, y,
                   kernel = "linear", ...) {
  M <- metR:::.tidy2matrix(data, formula, value.var = value.var)
  
  
  model <- spls::spls(M$matrix, y, ...)
  
  betas <- lapply(model$betamat, \(mat) {
    M$coldims |> 
      copy() |> 
      _[, estimate := as.vector(mat)] 
  }) |> 
    rbindlist(idcol = "coef") 
  
  list(model = model, 
       beta = betas,
       coef = M$coldims |> 
         copy() |> 
         _[, estimate := coef(model)])
}
```

```{r}
model <- clean_data |> 
  SPLSLm(time ~ lon + lat, value.var = "t_a", y = y$pp_a, 
         eta = 0.5, K = 2)
```

```{r}
model$beta |> 
  copy() |> 
  plot_regr() + 
  facet_wrap(~ coef) 
```

```{r}
model$beta |> 
  copy() |> 
  _[clean_data, on = .NATURAL, allow.cartesian = TRUE] |> 
  _[, .(pred = sum(estimate * t_a)), by = .(coef, time)] |> 
  _[pp, on = "time"] |> 
  _[, cor(pred, pp_a)^2, by = .(coef, year(time) < 2000)] |> 
  dcast(coef ~ year)
  
```


```{r}
model <- clean_data |> 
  SPLSLm(time ~ lon + lat, value.var = "t_a", y = pp$pp_a, K = 3)
```

```{r}
model$beta |> 
  copy() |> 
  _[, estimate := estimate/sd(estimate), by = .(coef)] |> 
  plot_regr() + 
  facet_wrap(~ coef, ncol = 1) 
```

```{r}
model$beta |> 
  copy() |> 
  _[clean_data, on = .NATURAL, allow.cartesian = TRUE] |> 
  _[, .(pred = sum(estimate * t_a)), by = .(coef, time)] |> 
  _[pp, on = "time"] |> 
  ggplot(aes(pp_a, pred)) +
  geom_point() +
  facet_wrap(~ coef)
```

## CCA

```{r}
CCALm <- function(data, formula, value.var, y, ...) {
  M <- metR:::.tidy2matrix(data, formula, value.var = value.var)
  
  
  browser()
  
  
  model <- spls::spls(M$matrix, y,
                      eta = 0.8, ...,
                      select = "pls2")
  
  betas <- lapply(model$betamat, \(mat) {
    M$coldims |> 
      copy() |> 
      _[, estimate := as.vector(mat)] 
  }) |> 
    rbindlist(idcol = "coef") 
  
  list(model = model, 
       beta = betas,
       coef = M$coldims |> 
         copy() |> 
         _[, estimate := coef(model)])
}


```

```{r}
model <- clean_data |> 
  CCALm(time ~ lon + lat, value.var = "t_a", y = pp$pp_a)
```

```{r}
CCA::rcc()
```


LARS

Tried, it, but didn't seem to work. 
I think the problem is that I have highly correlated predictors (nearby gridpoints are highly correlated) and the wikipedia article says that this method has problems with multicollinearity. 


```{r}
model <- lars::lars(M$matrix, y = pp$pp_a, type = "lar", 
                    use.Gram = FALSE, max.steps = 50)

M$coldims |> 
  copy() |> 
  _[, estimate := as.vector(coef(model)[34 ])] |> 
  ggplot(aes(lon, lat)) +
  geom_contour_fill(aes(z = estimate)) + 
  scale_fill_divergent(NULL) +
  map 

```



```{r}
copy(y) |> 
  _[, pred := predict(model)] |> 
  ggplot(aes(year, pred)) +
  geom_line(aes(color = "pred")) +
  geom_line(aes(y = pp_a, color = "truth"))

```


```{r}
y_std <- copy(pp)[, pp_a := pp_a/sd(pp_a)] |> 
  _[, .(year, pp_a)] 

W <- sst |>
  na.omit() |>
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)] |>
  # _[, .SD[.N > 5], by = .(lon, lat)] |> 
  _[y_std, on = "year"] |>
  _[, FitLm(pp_a, poly(t_a, degree = 2), r2 = TRUE), by = .(lon, lat)] |> 
  _[term == term[1]] |> 
  _[, .(lon, lat, w = adj.r.squared)] |> 
  _[w < 0 , w := 0]

e <- sst |> 
  na.omit() |> 
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)] |> 
  _[y_std, on = "year"] |> 
  _[W, on = .NATURAL] |> 
  _[, t_a := t_a * w * sqrt(cos(lat*pi/180))] |>
  EOF(t_a ~ lon + lat | year, n = 1:10, data = _)
```

```{r}
W |> 
  ggplot(aes(lon, lat)) +
  geom_raster(aes(fill = w)) +
  scale_fill_viridis_c()
```


```{r}
pcs <- e$right[y_std, on = "year"] |> 
  na.omit() |> 
  _[, cor(t_a, pp_a), by = PC] |> 
  _[order(-V1^2)] |> 
  _[, .(PC, sign = sign(V1), cor = V1, r = V1^2)]

e$left |> 
  _[pcs, on = "PC"] |>
  _[, PC := reorder(PC, -r)] |> 
  _[W, on = .NATURAL] |>
  # _[PC == "PC"] |> 
  ggplot(aes(lon, lat)) +
  geom_contour_fill(aes(z = t_a*sign/(sqrt(cos(lat*pi/180))))) +
  scale_fill_divergent() +
  facet_wrap(~ PC) +
  points
```


```{r}
order <- e$right[y_std, on = "year"] |> 
  _[, cor(t_a, pp_a), by = PC] |> 
  _[pcs, on = "PC"] |>
  _[, PC := reorder(PC, -r)] 

order |> 
  ggplot(aes(PC, V1^2)) +
  geom_col(aes(fill = factor(sign)))


```

```{r}
y_r <- e$right[PC == order$PC[1]] |> 
  _[y_std, on = "year"] |> 
  _[, .(year, pp_a = ResidLm(pp_a, t_a))]


W <- sst |>
  na.omit() |>
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)] |>
  # _[, .SD[.N > 5], by = .(lon, lat)] |> 
  _[y_r, on = "year"] |>
  _[, FitLm(pp_a, poly(t_a, degree = 2), r2 = TRUE), by = .(lon, lat)] |> 
  _[term == term[1]] |> 
  _[, .(lon, lat, w = adj.r.squared)] |> 
  _[w < 0 , w := 0]

e <- sst |> 
  na.omit() |> 
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)] |> 
  _[y_r, on = "year"] |> 
  _[W, on = .NATURAL] |> 
  _[, t_a := t_a * w * sqrt(cos(lat*pi/180))] |>
  EOF(t_a ~ lon + lat | year, n = 1:10, data = _)
```

```{r}
pcs <- e$right[y_std, on = "year"] |> 
  na.omit() |> 
  _[, cor(t_a, pp_a), by = PC] |> 
  _[order(-V1^2)] |> 
  _[, .(PC, sign = sign(V1), cor = V1, r = V1^2)]

e$left |> 
  _[pcs, on = "PC"] |>
  _[, PC := reorder(PC, -r)] |> 
  _[W, on = .NATURAL] |>
  _[PC == "PC2"] |>
  ggplot(aes(lon, lat)) +
  geom_contour_fill(aes(z = t_a*sign/(sqrt(cos(lat*pi/180))))) +
  scale_fill_divergent() +
  facet_wrap(~ PC) +
  points
```

```{r}

M <- sst |>
  na.omit() |>
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)] |>
  metR:::.tidy2matrix(year ~ lon + lat, value.var = "t_a")

debugonce(sRDA::sCCA)

as_nodrop <- function(x) {
  class(x) <- c("nodrop_matrix", class(x))
  x
}
`[.nodrop_matrix` <- function(x, ...) {
  class(x) <- class(x)[-1]
  
  base::`[`(x, ..., drop=FALSE)
}

cca <- CVR::SparseCCA(M$matrix, cbind(y$pp_a))

```

```{r}
pp |> 
  copy() |> 
  _[, pred := predict(model, newx = M$matrix)] |> 
  ggplot(aes(year, pred)) +
  geom_line() +
  geom_line(color= "red", aes(y = pp_a))
```


```{r}
y_std <- copy(pp)[, pp_a := pp_a/sd(pp_a)] |> 
  _[, .(year, pp_a)] 

d <- sst |>
  na.omit() |>
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)] |>
  _[y_std, on = "year"]


max <- d |> 
  _[, cor(pp_a, t_a)^2, by = .(lon, lat)] |> 
  _[which.max(V1)]

d[max, on = c("lon", "lat")] |> 
  _[, .(year, base = t_a)] |> 
  merge(d) |> 
  _[, FitLm(t_a, base), by = .(lon, lat)] |> 
  _[term == "base"] |> 
  plot_regr() 

```

```{r}

y_r <- d[max, on = c("lon", "lat")] |> 
  _[, .(year, pp_a = ResidLm(pp_a, t_a))]

d <- sst |>
  na.omit() |>
  _[, .SD[sd(t_a) != 0], by = .(lon, lat)] |>
  _[y_r, on = "year"]


max <- d |> 
  _[, cor(pp_a, t_a)^2, by = .(lon, lat)] |> 
  _[which.max(V1)]

d[max, on = c("lon", "lat")] |> 
  _[, .(year, base = t_a)] |> 
  merge(d) |> 
  _[, FitLm(t_a, base), by = .(lon, lat)] |> 
  _[term == "base"] |> 
  plot_regr() 

```

